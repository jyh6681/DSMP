{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "71910fc4-fe93-48ff-8a31-7e7263e5fe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import lobpcg\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import get_laplacian\n",
    "from torch_geometric.nn import GATConv\n",
    "import math\n",
    "import argparse\n",
    "import os.path as osp\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "\n",
    "# function for pre-processing\n",
    "@torch.no_grad()\n",
    "def scipy_to_torch_sparse(A):\n",
    "    A = sparse.coo_matrix(A)\n",
    "    row = torch.tensor(A.row)\n",
    "    col = torch.tensor(A.col)\n",
    "    index = torch.stack((row, col), dim=0)\n",
    "    value = torch.Tensor(A.data)\n",
    "\n",
    "    return torch.sparse_coo_tensor(index, value, A.shape)\n",
    "\n",
    "\n",
    "# function for pre-processing\n",
    "def ChebyshevApprox(f, n):  # assuming f : [0, pi] -> R\n",
    "    quad_points = 500\n",
    "    c = np.zeros(n)\n",
    "    a = np.pi / 2\n",
    "    for k in range(1, n + 1):\n",
    "        Integrand = lambda x: np.cos((k - 1) * x) * f(a * (np.cos(x) + 1))\n",
    "        x = np.linspace(0, np.pi, quad_points)\n",
    "        y = Integrand(x)\n",
    "        c[k - 1] = 2 / np.pi * np.trapz(y, x)\n",
    "\n",
    "    return c\n",
    "\n",
    "\n",
    "# function for pre-processing\n",
    "def get_operator(L, DFilters, n, s, J, Lev):\n",
    "    r = len(DFilters)\n",
    "    c = [None] * r\n",
    "    for j in range(r):\n",
    "        c[j] = ChebyshevApprox(DFilters[j], n)\n",
    "    a = np.pi / 2  # consider the domain of masks as [0, pi]\n",
    "    # Fast Tight Frame Decomposition (FTFD)\n",
    "    FD1 = sparse.identity(L.shape[0])\n",
    "    d = dict()\n",
    "    for l in range(1, Lev + 1):\n",
    "        for j in range(r):\n",
    "            T0F = FD1\n",
    "            T1F = ((s ** (-J + l - 1) / a) * L) @ T0F - T0F\n",
    "            d[j, l - 1] = (1 / 2) * c[j][0] * T0F + c[j][1] * T1F\n",
    "            for k in range(2, n):\n",
    "                TkF = ((2 / a * s ** (-J + l - 1)) * L) @ T1F - 2 * T1F - T0F\n",
    "                T0F = T1F\n",
    "                T1F = TkF\n",
    "                d[j, l - 1] += c[j][k] * TkF\n",
    "        FD1 = d[0, l - 1]\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "class UFGConv(nn.Module):\n",
    "    def __init__(self, in_features, out_features, r, Lev, num_nodes, shrinkage=None, threshold=1e-4, bias=True):\n",
    "        super(UFGConv, self).__init__()\n",
    "        self.Lev = Lev\n",
    "        self.shrinkage = shrinkage\n",
    "        self.threshold = threshold\n",
    "        self.crop_len = (Lev - 1) * num_nodes\n",
    "        if torch.cuda.is_available():\n",
    "            self.weight = nn.Parameter(torch.Tensor(in_features, out_features).cuda())\n",
    "#             self.filter = nn.Parameter(torch.Tensor(r * Lev * num_nodes, 1).cuda())\n",
    "            self.filter1 = nn.Parameter(torch.Tensor(num_nodes, 1).cuda())\n",
    "            self.filter2 = nn.Parameter(torch.Tensor(num_nodes, 1).cuda())\n",
    "            self.filter3 = nn.Parameter(torch.Tensor(num_nodes, 1).cuda())\n",
    "        else:\n",
    "            self.weight = nn.Parameter(torch.Tensor(in_features, out_features))\n",
    "            self.filter = nn.Parameter(torch.Tensor(r * Lev * num_nodes, 1))\n",
    "        if bias:\n",
    "            if torch.cuda.is_available():\n",
    "                self.bias = nn.Parameter(torch.Tensor(out_features).cuda())\n",
    "            else:\n",
    "                self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "        # GAT module\n",
    "        self.GATconv1 = GATConv(out_features, out_features, heads=1, dropout=0.6)\n",
    "        self.GATconv2 = GATConv(out_features, out_features, heads=1, dropout=0.6)\n",
    "        self.GATconv3 = GATConv(out_features, out_features, heads=1, dropout=0.6)\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "#         nn.init.uniform_(self.filter, 0.9, 1.1)\n",
    "        nn.init.uniform_(self.filter1, 0.9, 1.1)\n",
    "        nn.init.uniform_(self.filter2, 0.9, 1.1)\n",
    "        nn.init.uniform_(self.filter3, 0.9, 1.1)\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        if self.bias is not None:\n",
    "            nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x, edge_index, d_list):\n",
    "        # d_list is a list of matrix operators (torch sparse format), row-by-row\n",
    "        # x is a torch dense tensor\n",
    "        x = torch.matmul(x, self.weight)\n",
    "\n",
    "        # Fast Tight Frame Decomposition\n",
    "        x1 = torch.sparse.mm(d_list[1], x)\n",
    "        x2 = torch.sparse.mm(d_list[2], x)\n",
    "        x3 = torch.sparse.mm(d_list[3], x)\n",
    "#         x1 = torch.sparse.mm(torch.cat(d_list[:2], dim=0), x)\n",
    "#         x2 = torch.sparse.mm(torch.cat(d_list[2:], dim=0), x)\n",
    "#         x = torch.cat((x1, x2),0)\n",
    "        # the output x has shape [r * Lev * num_nodes, #Features]\n",
    "\n",
    "        # perform wavelet shrinkage (optional)\n",
    "        if self.shrinkage is not None:\n",
    "            if self.shrinkage == 'soft':\n",
    "                x = torch.mul(torch.sign(x), (((torch.abs(x) - self.threshold) + torch.abs(torch.abs(x) - self.threshold)) / 2))\n",
    "            elif self.shrinkage == 'hard':\n",
    "                x = torch.mul(x, (torch.abs(x) > self.threshold))\n",
    "            else:\n",
    "                raise Exception('Shrinkage type is invalid')\n",
    "\n",
    "        # Hadamard product in spectral domain\n",
    "#         x = self.filter * x\n",
    "        x1 = self.filter1 * x1\n",
    "#         x2 = self.filter2 * x2\n",
    "#         x3 = self.filter3 * x3\n",
    "        # filter has shape [r * Lev * num_nodes, 1]\n",
    "        # the output x has shape [r * Lev * num_nodes, #Features]\n",
    "        \n",
    "#         # GAT conv in spectral domain\n",
    "#         x1 = F.dropout(F.elu(self.GATconv1(x1, edge_index)), p=0.6, training=self.training)\n",
    "        x2 = F.dropout(F.elu(self.GATconv2(x2, edge_index)), p=0.6, training=self.training)\n",
    "        x3 = F.dropout(F.elu(self.GATconv3(x3, edge_index)), p=0.6, training=self.training)\n",
    "\n",
    "        # Fast Tight Frame Reconstruction\n",
    "#         x = torch.sparse.mm(torch.cat(d_list[self.Lev - 1:], dim=0).transpose(0,1), x[self.crop_len:, :])\n",
    "#         x1 = torch.sparse.mm(torch.cat(d_list[self.Lev - 1:2], dim=0).transpose(0,1), x1[self.crop_len:, :])\n",
    "#         x2 = torch.sparse.mm(torch.cat(d_list[2:], dim=0).transpose(0,1), x2)\n",
    "#         x = x1 + x2\n",
    "        x = torch.sparse.mm(torch.cat(d_list[1:], dim=0).transpose(0,1), torch.cat((x1,x2,x3),0))\n",
    "        if self.bias is not None:\n",
    "            x += self.bias\n",
    "        return x\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_features, nhid, num_classes, r, Lev, num_nodes, shrinkage=None, threshold=1e-4, dropout_prob=0.5):\n",
    "        super(Net, self).__init__()\n",
    "        self.GConv1 = UFGConv(num_features, nhid, r, Lev, num_nodes, shrinkage=shrinkage, threshold=threshold)\n",
    "        self.GConv2 = UFGConv(nhid, num_classes, r, Lev, num_nodes, shrinkage=shrinkage, threshold=threshold)\n",
    "        self.drop1 = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, data, d_list):\n",
    "        x = data.x  # x has shape [num_nodes, num_input_features]\n",
    "        edge_index = data.edge_index\n",
    "        x = F.relu(self.GConv1(x, edge_index, d_list))\n",
    "        x = self.drop1(x)\n",
    "        x = self.GConv2(x, edge_index, d_list)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "8939f0bc-6012-4f4d-9d11-ff20258665b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Training on CPU/GPU device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "02f032f9-213b-403e-b0f0-b87f1dc2f694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataname = 'Cora'\n",
    "rootname = osp.join(osp.abspath(''), 'data', dataname)\n",
    "dataset = Planetoid(root=rootname, name=dataname)\n",
    "\n",
    "num_nodes = dataset[0].x.shape[0]\n",
    "L = get_laplacian(dataset[0].edge_index, num_nodes=num_nodes, normalization='sym')\n",
    "L = sparse.coo_matrix((L[1].numpy(), (L[0][0, :].numpy(), L[0][1, :].numpy())), shape=(num_nodes, num_nodes))\n",
    "\n",
    "lobpcg_init = np.random.rand(num_nodes, 1)\n",
    "lambda_max, _ = lobpcg(L, lobpcg_init)\n",
    "lambda_max = lambda_max[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "916699d9-e08d-447a-9568-f5bcbae4de79",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FrameType = 'Haar'\n",
    "FrameType = 'Haar'\n",
    "if FrameType == 'Haar':\n",
    "    D1 = lambda x: np.cos(x / 2)\n",
    "    D2 = lambda x: np.sin(x / 2)\n",
    "    DFilters = [D1, D2]\n",
    "    RFilters = [D1, D2]\n",
    "elif FrameType == 'Linear':\n",
    "    D1 = lambda x: np.square(np.cos(x / 2))\n",
    "    D2 = lambda x: np.sin(x) / np.sqrt(2)\n",
    "    D3 = lambda x: np.square(np.sin(x / 2))\n",
    "    DFilters = [D1, D2, D3]\n",
    "    RFilters = [D1, D2, D3]\n",
    "elif FrameType == 'Quadratic':  # not accurate so far\n",
    "    D1 = lambda x: np.cos(x / 2) ** 3\n",
    "    D2 = lambda x: np.multiply((np.sqrt(3) * np.sin(x / 2)), np.cos(x / 2) ** 2)\n",
    "    D3 = lambda x: np.multiply((np.sqrt(3) * np.sin(x / 2) ** 2), np.cos(x / 2))\n",
    "    D4 = lambda x: np.sin(x / 2) ** 3\n",
    "    DFilters = [D1, D2, D3, D4]\n",
    "    RFilters = [D1, D2, D3, D4]\n",
    "else:\n",
    "    raise Exception('Invalid FrameType')\n",
    "\n",
    "Lev = 2  # level of transform\n",
    "s = 2  # dilation scale\n",
    "n = 2  # n - 1 = Degree of Chebyshev Polynomial Approximation\n",
    "J = np.log(lambda_max / np.pi) / np.log(s) + Lev - 1  # dilation level to start the decomposition\n",
    "r = len(DFilters)\n",
    "\n",
    "# get matrix operators\n",
    "d = get_operator(L, DFilters, n, s, J, Lev)\n",
    "# enhance sparseness of the matrix operators (optional)\n",
    "# d[np.abs(d) < 0.001] = 0.0\n",
    "# store the matrix operators (torch sparse format) into a list: row-by-row\n",
    "d_list = list()\n",
    "for l in range(Lev):\n",
    "    for i in range(r):\n",
    "        d_list.append(scipy_to_torch_sparse(d[i, l]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "e0fe6fdc-b8d4-4f13-b1d5-b09e945441a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameter Settings\n",
    "learning_rate = 0.01\n",
    "weight_decay = 0.01\n",
    "nhid = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "998019cf-07cd-4889-8912-a685407f0ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the data\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "# create result matrices\n",
    "num_epochs = 400\n",
    "num_reps = 1\n",
    "epoch_loss = dict()\n",
    "epoch_acc = dict()\n",
    "epoch_loss['train_mask'] = np.zeros((num_reps, num_epochs))\n",
    "epoch_acc['train_mask'] = np.zeros((num_reps, num_epochs))\n",
    "epoch_loss['val_mask'] = np.zeros((num_reps, num_epochs))\n",
    "epoch_acc['val_mask'] = np.zeros((num_reps, num_epochs))\n",
    "epoch_loss['test_mask'] = np.zeros((num_reps, num_epochs))\n",
    "epoch_acc['test_mask'] = np.zeros((num_reps, num_epochs))\n",
    "saved_model_val_acc = np.zeros(num_reps)\n",
    "saved_model_test_acc = np.zeros(num_reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "7f94419a-0d02-4e03-9056-9e2a06a6f794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Rep 1: training start ******\n",
      "Epoch:   1 train_loss: 1.8326 train_acc: 0.2071 val_loss: 2.1013 val_acc: 0.1860 test_loss: 2.0980 test_acc: 0.1670\n",
      "Epoch:   1 train_loss: 1.8326 train_acc: 0.2071 val_loss: 2.0159 val_acc: 0.1100 test_loss: 2.0980 test_acc: 0.1670\n",
      "Epoch:   1 train_loss: 1.8326 train_acc: 0.2071 val_loss: 2.0159 val_acc: 0.1100 test_loss: 1.9973 test_acc: 0.1100\n",
      "Epoch:   2 train_loss: 1.7688 train_acc: 0.5214 val_loss: 1.8286 val_acc: 0.2000 test_loss: 1.8354 test_acc: 0.1880\n",
      "Epoch:   2 train_loss: 1.7688 train_acc: 0.5214 val_loss: 1.8367 val_acc: 0.3400 test_loss: 1.8354 test_acc: 0.1880\n",
      "Epoch:   2 train_loss: 1.7688 train_acc: 0.5214 val_loss: 1.8367 val_acc: 0.3400 test_loss: 1.8352 test_acc: 0.3700\n",
      "Epoch:   3 train_loss: 1.7334 train_acc: 0.4714 val_loss: 1.8060 val_acc: 0.2800 test_loss: 1.8047 test_acc: 0.2870\n",
      "Epoch:   3 train_loss: 1.7334 train_acc: 0.4714 val_loss: 1.7660 val_acc: 0.3220 test_loss: 1.8047 test_acc: 0.2870\n",
      "Epoch:   3 train_loss: 1.7334 train_acc: 0.4714 val_loss: 1.7660 val_acc: 0.3220 test_loss: 1.7680 test_acc: 0.3490\n",
      "Epoch:   4 train_loss: 1.6743 train_acc: 0.5071 val_loss: 1.8244 val_acc: 0.2520 test_loss: 1.8152 test_acc: 0.2520\n",
      "Epoch:   4 train_loss: 1.6743 train_acc: 0.5071 val_loss: 1.7133 val_acc: 0.4220 test_loss: 1.8152 test_acc: 0.2520\n",
      "Epoch:   4 train_loss: 1.6743 train_acc: 0.5071 val_loss: 1.7133 val_acc: 0.4220 test_loss: 1.7128 test_acc: 0.4460\n",
      "Epoch:   5 train_loss: 1.5591 train_acc: 0.5714 val_loss: 1.7301 val_acc: 0.3240 test_loss: 1.7194 test_acc: 0.3240\n",
      "Epoch:   5 train_loss: 1.5591 train_acc: 0.5714 val_loss: 1.6685 val_acc: 0.4540 test_loss: 1.7194 test_acc: 0.3240\n",
      "Epoch:   5 train_loss: 1.5591 train_acc: 0.5714 val_loss: 1.6685 val_acc: 0.4540 test_loss: 1.6603 test_acc: 0.4750\n",
      "Epoch:   6 train_loss: 1.5079 train_acc: 0.5929 val_loss: 1.6575 val_acc: 0.4460 test_loss: 1.6456 test_acc: 0.4620\n",
      "Epoch:   6 train_loss: 1.5079 train_acc: 0.5929 val_loss: 1.6695 val_acc: 0.4300 test_loss: 1.6456 test_acc: 0.4620\n",
      "Epoch:   6 train_loss: 1.5079 train_acc: 0.5929 val_loss: 1.6695 val_acc: 0.4300 test_loss: 1.6560 test_acc: 0.4500\n",
      "Epoch:   7 train_loss: 1.4628 train_acc: 0.6214 val_loss: 1.6450 val_acc: 0.3780 test_loss: 1.6351 test_acc: 0.3780\n",
      "Epoch:   7 train_loss: 1.4628 train_acc: 0.6214 val_loss: 1.6638 val_acc: 0.4760 test_loss: 1.6351 test_acc: 0.3780\n",
      "Epoch:   7 train_loss: 1.4628 train_acc: 0.6214 val_loss: 1.6638 val_acc: 0.4760 test_loss: 1.6473 test_acc: 0.4620\n",
      "Epoch:   8 train_loss: 1.4206 train_acc: 0.6857 val_loss: 1.6252 val_acc: 0.3760 test_loss: 1.6191 test_acc: 0.3600\n",
      "Epoch:   8 train_loss: 1.4206 train_acc: 0.6857 val_loss: 1.6599 val_acc: 0.5060 test_loss: 1.6191 test_acc: 0.3600\n",
      "Epoch:   8 train_loss: 1.4206 train_acc: 0.6857 val_loss: 1.6599 val_acc: 0.5060 test_loss: 1.6423 test_acc: 0.4960\n",
      "Epoch:   9 train_loss: 1.3825 train_acc: 0.6643 val_loss: 1.5838 val_acc: 0.4120 test_loss: 1.5819 test_acc: 0.3950\n",
      "Epoch:   9 train_loss: 1.3825 train_acc: 0.6643 val_loss: 1.6567 val_acc: 0.4860 test_loss: 1.5819 test_acc: 0.3950\n",
      "Epoch:   9 train_loss: 1.3825 train_acc: 0.6643 val_loss: 1.6567 val_acc: 0.4860 test_loss: 1.6391 test_acc: 0.4900\n",
      "Epoch:  10 train_loss: 1.3398 train_acc: 0.6786 val_loss: 1.5601 val_acc: 0.4360 test_loss: 1.5651 test_acc: 0.4620\n",
      "Epoch:  10 train_loss: 1.3398 train_acc: 0.6786 val_loss: 1.6344 val_acc: 0.4860 test_loss: 1.5651 test_acc: 0.4620\n",
      "Epoch:  10 train_loss: 1.3398 train_acc: 0.6786 val_loss: 1.6344 val_acc: 0.4860 test_loss: 1.6175 test_acc: 0.4690\n",
      "Epoch:  11 train_loss: 1.2979 train_acc: 0.6786 val_loss: 1.5334 val_acc: 0.4580 test_loss: 1.5450 test_acc: 0.4770\n",
      "Epoch:  11 train_loss: 1.2979 train_acc: 0.6786 val_loss: 1.6037 val_acc: 0.4700 test_loss: 1.5450 test_acc: 0.4770\n",
      "Epoch:  11 train_loss: 1.2979 train_acc: 0.6786 val_loss: 1.6037 val_acc: 0.4700 test_loss: 1.5857 test_acc: 0.4620\n",
      "Epoch:  12 train_loss: 1.2501 train_acc: 0.7000 val_loss: 1.4991 val_acc: 0.4700 test_loss: 1.5155 test_acc: 0.4640\n",
      "Epoch:  12 train_loss: 1.2501 train_acc: 0.7000 val_loss: 1.5655 val_acc: 0.4760 test_loss: 1.5155 test_acc: 0.4640\n",
      "Epoch:  12 train_loss: 1.2501 train_acc: 0.7000 val_loss: 1.5655 val_acc: 0.4760 test_loss: 1.5450 test_acc: 0.4580\n",
      "Epoch:  13 train_loss: 1.2034 train_acc: 0.7071 val_loss: 1.4574 val_acc: 0.5020 test_loss: 1.4732 test_acc: 0.4960\n",
      "Epoch:  13 train_loss: 1.2034 train_acc: 0.7071 val_loss: 1.5234 val_acc: 0.5040 test_loss: 1.4732 test_acc: 0.4960\n",
      "Epoch:  13 train_loss: 1.2034 train_acc: 0.7071 val_loss: 1.5234 val_acc: 0.5040 test_loss: 1.5018 test_acc: 0.4860\n",
      "Epoch:  14 train_loss: 1.1505 train_acc: 0.7786 val_loss: 1.4096 val_acc: 0.5420 test_loss: 1.4240 test_acc: 0.5370\n",
      "Epoch:  14 train_loss: 1.1505 train_acc: 0.7786 val_loss: 1.4705 val_acc: 0.5700 test_loss: 1.4240 test_acc: 0.5370\n",
      "Epoch:  14 train_loss: 1.1505 train_acc: 0.7786 val_loss: 1.4705 val_acc: 0.5700 test_loss: 1.4492 test_acc: 0.5590\n",
      "Epoch:  15 train_loss: 1.1045 train_acc: 0.7857 val_loss: 1.3631 val_acc: 0.6000 test_loss: 1.3773 test_acc: 0.5850\n",
      "Epoch:  15 train_loss: 1.1045 train_acc: 0.7857 val_loss: 1.4346 val_acc: 0.5820 test_loss: 1.3773 test_acc: 0.5850\n",
      "Epoch:  15 train_loss: 1.1045 train_acc: 0.7857 val_loss: 1.4346 val_acc: 0.5820 test_loss: 1.4125 test_acc: 0.5770\n",
      "Epoch:  16 train_loss: 1.0621 train_acc: 0.8286 val_loss: 1.3299 val_acc: 0.5920 test_loss: 1.3445 test_acc: 0.5980\n",
      "Epoch:  16 train_loss: 1.0621 train_acc: 0.8286 val_loss: 1.4171 val_acc: 0.5940 test_loss: 1.3445 test_acc: 0.5980\n",
      "Epoch:  16 train_loss: 1.0621 train_acc: 0.8286 val_loss: 1.4171 val_acc: 0.5940 test_loss: 1.3930 test_acc: 0.5960\n",
      "Epoch:  17 train_loss: 1.0217 train_acc: 0.8429 val_loss: 1.3269 val_acc: 0.5640 test_loss: 1.3452 test_acc: 0.5720\n",
      "Epoch:  17 train_loss: 1.0217 train_acc: 0.8429 val_loss: 1.4052 val_acc: 0.5840 test_loss: 1.3452 test_acc: 0.5720\n",
      "Epoch:  17 train_loss: 1.0217 train_acc: 0.8429 val_loss: 1.4052 val_acc: 0.5840 test_loss: 1.3777 test_acc: 0.5990\n",
      "Epoch:  18 train_loss: 0.9879 train_acc: 0.8143 val_loss: 1.3345 val_acc: 0.5240 test_loss: 1.3591 test_acc: 0.5470\n",
      "Epoch:  18 train_loss: 0.9879 train_acc: 0.8143 val_loss: 1.4033 val_acc: 0.5740 test_loss: 1.3591 test_acc: 0.5470\n",
      "Epoch:  18 train_loss: 0.9879 train_acc: 0.8143 val_loss: 1.4033 val_acc: 0.5740 test_loss: 1.3706 test_acc: 0.5740\n",
      "Epoch:  19 train_loss: 0.9569 train_acc: 0.7786 val_loss: 1.3509 val_acc: 0.5260 test_loss: 1.3810 test_acc: 0.5310\n",
      "Epoch:  19 train_loss: 0.9569 train_acc: 0.7786 val_loss: 1.4017 val_acc: 0.5840 test_loss: 1.3810 test_acc: 0.5310\n",
      "Epoch:  19 train_loss: 0.9569 train_acc: 0.7786 val_loss: 1.4017 val_acc: 0.5840 test_loss: 1.3635 test_acc: 0.5620\n",
      "Epoch:  20 train_loss: 0.9279 train_acc: 0.7929 val_loss: 1.3222 val_acc: 0.5840 test_loss: 1.3556 test_acc: 0.5830\n",
      "Epoch:  20 train_loss: 0.9279 train_acc: 0.7929 val_loss: 1.3827 val_acc: 0.6000 test_loss: 1.3556 test_acc: 0.5830\n",
      "Epoch:  20 train_loss: 0.9279 train_acc: 0.7929 val_loss: 1.3827 val_acc: 0.6000 test_loss: 1.3429 test_acc: 0.5820\n",
      "Epoch:  21 train_loss: 0.9010 train_acc: 0.8071 val_loss: 1.2535 val_acc: 0.6300 test_loss: 1.2858 test_acc: 0.6140\n",
      "Epoch:  21 train_loss: 0.9010 train_acc: 0.8071 val_loss: 1.3641 val_acc: 0.5900 test_loss: 1.2858 test_acc: 0.6140\n",
      "Epoch:  21 train_loss: 0.9010 train_acc: 0.8071 val_loss: 1.3641 val_acc: 0.5900 test_loss: 1.3230 test_acc: 0.5920\n",
      "Epoch:  22 train_loss: 0.8758 train_acc: 0.8214 val_loss: 1.2036 val_acc: 0.6560 test_loss: 1.2320 test_acc: 0.6440\n",
      "Epoch:  22 train_loss: 0.8758 train_acc: 0.8214 val_loss: 1.3371 val_acc: 0.5840 test_loss: 1.2320 test_acc: 0.6440\n",
      "Epoch:  22 train_loss: 0.8758 train_acc: 0.8214 val_loss: 1.3371 val_acc: 0.5840 test_loss: 1.2976 test_acc: 0.5960\n",
      "Epoch:  23 train_loss: 0.8560 train_acc: 0.8214 val_loss: 1.1616 val_acc: 0.6980 test_loss: 1.1856 test_acc: 0.6830\n",
      "Epoch:  23 train_loss: 0.8560 train_acc: 0.8214 val_loss: 1.3048 val_acc: 0.5960 test_loss: 1.1856 test_acc: 0.6830\n",
      "Epoch:  23 train_loss: 0.8560 train_acc: 0.8214 val_loss: 1.3048 val_acc: 0.5960 test_loss: 1.2690 test_acc: 0.5940\n",
      "Epoch:  24 train_loss: 0.8444 train_acc: 0.8643 val_loss: 1.1366 val_acc: 0.7060 test_loss: 1.1580 test_acc: 0.7130\n",
      "Epoch:  24 train_loss: 0.8444 train_acc: 0.8643 val_loss: 1.2910 val_acc: 0.6520 test_loss: 1.1580 test_acc: 0.7130\n",
      "Epoch:  24 train_loss: 0.8444 train_acc: 0.8643 val_loss: 1.2910 val_acc: 0.6520 test_loss: 1.2557 test_acc: 0.6470\n",
      "Epoch:  25 train_loss: 0.8326 train_acc: 0.8786 val_loss: 1.1212 val_acc: 0.7220 test_loss: 1.1416 test_acc: 0.7220\n",
      "Epoch:  25 train_loss: 0.8326 train_acc: 0.8786 val_loss: 1.2798 val_acc: 0.6620 test_loss: 1.1416 test_acc: 0.7220\n",
      "Epoch:  25 train_loss: 0.8326 train_acc: 0.8786 val_loss: 1.2798 val_acc: 0.6620 test_loss: 1.2444 test_acc: 0.6630\n",
      "Epoch:  26 train_loss: 0.8194 train_acc: 0.8714 val_loss: 1.1132 val_acc: 0.7360 test_loss: 1.1260 test_acc: 0.7420\n",
      "Epoch:  26 train_loss: 0.8194 train_acc: 0.8714 val_loss: 1.2675 val_acc: 0.6420 test_loss: 1.1260 test_acc: 0.7420\n",
      "Epoch:  26 train_loss: 0.8194 train_acc: 0.8714 val_loss: 1.2675 val_acc: 0.6420 test_loss: 1.2318 test_acc: 0.6760\n",
      "Epoch:  27 train_loss: 0.8023 train_acc: 0.8786 val_loss: 1.1161 val_acc: 0.7540 test_loss: 1.1226 test_acc: 0.7460\n",
      "Epoch:  27 train_loss: 0.8023 train_acc: 0.8786 val_loss: 1.2562 val_acc: 0.6480 test_loss: 1.1226 test_acc: 0.7460\n",
      "Epoch:  27 train_loss: 0.8023 train_acc: 0.8786 val_loss: 1.2562 val_acc: 0.6480 test_loss: 1.2199 test_acc: 0.6820\n",
      "Epoch:  28 train_loss: 0.7820 train_acc: 0.8857 val_loss: 1.1109 val_acc: 0.7640 test_loss: 1.1125 test_acc: 0.7850\n",
      "Epoch:  28 train_loss: 0.7820 train_acc: 0.8857 val_loss: 1.2459 val_acc: 0.6720 test_loss: 1.1125 test_acc: 0.7850\n",
      "Epoch:  28 train_loss: 0.7820 train_acc: 0.8857 val_loss: 1.2459 val_acc: 0.6720 test_loss: 1.2100 test_acc: 0.6890\n",
      "Epoch:  29 train_loss: 0.7602 train_acc: 0.9000 val_loss: 1.1138 val_acc: 0.7540 test_loss: 1.1128 test_acc: 0.7780\n",
      "Epoch:  29 train_loss: 0.7602 train_acc: 0.9000 val_loss: 1.2333 val_acc: 0.7040 test_loss: 1.1128 test_acc: 0.7780\n",
      "Epoch:  29 train_loss: 0.7602 train_acc: 0.9000 val_loss: 1.2333 val_acc: 0.7040 test_loss: 1.1975 test_acc: 0.7190\n",
      "Epoch:  30 train_loss: 0.7393 train_acc: 0.9143 val_loss: 1.1326 val_acc: 0.7360 test_loss: 1.1316 test_acc: 0.7670\n",
      "Epoch:  30 train_loss: 0.7393 train_acc: 0.9143 val_loss: 1.2097 val_acc: 0.7380 test_loss: 1.1316 test_acc: 0.7670\n",
      "Epoch:  30 train_loss: 0.7393 train_acc: 0.9143 val_loss: 1.2097 val_acc: 0.7380 test_loss: 1.1759 test_acc: 0.7430\n",
      "Epoch:  31 train_loss: 0.7173 train_acc: 0.9214 val_loss: 1.1563 val_acc: 0.6980 test_loss: 1.1561 test_acc: 0.7280\n",
      "Epoch:  31 train_loss: 0.7173 train_acc: 0.9214 val_loss: 1.1812 val_acc: 0.7500 test_loss: 1.1561 test_acc: 0.7280\n",
      "Epoch:  31 train_loss: 0.7173 train_acc: 0.9214 val_loss: 1.1812 val_acc: 0.7500 test_loss: 1.1507 test_acc: 0.7560\n",
      "Epoch:  32 train_loss: 0.7049 train_acc: 0.9286 val_loss: 1.1663 val_acc: 0.7060 test_loss: 1.1689 test_acc: 0.7150\n",
      "Epoch:  32 train_loss: 0.7049 train_acc: 0.9286 val_loss: 1.1684 val_acc: 0.7500 test_loss: 1.1689 test_acc: 0.7150\n",
      "Epoch:  32 train_loss: 0.7049 train_acc: 0.9286 val_loss: 1.1684 val_acc: 0.7500 test_loss: 1.1388 test_acc: 0.7490\n",
      "Epoch:  33 train_loss: 0.7016 train_acc: 0.9286 val_loss: 1.1763 val_acc: 0.7040 test_loss: 1.1786 test_acc: 0.7010\n",
      "Epoch:  33 train_loss: 0.7016 train_acc: 0.9286 val_loss: 1.1616 val_acc: 0.7440 test_loss: 1.1786 test_acc: 0.7010\n",
      "Epoch:  33 train_loss: 0.7016 train_acc: 0.9286 val_loss: 1.1616 val_acc: 0.7440 test_loss: 1.1322 test_acc: 0.7530\n",
      "Epoch:  34 train_loss: 0.6889 train_acc: 0.9429 val_loss: 1.1776 val_acc: 0.6820 test_loss: 1.1770 test_acc: 0.6760\n",
      "Epoch:  34 train_loss: 0.6889 train_acc: 0.9429 val_loss: 1.1389 val_acc: 0.7660 test_loss: 1.1770 test_acc: 0.6760\n",
      "Epoch:  34 train_loss: 0.6889 train_acc: 0.9429 val_loss: 1.1389 val_acc: 0.7660 test_loss: 1.1106 test_acc: 0.7680\n",
      "Epoch:  35 train_loss: 0.6857 train_acc: 0.9571 val_loss: 1.1507 val_acc: 0.7180 test_loss: 1.1471 test_acc: 0.7020\n",
      "Epoch:  35 train_loss: 0.6857 train_acc: 0.9571 val_loss: 1.1265 val_acc: 0.7860 test_loss: 1.1471 test_acc: 0.7020\n",
      "Epoch:  35 train_loss: 0.6857 train_acc: 0.9571 val_loss: 1.1265 val_acc: 0.7860 test_loss: 1.0982 test_acc: 0.7790\n",
      "Epoch:  36 train_loss: 0.6909 train_acc: 0.9500 val_loss: 1.1054 val_acc: 0.7520 test_loss: 1.0974 test_acc: 0.7450\n",
      "Epoch:  36 train_loss: 0.6909 train_acc: 0.9500 val_loss: 1.1220 val_acc: 0.7700 test_loss: 1.0974 test_acc: 0.7450\n",
      "Epoch:  36 train_loss: 0.6909 train_acc: 0.9500 val_loss: 1.1220 val_acc: 0.7700 test_loss: 1.0925 test_acc: 0.7660\n",
      "Epoch:  37 train_loss: 0.6841 train_acc: 0.9500 val_loss: 1.0625 val_acc: 0.7500 test_loss: 1.0494 test_acc: 0.7770\n",
      "Epoch:  37 train_loss: 0.6841 train_acc: 0.9500 val_loss: 1.1114 val_acc: 0.7700 test_loss: 1.0494 test_acc: 0.7770\n",
      "Epoch:  37 train_loss: 0.6841 train_acc: 0.9500 val_loss: 1.1114 val_acc: 0.7700 test_loss: 1.0801 test_acc: 0.7780\n",
      "Epoch:  38 train_loss: 0.6662 train_acc: 0.9286 val_loss: 1.0261 val_acc: 0.7500 test_loss: 1.0099 test_acc: 0.7760\n",
      "Epoch:  38 train_loss: 0.6662 train_acc: 0.9286 val_loss: 1.0990 val_acc: 0.7760 test_loss: 1.0099 test_acc: 0.7760\n",
      "Epoch:  38 train_loss: 0.6662 train_acc: 0.9286 val_loss: 1.0990 val_acc: 0.7760 test_loss: 1.0648 test_acc: 0.7790\n",
      "Epoch:  39 train_loss: 0.6501 train_acc: 0.9214 val_loss: 1.0109 val_acc: 0.7300 test_loss: 0.9911 test_acc: 0.7690\n",
      "Epoch:  39 train_loss: 0.6501 train_acc: 0.9214 val_loss: 1.0932 val_acc: 0.7700 test_loss: 0.9911 test_acc: 0.7690\n",
      "Epoch:  39 train_loss: 0.6501 train_acc: 0.9214 val_loss: 1.0932 val_acc: 0.7700 test_loss: 1.0563 test_acc: 0.7760\n",
      "Epoch:  40 train_loss: 0.6284 train_acc: 0.9357 val_loss: 1.0103 val_acc: 0.7280 test_loss: 0.9894 test_acc: 0.7680\n",
      "Epoch:  40 train_loss: 0.6284 train_acc: 0.9357 val_loss: 1.0860 val_acc: 0.7640 test_loss: 0.9894 test_acc: 0.7680\n",
      "Epoch:  40 train_loss: 0.6284 train_acc: 0.9357 val_loss: 1.0860 val_acc: 0.7640 test_loss: 1.0464 test_acc: 0.7730\n",
      "Epoch:  41 train_loss: 0.6055 train_acc: 0.9571 val_loss: 1.0054 val_acc: 0.7440 test_loss: 0.9857 test_acc: 0.7750\n",
      "Epoch:  41 train_loss: 0.6055 train_acc: 0.9571 val_loss: 1.0771 val_acc: 0.7540 test_loss: 0.9857 test_acc: 0.7750\n",
      "Epoch:  41 train_loss: 0.6055 train_acc: 0.9571 val_loss: 1.0771 val_acc: 0.7540 test_loss: 1.0350 test_acc: 0.7710\n",
      "Epoch:  42 train_loss: 0.5828 train_acc: 0.9643 val_loss: 1.0130 val_acc: 0.7340 test_loss: 0.9963 test_acc: 0.7600\n",
      "Epoch:  42 train_loss: 0.5828 train_acc: 0.9643 val_loss: 1.0690 val_acc: 0.7560 test_loss: 0.9963 test_acc: 0.7600\n",
      "Epoch:  42 train_loss: 0.5828 train_acc: 0.9643 val_loss: 1.0690 val_acc: 0.7560 test_loss: 1.0240 test_acc: 0.7700\n",
      "Epoch:  43 train_loss: 0.5572 train_acc: 0.9714 val_loss: 1.0189 val_acc: 0.7360 test_loss: 1.0056 test_acc: 0.7510\n",
      "Epoch:  43 train_loss: 0.5572 train_acc: 0.9714 val_loss: 1.0573 val_acc: 0.7640 test_loss: 1.0056 test_acc: 0.7510\n",
      "Epoch:  43 train_loss: 0.5572 train_acc: 0.9714 val_loss: 1.0573 val_acc: 0.7640 test_loss: 1.0088 test_acc: 0.7740\n",
      "Epoch:  44 train_loss: 0.5319 train_acc: 0.9643 val_loss: 1.0124 val_acc: 0.7340 test_loss: 1.0018 test_acc: 0.7510\n",
      "Epoch:  44 train_loss: 0.5319 train_acc: 0.9643 val_loss: 1.0444 val_acc: 0.7780 test_loss: 1.0018 test_acc: 0.7510\n",
      "Epoch:  44 train_loss: 0.5319 train_acc: 0.9643 val_loss: 1.0444 val_acc: 0.7780 test_loss: 0.9924 test_acc: 0.7820\n",
      "Epoch:  45 train_loss: 0.5087 train_acc: 0.9571 val_loss: 0.9865 val_acc: 0.7540 test_loss: 0.9763 test_acc: 0.7610\n",
      "Epoch:  45 train_loss: 0.5087 train_acc: 0.9571 val_loss: 1.0297 val_acc: 0.7700 test_loss: 0.9763 test_acc: 0.7610\n",
      "Epoch:  45 train_loss: 0.5087 train_acc: 0.9571 val_loss: 1.0297 val_acc: 0.7700 test_loss: 0.9742 test_acc: 0.7770\n",
      "Epoch:  46 train_loss: 0.4892 train_acc: 0.9500 val_loss: 0.9765 val_acc: 0.7580 test_loss: 0.9635 test_acc: 0.7700\n",
      "Epoch:  46 train_loss: 0.4892 train_acc: 0.9500 val_loss: 1.0184 val_acc: 0.7640 test_loss: 0.9635 test_acc: 0.7700\n",
      "Epoch:  46 train_loss: 0.4892 train_acc: 0.9500 val_loss: 1.0184 val_acc: 0.7640 test_loss: 0.9595 test_acc: 0.7690\n",
      "Epoch:  47 train_loss: 0.4715 train_acc: 0.9429 val_loss: 0.9477 val_acc: 0.7800 test_loss: 0.9304 test_acc: 0.7730\n",
      "Epoch:  47 train_loss: 0.4715 train_acc: 0.9429 val_loss: 1.0029 val_acc: 0.7580 test_loss: 0.9304 test_acc: 0.7730\n",
      "Epoch:  47 train_loss: 0.4715 train_acc: 0.9429 val_loss: 1.0029 val_acc: 0.7580 test_loss: 0.9418 test_acc: 0.7610\n",
      "Epoch:  48 train_loss: 0.4443 train_acc: 0.9429 val_loss: 0.9346 val_acc: 0.7760 test_loss: 0.9106 test_acc: 0.7860\n",
      "Epoch:  48 train_loss: 0.4443 train_acc: 0.9429 val_loss: 0.9720 val_acc: 0.7800 test_loss: 0.9106 test_acc: 0.7860\n",
      "Epoch:  48 train_loss: 0.4443 train_acc: 0.9429 val_loss: 0.9720 val_acc: 0.7800 test_loss: 0.9077 test_acc: 0.7670\n",
      "Epoch:  49 train_loss: 0.4237 train_acc: 0.9571 val_loss: 0.9333 val_acc: 0.7680 test_loss: 0.9050 test_acc: 0.7800\n",
      "Epoch:  49 train_loss: 0.4237 train_acc: 0.9571 val_loss: 0.9467 val_acc: 0.7880 test_loss: 0.9050 test_acc: 0.7800\n",
      "Epoch:  49 train_loss: 0.4237 train_acc: 0.9571 val_loss: 0.9467 val_acc: 0.7880 test_loss: 0.8804 test_acc: 0.7850\n",
      "Epoch:  50 train_loss: 0.4125 train_acc: 0.9857 val_loss: 0.9403 val_acc: 0.7500 test_loss: 0.9106 test_acc: 0.7580\n",
      "Epoch:  50 train_loss: 0.4125 train_acc: 0.9857 val_loss: 0.9355 val_acc: 0.7940 test_loss: 0.9106 test_acc: 0.7580\n",
      "Epoch:  50 train_loss: 0.4125 train_acc: 0.9857 val_loss: 0.9355 val_acc: 0.7940 test_loss: 0.8663 test_acc: 0.7900\n",
      "Epoch:  51 train_loss: 0.4086 train_acc: 0.9786 val_loss: 0.9431 val_acc: 0.7500 test_loss: 0.9122 test_acc: 0.7590\n",
      "Epoch:  51 train_loss: 0.4086 train_acc: 0.9786 val_loss: 0.9313 val_acc: 0.7920 test_loss: 0.9122 test_acc: 0.7590\n",
      "Epoch:  51 train_loss: 0.4086 train_acc: 0.9786 val_loss: 0.9313 val_acc: 0.7920 test_loss: 0.8593 test_acc: 0.7930\n",
      "Epoch:  52 train_loss: 0.4117 train_acc: 0.9571 val_loss: 0.9796 val_acc: 0.7440 test_loss: 0.9521 test_acc: 0.7560\n",
      "Epoch:  52 train_loss: 0.4117 train_acc: 0.9571 val_loss: 0.9410 val_acc: 0.7880 test_loss: 0.9521 test_acc: 0.7560\n",
      "Epoch:  52 train_loss: 0.4117 train_acc: 0.9571 val_loss: 0.9410 val_acc: 0.7880 test_loss: 0.8646 test_acc: 0.7930\n",
      "Epoch:  53 train_loss: 0.4031 train_acc: 0.9571 val_loss: 0.9869 val_acc: 0.7440 test_loss: 0.9631 test_acc: 0.7430\n",
      "Epoch:  53 train_loss: 0.4031 train_acc: 0.9571 val_loss: 0.9394 val_acc: 0.7780 test_loss: 0.9631 test_acc: 0.7430\n",
      "Epoch:  53 train_loss: 0.4031 train_acc: 0.9571 val_loss: 0.9394 val_acc: 0.7780 test_loss: 0.8614 test_acc: 0.7820\n",
      "Epoch:  54 train_loss: 0.3983 train_acc: 0.9643 val_loss: 0.9870 val_acc: 0.7140 test_loss: 0.9665 test_acc: 0.7340\n",
      "Epoch:  54 train_loss: 0.3983 train_acc: 0.9643 val_loss: 0.9465 val_acc: 0.7640 test_loss: 0.9665 test_acc: 0.7340\n",
      "Epoch:  54 train_loss: 0.3983 train_acc: 0.9643 val_loss: 0.9465 val_acc: 0.7640 test_loss: 0.8675 test_acc: 0.7670\n",
      "Epoch:  55 train_loss: 0.3899 train_acc: 0.9786 val_loss: 1.0128 val_acc: 0.7020 test_loss: 0.9915 test_acc: 0.7220\n",
      "Epoch:  55 train_loss: 0.3899 train_acc: 0.9786 val_loss: 0.9558 val_acc: 0.7480 test_loss: 0.9915 test_acc: 0.7220\n",
      "Epoch:  55 train_loss: 0.3899 train_acc: 0.9786 val_loss: 0.9558 val_acc: 0.7480 test_loss: 0.8775 test_acc: 0.7580\n",
      "Epoch:  56 train_loss: 0.3676 train_acc: 0.9786 val_loss: 0.9750 val_acc: 0.7120 test_loss: 0.9485 test_acc: 0.7460\n",
      "Epoch:  56 train_loss: 0.3676 train_acc: 0.9786 val_loss: 0.9460 val_acc: 0.7500 test_loss: 0.9485 test_acc: 0.7460\n",
      "Epoch:  56 train_loss: 0.3676 train_acc: 0.9786 val_loss: 0.9460 val_acc: 0.7500 test_loss: 0.8688 test_acc: 0.7520\n",
      "Epoch:  57 train_loss: 0.3426 train_acc: 0.9714 val_loss: 0.9237 val_acc: 0.7400 test_loss: 0.8940 test_acc: 0.7600\n",
      "Epoch:  57 train_loss: 0.3426 train_acc: 0.9714 val_loss: 0.9235 val_acc: 0.7620 test_loss: 0.8940 test_acc: 0.7600\n",
      "Epoch:  57 train_loss: 0.3426 train_acc: 0.9714 val_loss: 0.9235 val_acc: 0.7620 test_loss: 0.8476 test_acc: 0.7670\n",
      "Epoch:  58 train_loss: 0.3201 train_acc: 0.9786 val_loss: 0.9544 val_acc: 0.7140 test_loss: 0.9147 test_acc: 0.7270\n",
      "Epoch:  58 train_loss: 0.3201 train_acc: 0.9786 val_loss: 0.8863 val_acc: 0.7780 test_loss: 0.9147 test_acc: 0.7270\n",
      "Epoch:  58 train_loss: 0.3201 train_acc: 0.9786 val_loss: 0.8863 val_acc: 0.7780 test_loss: 0.8122 test_acc: 0.7840\n",
      "Epoch:  59 train_loss: 0.3045 train_acc: 0.9786 val_loss: 1.0343 val_acc: 0.6640 test_loss: 0.9849 test_acc: 0.6890\n",
      "Epoch:  59 train_loss: 0.3045 train_acc: 0.9786 val_loss: 0.8616 val_acc: 0.7880 test_loss: 0.9849 test_acc: 0.6890\n",
      "Epoch:  59 train_loss: 0.3045 train_acc: 0.9786 val_loss: 0.8616 val_acc: 0.7880 test_loss: 0.7879 test_acc: 0.7890\n",
      "Epoch:  60 train_loss: 0.2959 train_acc: 0.9643 val_loss: 1.0245 val_acc: 0.7080 test_loss: 0.9706 test_acc: 0.7350\n",
      "Epoch:  60 train_loss: 0.2959 train_acc: 0.9643 val_loss: 0.8419 val_acc: 0.7900 test_loss: 0.9706 test_acc: 0.7350\n",
      "Epoch:  60 train_loss: 0.2959 train_acc: 0.9643 val_loss: 0.8419 val_acc: 0.7900 test_loss: 0.7676 test_acc: 0.7840\n",
      "Epoch:  61 train_loss: 0.3022 train_acc: 0.9571 val_loss: 1.0223 val_acc: 0.7060 test_loss: 0.9696 test_acc: 0.7450\n",
      "Epoch:  61 train_loss: 0.3022 train_acc: 0.9571 val_loss: 0.8565 val_acc: 0.7780 test_loss: 0.9696 test_acc: 0.7450\n",
      "Epoch:  61 train_loss: 0.3022 train_acc: 0.9571 val_loss: 0.8565 val_acc: 0.7780 test_loss: 0.7802 test_acc: 0.7730\n",
      "Epoch:  62 train_loss: 0.2997 train_acc: 0.9571 val_loss: 1.0268 val_acc: 0.6720 test_loss: 0.9804 test_acc: 0.6940\n",
      "Epoch:  62 train_loss: 0.2997 train_acc: 0.9571 val_loss: 0.8744 val_acc: 0.7720 test_loss: 0.9804 test_acc: 0.6940\n",
      "Epoch:  62 train_loss: 0.2997 train_acc: 0.9571 val_loss: 0.8744 val_acc: 0.7720 test_loss: 0.7934 test_acc: 0.7640\n",
      "Epoch:  63 train_loss: 0.2890 train_acc: 0.9571 val_loss: 0.9915 val_acc: 0.7000 test_loss: 0.9508 test_acc: 0.7190\n",
      "Epoch:  63 train_loss: 0.2890 train_acc: 0.9571 val_loss: 0.8842 val_acc: 0.7600 test_loss: 0.9508 test_acc: 0.7190\n",
      "Epoch:  63 train_loss: 0.2890 train_acc: 0.9571 val_loss: 0.8842 val_acc: 0.7600 test_loss: 0.7966 test_acc: 0.7680\n",
      "Epoch:  64 train_loss: 0.2720 train_acc: 0.9786 val_loss: 0.9614 val_acc: 0.7580 test_loss: 0.9236 test_acc: 0.7610\n",
      "Epoch:  64 train_loss: 0.2720 train_acc: 0.9786 val_loss: 0.8619 val_acc: 0.7740 test_loss: 0.9236 test_acc: 0.7610\n",
      "Epoch:  64 train_loss: 0.2720 train_acc: 0.9786 val_loss: 0.8619 val_acc: 0.7740 test_loss: 0.7725 test_acc: 0.7850\n",
      "Epoch:  65 train_loss: 0.2743 train_acc: 0.9643 val_loss: 0.9753 val_acc: 0.7520 test_loss: 0.9426 test_acc: 0.7530\n",
      "Epoch:  65 train_loss: 0.2743 train_acc: 0.9643 val_loss: 0.8420 val_acc: 0.7800 test_loss: 0.9426 test_acc: 0.7530\n",
      "Epoch:  65 train_loss: 0.2743 train_acc: 0.9643 val_loss: 0.8420 val_acc: 0.7800 test_loss: 0.7531 test_acc: 0.7990\n",
      "Epoch:  66 train_loss: 0.2862 train_acc: 0.9571 val_loss: 0.9556 val_acc: 0.7540 test_loss: 0.9283 test_acc: 0.7450\n",
      "Epoch:  66 train_loss: 0.2862 train_acc: 0.9571 val_loss: 0.8311 val_acc: 0.7920 test_loss: 0.9283 test_acc: 0.7450\n",
      "Epoch:  66 train_loss: 0.2862 train_acc: 0.9571 val_loss: 0.8311 val_acc: 0.7920 test_loss: 0.7441 test_acc: 0.7980\n",
      "Epoch:  67 train_loss: 0.2907 train_acc: 0.9571 val_loss: 0.9515 val_acc: 0.7360 test_loss: 0.9283 test_acc: 0.7400\n",
      "Epoch:  67 train_loss: 0.2907 train_acc: 0.9571 val_loss: 0.8194 val_acc: 0.8020 test_loss: 0.9283 test_acc: 0.7400\n",
      "Epoch:  67 train_loss: 0.2907 train_acc: 0.9571 val_loss: 0.8194 val_acc: 0.8020 test_loss: 0.7344 test_acc: 0.7980\n",
      "Epoch:  68 train_loss: 0.2771 train_acc: 0.9571 val_loss: 0.9561 val_acc: 0.7520 test_loss: 0.9327 test_acc: 0.7570\n",
      "Epoch:  68 train_loss: 0.2771 train_acc: 0.9571 val_loss: 0.8057 val_acc: 0.8000 test_loss: 0.9327 test_acc: 0.7570\n",
      "Epoch:  68 train_loss: 0.2771 train_acc: 0.9571 val_loss: 0.8057 val_acc: 0.8000 test_loss: 0.7201 test_acc: 0.8010\n",
      "Epoch:  69 train_loss: 0.2505 train_acc: 0.9643 val_loss: 0.9249 val_acc: 0.7620 test_loss: 0.8974 test_acc: 0.7780\n",
      "Epoch:  69 train_loss: 0.2505 train_acc: 0.9643 val_loss: 0.7937 val_acc: 0.8020 test_loss: 0.8974 test_acc: 0.7780\n",
      "Epoch:  69 train_loss: 0.2505 train_acc: 0.9643 val_loss: 0.7937 val_acc: 0.8020 test_loss: 0.7062 test_acc: 0.8050\n",
      "Epoch:  70 train_loss: 0.2356 train_acc: 0.9714 val_loss: 0.9178 val_acc: 0.7640 test_loss: 0.8855 test_acc: 0.7910\n",
      "Epoch:  70 train_loss: 0.2356 train_acc: 0.9714 val_loss: 0.8011 val_acc: 0.7940 test_loss: 0.8855 test_acc: 0.7910\n",
      "Epoch:  70 train_loss: 0.2356 train_acc: 0.9714 val_loss: 0.8011 val_acc: 0.7940 test_loss: 0.7130 test_acc: 0.8000\n",
      "Epoch:  71 train_loss: 0.2205 train_acc: 0.9786 val_loss: 0.9283 val_acc: 0.7420 test_loss: 0.8906 test_acc: 0.7700\n",
      "Epoch:  71 train_loss: 0.2205 train_acc: 0.9786 val_loss: 0.8090 val_acc: 0.7880 test_loss: 0.8906 test_acc: 0.7700\n",
      "Epoch:  71 train_loss: 0.2205 train_acc: 0.9786 val_loss: 0.8090 val_acc: 0.7880 test_loss: 0.7190 test_acc: 0.7960\n",
      "Epoch:  72 train_loss: 0.2156 train_acc: 0.9786 val_loss: 0.9436 val_acc: 0.7140 test_loss: 0.9023 test_acc: 0.7540\n",
      "Epoch:  72 train_loss: 0.2156 train_acc: 0.9786 val_loss: 0.8403 val_acc: 0.7820 test_loss: 0.9023 test_acc: 0.7540\n",
      "Epoch:  72 train_loss: 0.2156 train_acc: 0.9786 val_loss: 0.8403 val_acc: 0.7820 test_loss: 0.7482 test_acc: 0.7880\n",
      "Epoch:  73 train_loss: 0.2172 train_acc: 0.9786 val_loss: 0.9363 val_acc: 0.7480 test_loss: 0.8967 test_acc: 0.7670\n",
      "Epoch:  73 train_loss: 0.2172 train_acc: 0.9786 val_loss: 0.8707 val_acc: 0.7740 test_loss: 0.8967 test_acc: 0.7670\n",
      "Epoch:  73 train_loss: 0.2172 train_acc: 0.9786 val_loss: 0.8707 val_acc: 0.7740 test_loss: 0.7766 test_acc: 0.7820\n",
      "Epoch:  74 train_loss: 0.2128 train_acc: 0.9786 val_loss: 0.9271 val_acc: 0.7640 test_loss: 0.8892 test_acc: 0.7760\n",
      "Epoch:  74 train_loss: 0.2128 train_acc: 0.9786 val_loss: 0.8648 val_acc: 0.7700 test_loss: 0.8892 test_acc: 0.7760\n",
      "Epoch:  74 train_loss: 0.2128 train_acc: 0.9786 val_loss: 0.8648 val_acc: 0.7700 test_loss: 0.7694 test_acc: 0.7800\n",
      "Epoch:  75 train_loss: 0.2059 train_acc: 0.9714 val_loss: 0.9606 val_acc: 0.7640 test_loss: 0.9279 test_acc: 0.7620\n",
      "Epoch:  75 train_loss: 0.2059 train_acc: 0.9714 val_loss: 0.8513 val_acc: 0.7640 test_loss: 0.9279 test_acc: 0.7620\n",
      "Epoch:  75 train_loss: 0.2059 train_acc: 0.9714 val_loss: 0.8513 val_acc: 0.7640 test_loss: 0.7554 test_acc: 0.7790\n",
      "Epoch:  76 train_loss: 0.2027 train_acc: 0.9929 val_loss: 0.9855 val_acc: 0.7480 test_loss: 0.9580 test_acc: 0.7470\n",
      "Epoch:  76 train_loss: 0.2027 train_acc: 0.9929 val_loss: 0.8430 val_acc: 0.7700 test_loss: 0.9580 test_acc: 0.7470\n",
      "Epoch:  76 train_loss: 0.2027 train_acc: 0.9929 val_loss: 0.8430 val_acc: 0.7700 test_loss: 0.7457 test_acc: 0.7820\n",
      "Epoch:  77 train_loss: 0.2058 train_acc: 0.9929 val_loss: 0.9661 val_acc: 0.7560 test_loss: 0.9423 test_acc: 0.7470\n",
      "Epoch:  77 train_loss: 0.2058 train_acc: 0.9929 val_loss: 0.8358 val_acc: 0.7880 test_loss: 0.9423 test_acc: 0.7470\n",
      "Epoch:  77 train_loss: 0.2058 train_acc: 0.9929 val_loss: 0.8358 val_acc: 0.7880 test_loss: 0.7371 test_acc: 0.7800\n",
      "Epoch:  78 train_loss: 0.2162 train_acc: 0.9857 val_loss: 0.9273 val_acc: 0.7640 test_loss: 0.9079 test_acc: 0.7560\n",
      "Epoch:  78 train_loss: 0.2162 train_acc: 0.9857 val_loss: 0.8383 val_acc: 0.7860 test_loss: 0.9079 test_acc: 0.7560\n",
      "Epoch:  78 train_loss: 0.2162 train_acc: 0.9857 val_loss: 0.8383 val_acc: 0.7860 test_loss: 0.7383 test_acc: 0.7760\n",
      "Epoch:  79 train_loss: 0.2227 train_acc: 0.9571 val_loss: 0.8881 val_acc: 0.7700 test_loss: 0.8668 test_acc: 0.7750\n",
      "Epoch:  79 train_loss: 0.2227 train_acc: 0.9571 val_loss: 0.8311 val_acc: 0.7760 test_loss: 0.8668 test_acc: 0.7750\n",
      "Epoch:  79 train_loss: 0.2227 train_acc: 0.9571 val_loss: 0.8311 val_acc: 0.7760 test_loss: 0.7296 test_acc: 0.7770\n",
      "Epoch:  80 train_loss: 0.2219 train_acc: 0.9714 val_loss: 0.8468 val_acc: 0.7880 test_loss: 0.8201 test_acc: 0.8030\n",
      "Epoch:  80 train_loss: 0.2219 train_acc: 0.9714 val_loss: 0.8319 val_acc: 0.7760 test_loss: 0.8201 test_acc: 0.8030\n",
      "Epoch:  80 train_loss: 0.2219 train_acc: 0.9714 val_loss: 0.8319 val_acc: 0.7760 test_loss: 0.7295 test_acc: 0.7760\n",
      "Epoch:  81 train_loss: 0.2151 train_acc: 0.9786 val_loss: 0.8258 val_acc: 0.7880 test_loss: 0.7977 test_acc: 0.7970\n",
      "Epoch:  81 train_loss: 0.2151 train_acc: 0.9786 val_loss: 0.8481 val_acc: 0.7720 test_loss: 0.7977 test_acc: 0.7970\n",
      "Epoch:  81 train_loss: 0.2151 train_acc: 0.9786 val_loss: 0.8481 val_acc: 0.7720 test_loss: 0.7430 test_acc: 0.7710\n",
      "Epoch:  82 train_loss: 0.2069 train_acc: 0.9786 val_loss: 0.8277 val_acc: 0.7840 test_loss: 0.8036 test_acc: 0.7740\n",
      "Epoch:  82 train_loss: 0.2069 train_acc: 0.9786 val_loss: 0.8563 val_acc: 0.7720 test_loss: 0.8036 test_acc: 0.7740\n",
      "Epoch:  82 train_loss: 0.2069 train_acc: 0.9786 val_loss: 0.8563 val_acc: 0.7720 test_loss: 0.7475 test_acc: 0.7720\n",
      "Epoch:  83 train_loss: 0.2013 train_acc: 0.9786 val_loss: 0.8399 val_acc: 0.7420 test_loss: 0.8213 test_acc: 0.7250\n",
      "Epoch:  83 train_loss: 0.2013 train_acc: 0.9786 val_loss: 0.8746 val_acc: 0.7680 test_loss: 0.8213 test_acc: 0.7250\n",
      "Epoch:  83 train_loss: 0.2013 train_acc: 0.9786 val_loss: 0.8746 val_acc: 0.7680 test_loss: 0.7616 test_acc: 0.7780\n",
      "Epoch:  84 train_loss: 0.1871 train_acc: 0.9714 val_loss: 0.8607 val_acc: 0.7300 test_loss: 0.8472 test_acc: 0.7150\n",
      "Epoch:  84 train_loss: 0.1871 train_acc: 0.9714 val_loss: 0.8656 val_acc: 0.7760 test_loss: 0.8472 test_acc: 0.7150\n",
      "Epoch:  84 train_loss: 0.1871 train_acc: 0.9714 val_loss: 0.8656 val_acc: 0.7760 test_loss: 0.7499 test_acc: 0.7840\n",
      "Epoch:  85 train_loss: 0.1726 train_acc: 0.9786 val_loss: 0.8741 val_acc: 0.7420 test_loss: 0.8603 test_acc: 0.7360\n",
      "Epoch:  85 train_loss: 0.1726 train_acc: 0.9786 val_loss: 0.8448 val_acc: 0.7820 test_loss: 0.8603 test_acc: 0.7360\n",
      "Epoch:  85 train_loss: 0.1726 train_acc: 0.9786 val_loss: 0.8448 val_acc: 0.7820 test_loss: 0.7295 test_acc: 0.7910\n",
      "Epoch:  86 train_loss: 0.1681 train_acc: 0.9857 val_loss: 0.9097 val_acc: 0.7620 test_loss: 0.8943 test_acc: 0.7470\n",
      "Epoch:  86 train_loss: 0.1681 train_acc: 0.9857 val_loss: 0.8418 val_acc: 0.7840 test_loss: 0.8943 test_acc: 0.7470\n",
      "Epoch:  86 train_loss: 0.1681 train_acc: 0.9857 val_loss: 0.8418 val_acc: 0.7840 test_loss: 0.7268 test_acc: 0.7840\n",
      "Epoch:  87 train_loss: 0.1611 train_acc: 0.9857 val_loss: 0.9360 val_acc: 0.7540 test_loss: 0.9145 test_acc: 0.7490\n",
      "Epoch:  87 train_loss: 0.1611 train_acc: 0.9857 val_loss: 0.8129 val_acc: 0.7800 test_loss: 0.9145 test_acc: 0.7490\n",
      "Epoch:  87 train_loss: 0.1611 train_acc: 0.9857 val_loss: 0.8129 val_acc: 0.7800 test_loss: 0.7068 test_acc: 0.7900\n",
      "Epoch:  88 train_loss: 0.1638 train_acc: 0.9786 val_loss: 0.9696 val_acc: 0.7280 test_loss: 0.9389 test_acc: 0.7450\n",
      "Epoch:  88 train_loss: 0.1638 train_acc: 0.9786 val_loss: 0.8014 val_acc: 0.7800 test_loss: 0.9389 test_acc: 0.7450\n",
      "Epoch:  88 train_loss: 0.1638 train_acc: 0.9786 val_loss: 0.8014 val_acc: 0.7800 test_loss: 0.7012 test_acc: 0.7830\n",
      "Epoch:  89 train_loss: 0.1630 train_acc: 0.9857 val_loss: 0.9720 val_acc: 0.7280 test_loss: 0.9386 test_acc: 0.7450\n",
      "Epoch:  89 train_loss: 0.1630 train_acc: 0.9857 val_loss: 0.7920 val_acc: 0.7800 test_loss: 0.9386 test_acc: 0.7450\n",
      "Epoch:  89 train_loss: 0.1630 train_acc: 0.9857 val_loss: 0.7920 val_acc: 0.7800 test_loss: 0.6952 test_acc: 0.7800\n",
      "Epoch:  90 train_loss: 0.1602 train_acc: 0.9857 val_loss: 0.9421 val_acc: 0.7480 test_loss: 0.9037 test_acc: 0.7480\n",
      "Epoch:  90 train_loss: 0.1602 train_acc: 0.9857 val_loss: 0.7822 val_acc: 0.7920 test_loss: 0.9037 test_acc: 0.7480\n",
      "Epoch:  90 train_loss: 0.1602 train_acc: 0.9857 val_loss: 0.7822 val_acc: 0.7920 test_loss: 0.6871 test_acc: 0.7830\n",
      "Epoch:  91 train_loss: 0.1581 train_acc: 0.9857 val_loss: 0.8983 val_acc: 0.7600 test_loss: 0.8585 test_acc: 0.7640\n",
      "Epoch:  91 train_loss: 0.1581 train_acc: 0.9857 val_loss: 0.7803 val_acc: 0.7900 test_loss: 0.8585 test_acc: 0.7640\n",
      "Epoch:  91 train_loss: 0.1581 train_acc: 0.9857 val_loss: 0.7803 val_acc: 0.7900 test_loss: 0.6855 test_acc: 0.7890\n",
      "Epoch:  92 train_loss: 0.1576 train_acc: 0.9857 val_loss: 0.8587 val_acc: 0.7880 test_loss: 0.8183 test_acc: 0.7730\n",
      "Epoch:  92 train_loss: 0.1576 train_acc: 0.9857 val_loss: 0.7900 val_acc: 0.7920 test_loss: 0.8183 test_acc: 0.7730\n",
      "Epoch:  92 train_loss: 0.1576 train_acc: 0.9857 val_loss: 0.7900 val_acc: 0.7920 test_loss: 0.6925 test_acc: 0.7890\n",
      "Epoch:  93 train_loss: 0.1587 train_acc: 0.9857 val_loss: 0.8376 val_acc: 0.7840 test_loss: 0.7988 test_acc: 0.7720\n",
      "Epoch:  93 train_loss: 0.1587 train_acc: 0.9857 val_loss: 0.8014 val_acc: 0.7840 test_loss: 0.7988 test_acc: 0.7720\n",
      "Epoch:  93 train_loss: 0.1587 train_acc: 0.9857 val_loss: 0.8014 val_acc: 0.7840 test_loss: 0.7012 test_acc: 0.7910\n",
      "Epoch:  94 train_loss: 0.1594 train_acc: 0.9786 val_loss: 0.8151 val_acc: 0.7820 test_loss: 0.7743 test_acc: 0.7770\n",
      "Epoch:  94 train_loss: 0.1594 train_acc: 0.9786 val_loss: 0.8207 val_acc: 0.7720 test_loss: 0.7743 test_acc: 0.7770\n",
      "Epoch:  94 train_loss: 0.1594 train_acc: 0.9786 val_loss: 0.8207 val_acc: 0.7720 test_loss: 0.7162 test_acc: 0.7900\n",
      "Epoch:  95 train_loss: 0.1572 train_acc: 0.9786 val_loss: 0.8141 val_acc: 0.7780 test_loss: 0.7734 test_acc: 0.7780\n",
      "Epoch:  95 train_loss: 0.1572 train_acc: 0.9786 val_loss: 0.8032 val_acc: 0.7740 test_loss: 0.7734 test_acc: 0.7780\n",
      "Epoch:  95 train_loss: 0.1572 train_acc: 0.9786 val_loss: 0.8032 val_acc: 0.7740 test_loss: 0.7000 test_acc: 0.7920\n",
      "Epoch:  96 train_loss: 0.1514 train_acc: 0.9857 val_loss: 0.8085 val_acc: 0.7840 test_loss: 0.7664 test_acc: 0.7950\n",
      "Epoch:  96 train_loss: 0.1514 train_acc: 0.9857 val_loss: 0.7992 val_acc: 0.7800 test_loss: 0.7664 test_acc: 0.7950\n",
      "Epoch:  96 train_loss: 0.1514 train_acc: 0.9857 val_loss: 0.7992 val_acc: 0.7800 test_loss: 0.6964 test_acc: 0.7910\n",
      "Epoch:  97 train_loss: 0.1466 train_acc: 0.9929 val_loss: 0.8285 val_acc: 0.7760 test_loss: 0.7819 test_acc: 0.7950\n",
      "Epoch:  97 train_loss: 0.1466 train_acc: 0.9929 val_loss: 0.8009 val_acc: 0.7820 test_loss: 0.7819 test_acc: 0.7950\n",
      "Epoch:  97 train_loss: 0.1466 train_acc: 0.9929 val_loss: 0.8009 val_acc: 0.7820 test_loss: 0.6982 test_acc: 0.7910\n",
      "Epoch:  98 train_loss: 0.1460 train_acc: 0.9857 val_loss: 0.8500 val_acc: 0.7600 test_loss: 0.8005 test_acc: 0.7690\n",
      "Epoch:  98 train_loss: 0.1460 train_acc: 0.9857 val_loss: 0.8107 val_acc: 0.7920 test_loss: 0.8005 test_acc: 0.7690\n",
      "Epoch:  98 train_loss: 0.1460 train_acc: 0.9857 val_loss: 0.8107 val_acc: 0.7920 test_loss: 0.7064 test_acc: 0.7880\n",
      "Epoch:  99 train_loss: 0.1498 train_acc: 0.9714 val_loss: 0.8653 val_acc: 0.7600 test_loss: 0.8135 test_acc: 0.7710\n",
      "Epoch:  99 train_loss: 0.1498 train_acc: 0.9714 val_loss: 0.8242 val_acc: 0.7740 test_loss: 0.8135 test_acc: 0.7710\n",
      "Epoch:  99 train_loss: 0.1498 train_acc: 0.9714 val_loss: 0.8242 val_acc: 0.7740 test_loss: 0.7181 test_acc: 0.7830\n",
      "Epoch: 100 train_loss: 0.1476 train_acc: 0.9786 val_loss: 0.8573 val_acc: 0.7700 test_loss: 0.8072 test_acc: 0.7820\n",
      "Epoch: 100 train_loss: 0.1476 train_acc: 0.9786 val_loss: 0.8259 val_acc: 0.7780 test_loss: 0.8072 test_acc: 0.7820\n",
      "Epoch: 100 train_loss: 0.1476 train_acc: 0.9786 val_loss: 0.8259 val_acc: 0.7780 test_loss: 0.7187 test_acc: 0.7850\n",
      "Epoch: 101 train_loss: 0.1484 train_acc: 0.9786 val_loss: 0.8449 val_acc: 0.7860 test_loss: 0.7975 test_acc: 0.7800\n",
      "Epoch: 101 train_loss: 0.1484 train_acc: 0.9786 val_loss: 0.8374 val_acc: 0.7780 test_loss: 0.7975 test_acc: 0.7800\n",
      "Epoch: 101 train_loss: 0.1484 train_acc: 0.9786 val_loss: 0.8374 val_acc: 0.7780 test_loss: 0.7305 test_acc: 0.7850\n",
      "Epoch: 102 train_loss: 0.1457 train_acc: 0.9714 val_loss: 0.8375 val_acc: 0.7840 test_loss: 0.7933 test_acc: 0.7730\n",
      "Epoch: 102 train_loss: 0.1457 train_acc: 0.9714 val_loss: 0.8386 val_acc: 0.7800 test_loss: 0.7933 test_acc: 0.7730\n",
      "Epoch: 102 train_loss: 0.1457 train_acc: 0.9714 val_loss: 0.8386 val_acc: 0.7800 test_loss: 0.7335 test_acc: 0.7820\n",
      "Epoch: 103 train_loss: 0.1425 train_acc: 0.9714 val_loss: 0.8560 val_acc: 0.7700 test_loss: 0.8113 test_acc: 0.7740\n",
      "Epoch: 103 train_loss: 0.1425 train_acc: 0.9714 val_loss: 0.8382 val_acc: 0.7840 test_loss: 0.8113 test_acc: 0.7740\n",
      "Epoch: 103 train_loss: 0.1425 train_acc: 0.9714 val_loss: 0.8382 val_acc: 0.7840 test_loss: 0.7346 test_acc: 0.7820\n",
      "Epoch: 104 train_loss: 0.1388 train_acc: 0.9714 val_loss: 0.8534 val_acc: 0.7600 test_loss: 0.8030 test_acc: 0.7730\n",
      "Epoch: 104 train_loss: 0.1388 train_acc: 0.9714 val_loss: 0.8288 val_acc: 0.7860 test_loss: 0.8030 test_acc: 0.7730\n",
      "Epoch: 104 train_loss: 0.1388 train_acc: 0.9714 val_loss: 0.8288 val_acc: 0.7860 test_loss: 0.7282 test_acc: 0.7830\n",
      "Epoch: 105 train_loss: 0.1344 train_acc: 0.9786 val_loss: 0.8543 val_acc: 0.7520 test_loss: 0.8002 test_acc: 0.7710\n",
      "Epoch: 105 train_loss: 0.1344 train_acc: 0.9786 val_loss: 0.8173 val_acc: 0.7880 test_loss: 0.8002 test_acc: 0.7710\n",
      "Epoch: 105 train_loss: 0.1344 train_acc: 0.9786 val_loss: 0.8173 val_acc: 0.7880 test_loss: 0.7167 test_acc: 0.7830\n",
      "Epoch: 106 train_loss: 0.1295 train_acc: 0.9786 val_loss: 0.8376 val_acc: 0.7620 test_loss: 0.7833 test_acc: 0.7740\n",
      "Epoch: 106 train_loss: 0.1295 train_acc: 0.9786 val_loss: 0.8017 val_acc: 0.7880 test_loss: 0.7833 test_acc: 0.7740\n",
      "Epoch: 106 train_loss: 0.1295 train_acc: 0.9786 val_loss: 0.8017 val_acc: 0.7880 test_loss: 0.7006 test_acc: 0.7850\n",
      "Epoch: 107 train_loss: 0.1220 train_acc: 0.9857 val_loss: 0.8148 val_acc: 0.7780 test_loss: 0.7662 test_acc: 0.7920\n",
      "Epoch: 107 train_loss: 0.1220 train_acc: 0.9857 val_loss: 0.7796 val_acc: 0.7920 test_loss: 0.7662 test_acc: 0.7920\n",
      "Epoch: 107 train_loss: 0.1220 train_acc: 0.9857 val_loss: 0.7796 val_acc: 0.7920 test_loss: 0.6761 test_acc: 0.7950\n",
      "Epoch: 108 train_loss: 0.1204 train_acc: 0.9857 val_loss: 0.8178 val_acc: 0.8000 test_loss: 0.7725 test_acc: 0.8000\n",
      "Epoch: 108 train_loss: 0.1204 train_acc: 0.9857 val_loss: 0.7634 val_acc: 0.7860 test_loss: 0.7725 test_acc: 0.8000\n",
      "Epoch: 108 train_loss: 0.1204 train_acc: 0.9857 val_loss: 0.7634 val_acc: 0.7860 test_loss: 0.6558 test_acc: 0.8040\n",
      "Epoch: 109 train_loss: 0.1230 train_acc: 0.9714 val_loss: 0.8329 val_acc: 0.8040 test_loss: 0.7928 test_acc: 0.7910\n",
      "Epoch: 109 train_loss: 0.1230 train_acc: 0.9714 val_loss: 0.7456 val_acc: 0.7960 test_loss: 0.7928 test_acc: 0.7910\n",
      "Epoch: 109 train_loss: 0.1230 train_acc: 0.9714 val_loss: 0.7456 val_acc: 0.7960 test_loss: 0.6354 test_acc: 0.8090\n",
      "Epoch: 110 train_loss: 0.1246 train_acc: 0.9714 val_loss: 0.8279 val_acc: 0.7940 test_loss: 0.7948 test_acc: 0.7920\n",
      "Epoch: 110 train_loss: 0.1246 train_acc: 0.9714 val_loss: 0.7322 val_acc: 0.7920 test_loss: 0.7948 test_acc: 0.7920\n",
      "Epoch: 110 train_loss: 0.1246 train_acc: 0.9714 val_loss: 0.7322 val_acc: 0.7920 test_loss: 0.6220 test_acc: 0.8100\n",
      "Epoch: 111 train_loss: 0.1273 train_acc: 0.9786 val_loss: 0.8018 val_acc: 0.8060 test_loss: 0.7676 test_acc: 0.8100\n",
      "Epoch: 111 train_loss: 0.1273 train_acc: 0.9786 val_loss: 0.7258 val_acc: 0.7960 test_loss: 0.7676 test_acc: 0.8100\n",
      "Epoch: 111 train_loss: 0.1273 train_acc: 0.9786 val_loss: 0.7258 val_acc: 0.7960 test_loss: 0.6164 test_acc: 0.8140\n",
      "Epoch: 112 train_loss: 0.1284 train_acc: 0.9643 val_loss: 0.8021 val_acc: 0.7840 test_loss: 0.7674 test_acc: 0.8020\n",
      "Epoch: 112 train_loss: 0.1284 train_acc: 0.9643 val_loss: 0.7446 val_acc: 0.7940 test_loss: 0.7674 test_acc: 0.8020\n",
      "Epoch: 112 train_loss: 0.1284 train_acc: 0.9643 val_loss: 0.7446 val_acc: 0.7940 test_loss: 0.6350 test_acc: 0.8040\n",
      "Epoch: 113 train_loss: 0.1299 train_acc: 0.9643 val_loss: 0.8248 val_acc: 0.7780 test_loss: 0.7916 test_acc: 0.7910\n",
      "Epoch: 113 train_loss: 0.1299 train_acc: 0.9643 val_loss: 0.7748 val_acc: 0.7920 test_loss: 0.7916 test_acc: 0.7910\n",
      "Epoch: 113 train_loss: 0.1299 train_acc: 0.9643 val_loss: 0.7748 val_acc: 0.7920 test_loss: 0.6670 test_acc: 0.8020\n",
      "Epoch: 114 train_loss: 0.1347 train_acc: 0.9714 val_loss: 0.8432 val_acc: 0.7700 test_loss: 0.8088 test_acc: 0.7810\n",
      "Epoch: 114 train_loss: 0.1347 train_acc: 0.9714 val_loss: 0.8122 val_acc: 0.7780 test_loss: 0.8088 test_acc: 0.7810\n",
      "Epoch: 114 train_loss: 0.1347 train_acc: 0.9714 val_loss: 0.8122 val_acc: 0.7780 test_loss: 0.7079 test_acc: 0.7860\n",
      "Epoch: 115 train_loss: 0.1340 train_acc: 0.9714 val_loss: 0.8449 val_acc: 0.7820 test_loss: 0.8117 test_acc: 0.7730\n",
      "Epoch: 115 train_loss: 0.1340 train_acc: 0.9714 val_loss: 0.8348 val_acc: 0.7720 test_loss: 0.8117 test_acc: 0.7730\n",
      "Epoch: 115 train_loss: 0.1340 train_acc: 0.9714 val_loss: 0.8348 val_acc: 0.7720 test_loss: 0.7338 test_acc: 0.7800\n",
      "Epoch: 116 train_loss: 0.1319 train_acc: 0.9786 val_loss: 0.8548 val_acc: 0.8040 test_loss: 0.8237 test_acc: 0.7780\n",
      "Epoch: 116 train_loss: 0.1319 train_acc: 0.9786 val_loss: 0.8543 val_acc: 0.7740 test_loss: 0.8237 test_acc: 0.7780\n",
      "Epoch: 116 train_loss: 0.1319 train_acc: 0.9786 val_loss: 0.8543 val_acc: 0.7740 test_loss: 0.7568 test_acc: 0.7740\n",
      "Epoch: 117 train_loss: 0.1275 train_acc: 0.9857 val_loss: 0.8892 val_acc: 0.7980 test_loss: 0.8555 test_acc: 0.7740\n",
      "Epoch: 117 train_loss: 0.1275 train_acc: 0.9857 val_loss: 0.8637 val_acc: 0.7780 test_loss: 0.8555 test_acc: 0.7740\n",
      "Epoch: 117 train_loss: 0.1275 train_acc: 0.9857 val_loss: 0.8637 val_acc: 0.7780 test_loss: 0.7667 test_acc: 0.7750\n",
      "Epoch: 118 train_loss: 0.1278 train_acc: 0.9786 val_loss: 0.9263 val_acc: 0.7700 test_loss: 0.8848 test_acc: 0.7600\n",
      "Epoch: 118 train_loss: 0.1278 train_acc: 0.9786 val_loss: 0.8737 val_acc: 0.7720 test_loss: 0.8848 test_acc: 0.7600\n",
      "Epoch: 118 train_loss: 0.1278 train_acc: 0.9786 val_loss: 0.8737 val_acc: 0.7720 test_loss: 0.7754 test_acc: 0.7710\n",
      "Epoch: 119 train_loss: 0.1277 train_acc: 0.9786 val_loss: 0.8933 val_acc: 0.7760 test_loss: 0.8495 test_acc: 0.7670\n",
      "Epoch: 119 train_loss: 0.1277 train_acc: 0.9786 val_loss: 0.8730 val_acc: 0.7740 test_loss: 0.8495 test_acc: 0.7670\n",
      "Epoch: 119 train_loss: 0.1277 train_acc: 0.9786 val_loss: 0.8730 val_acc: 0.7740 test_loss: 0.7731 test_acc: 0.7640\n",
      "Epoch: 120 train_loss: 0.1238 train_acc: 0.9714 val_loss: 0.8484 val_acc: 0.7800 test_loss: 0.7991 test_acc: 0.7800\n",
      "Epoch: 120 train_loss: 0.1238 train_acc: 0.9714 val_loss: 0.8589 val_acc: 0.7660 test_loss: 0.7991 test_acc: 0.7800\n",
      "Epoch: 120 train_loss: 0.1238 train_acc: 0.9714 val_loss: 0.8589 val_acc: 0.7660 test_loss: 0.7571 test_acc: 0.7670\n",
      "Epoch: 121 train_loss: 0.1166 train_acc: 0.9714 val_loss: 0.8246 val_acc: 0.7960 test_loss: 0.7721 test_acc: 0.7910\n",
      "Epoch: 121 train_loss: 0.1166 train_acc: 0.9714 val_loss: 0.8365 val_acc: 0.7680 test_loss: 0.7721 test_acc: 0.7910\n",
      "Epoch: 121 train_loss: 0.1166 train_acc: 0.9714 val_loss: 0.8365 val_acc: 0.7680 test_loss: 0.7346 test_acc: 0.7710\n",
      "Epoch: 122 train_loss: 0.1147 train_acc: 0.9857 val_loss: 0.8295 val_acc: 0.7940 test_loss: 0.7785 test_acc: 0.7870\n",
      "Epoch: 122 train_loss: 0.1147 train_acc: 0.9857 val_loss: 0.8190 val_acc: 0.7780 test_loss: 0.7785 test_acc: 0.7870\n",
      "Epoch: 122 train_loss: 0.1147 train_acc: 0.9857 val_loss: 0.8190 val_acc: 0.7780 test_loss: 0.7178 test_acc: 0.7790\n",
      "Epoch: 123 train_loss: 0.1172 train_acc: 0.9857 val_loss: 0.8330 val_acc: 0.7760 test_loss: 0.7820 test_acc: 0.7870\n",
      "Epoch: 123 train_loss: 0.1172 train_acc: 0.9857 val_loss: 0.7908 val_acc: 0.7860 test_loss: 0.7820 test_acc: 0.7870\n",
      "Epoch: 123 train_loss: 0.1172 train_acc: 0.9857 val_loss: 0.7908 val_acc: 0.7860 test_loss: 0.6923 test_acc: 0.7880\n",
      "Epoch: 124 train_loss: 0.1262 train_acc: 0.9786 val_loss: 0.8551 val_acc: 0.7520 test_loss: 0.8056 test_acc: 0.7730\n",
      "Epoch: 124 train_loss: 0.1262 train_acc: 0.9786 val_loss: 0.7565 val_acc: 0.7940 test_loss: 0.8056 test_acc: 0.7730\n",
      "Epoch: 124 train_loss: 0.1262 train_acc: 0.9786 val_loss: 0.7565 val_acc: 0.7940 test_loss: 0.6593 test_acc: 0.7950\n",
      "Epoch: 125 train_loss: 0.1316 train_acc: 0.9643 val_loss: 0.9056 val_acc: 0.7560 test_loss: 0.8705 test_acc: 0.7510\n",
      "Epoch: 125 train_loss: 0.1316 train_acc: 0.9643 val_loss: 0.7490 val_acc: 0.7940 test_loss: 0.8705 test_acc: 0.7510\n",
      "Epoch: 125 train_loss: 0.1316 train_acc: 0.9643 val_loss: 0.7490 val_acc: 0.7940 test_loss: 0.6532 test_acc: 0.7990\n",
      "Epoch: 126 train_loss: 0.1214 train_acc: 0.9786 val_loss: 0.9974 val_acc: 0.7100 test_loss: 0.9719 test_acc: 0.7090\n",
      "Epoch: 126 train_loss: 0.1214 train_acc: 0.9786 val_loss: 0.7734 val_acc: 0.7900 test_loss: 0.9719 test_acc: 0.7090\n",
      "Epoch: 126 train_loss: 0.1214 train_acc: 0.9786 val_loss: 0.7734 val_acc: 0.7900 test_loss: 0.6788 test_acc: 0.7940\n",
      "Epoch: 127 train_loss: 0.1185 train_acc: 0.9857 val_loss: 1.0376 val_acc: 0.6700 test_loss: 1.0157 test_acc: 0.6610\n",
      "Epoch: 127 train_loss: 0.1185 train_acc: 0.9857 val_loss: 0.8115 val_acc: 0.7800 test_loss: 1.0157 test_acc: 0.6610\n",
      "Epoch: 127 train_loss: 0.1185 train_acc: 0.9857 val_loss: 0.8115 val_acc: 0.7800 test_loss: 0.7180 test_acc: 0.7860\n",
      "Epoch: 128 train_loss: 0.1245 train_acc: 0.9786 val_loss: 0.9618 val_acc: 0.7400 test_loss: 0.9337 test_acc: 0.7230\n",
      "Epoch: 128 train_loss: 0.1245 train_acc: 0.9786 val_loss: 0.8658 val_acc: 0.7600 test_loss: 0.9337 test_acc: 0.7230\n",
      "Epoch: 128 train_loss: 0.1245 train_acc: 0.9786 val_loss: 0.8658 val_acc: 0.7600 test_loss: 0.7740 test_acc: 0.7650\n",
      "Epoch: 129 train_loss: 0.1335 train_acc: 0.9786 val_loss: 0.9034 val_acc: 0.7560 test_loss: 0.8658 test_acc: 0.7760\n",
      "Epoch: 129 train_loss: 0.1335 train_acc: 0.9786 val_loss: 0.9184 val_acc: 0.7520 test_loss: 0.8658 test_acc: 0.7760\n",
      "Epoch: 129 train_loss: 0.1335 train_acc: 0.9786 val_loss: 0.9184 val_acc: 0.7520 test_loss: 0.8269 test_acc: 0.7500\n",
      "Epoch: 130 train_loss: 0.1370 train_acc: 0.9857 val_loss: 0.8982 val_acc: 0.7260 test_loss: 0.8540 test_acc: 0.7450\n",
      "Epoch: 130 train_loss: 0.1370 train_acc: 0.9857 val_loss: 0.9561 val_acc: 0.7560 test_loss: 0.8540 test_acc: 0.7450\n",
      "Epoch: 130 train_loss: 0.1370 train_acc: 0.9857 val_loss: 0.9561 val_acc: 0.7560 test_loss: 0.8604 test_acc: 0.7450\n",
      "Epoch: 131 train_loss: 0.1281 train_acc: 0.9786 val_loss: 0.8869 val_acc: 0.7160 test_loss: 0.8438 test_acc: 0.7380\n",
      "Epoch: 131 train_loss: 0.1281 train_acc: 0.9786 val_loss: 0.9567 val_acc: 0.7640 test_loss: 0.8438 test_acc: 0.7380\n",
      "Epoch: 131 train_loss: 0.1281 train_acc: 0.9786 val_loss: 0.9567 val_acc: 0.7640 test_loss: 0.8559 test_acc: 0.7620\n",
      "Epoch: 132 train_loss: 0.1173 train_acc: 0.9786 val_loss: 0.8541 val_acc: 0.7460 test_loss: 0.8165 test_acc: 0.7640\n",
      "Epoch: 132 train_loss: 0.1173 train_acc: 0.9786 val_loss: 0.9294 val_acc: 0.7660 test_loss: 0.8165 test_acc: 0.7640\n",
      "Epoch: 132 train_loss: 0.1173 train_acc: 0.9786 val_loss: 0.9294 val_acc: 0.7660 test_loss: 0.8249 test_acc: 0.7670\n",
      "Epoch: 133 train_loss: 0.1073 train_acc: 0.9857 val_loss: 0.8385 val_acc: 0.7700 test_loss: 0.8095 test_acc: 0.7760\n",
      "Epoch: 133 train_loss: 0.1073 train_acc: 0.9857 val_loss: 0.8956 val_acc: 0.7680 test_loss: 0.8095 test_acc: 0.7760\n",
      "Epoch: 133 train_loss: 0.1073 train_acc: 0.9857 val_loss: 0.8956 val_acc: 0.7680 test_loss: 0.7871 test_acc: 0.7810\n",
      "Epoch: 134 train_loss: 0.1033 train_acc: 0.9929 val_loss: 0.8444 val_acc: 0.7720 test_loss: 0.8230 test_acc: 0.7530\n",
      "Epoch: 134 train_loss: 0.1033 train_acc: 0.9929 val_loss: 0.8689 val_acc: 0.7680 test_loss: 0.8230 test_acc: 0.7530\n",
      "Epoch: 134 train_loss: 0.1033 train_acc: 0.9929 val_loss: 0.8689 val_acc: 0.7680 test_loss: 0.7573 test_acc: 0.7850\n",
      "Epoch: 135 train_loss: 0.0971 train_acc: 1.0000 val_loss: 0.8611 val_acc: 0.7680 test_loss: 0.8446 test_acc: 0.7630\n",
      "Epoch: 135 train_loss: 0.0971 train_acc: 1.0000 val_loss: 0.8182 val_acc: 0.7820 test_loss: 0.8446 test_acc: 0.7630\n",
      "Epoch: 135 train_loss: 0.0971 train_acc: 1.0000 val_loss: 0.8182 val_acc: 0.7820 test_loss: 0.7085 test_acc: 0.7880\n",
      "Epoch: 136 train_loss: 0.0982 train_acc: 0.9929 val_loss: 0.8798 val_acc: 0.7560 test_loss: 0.8639 test_acc: 0.7490\n",
      "Epoch: 136 train_loss: 0.0982 train_acc: 0.9929 val_loss: 0.7881 val_acc: 0.7940 test_loss: 0.8639 test_acc: 0.7490\n",
      "Epoch: 136 train_loss: 0.0982 train_acc: 0.9929 val_loss: 0.7881 val_acc: 0.7940 test_loss: 0.6806 test_acc: 0.7970\n",
      "Epoch: 137 train_loss: 0.0979 train_acc: 1.0000 val_loss: 0.8626 val_acc: 0.7680 test_loss: 0.8368 test_acc: 0.7540\n",
      "Epoch: 137 train_loss: 0.0979 train_acc: 1.0000 val_loss: 0.7761 val_acc: 0.8040 test_loss: 0.8368 test_acc: 0.7540\n",
      "Epoch: 137 train_loss: 0.0979 train_acc: 1.0000 val_loss: 0.7761 val_acc: 0.8040 test_loss: 0.6683 test_acc: 0.8040\n",
      "Epoch: 138 train_loss: 0.1000 train_acc: 1.0000 val_loss: 0.8490 val_acc: 0.7580 test_loss: 0.8131 test_acc: 0.7600\n",
      "Epoch: 138 train_loss: 0.1000 train_acc: 1.0000 val_loss: 0.7549 val_acc: 0.8000 test_loss: 0.8131 test_acc: 0.7600\n",
      "Epoch: 138 train_loss: 0.1000 train_acc: 1.0000 val_loss: 0.7549 val_acc: 0.8000 test_loss: 0.6443 test_acc: 0.8100\n",
      "Epoch: 139 train_loss: 0.1047 train_acc: 1.0000 val_loss: 0.8361 val_acc: 0.7500 test_loss: 0.7918 test_acc: 0.7760\n",
      "Epoch: 139 train_loss: 0.1047 train_acc: 1.0000 val_loss: 0.7357 val_acc: 0.8080 test_loss: 0.7918 test_acc: 0.7760\n",
      "Epoch: 139 train_loss: 0.1047 train_acc: 1.0000 val_loss: 0.7357 val_acc: 0.8080 test_loss: 0.6199 test_acc: 0.8070\n",
      "Epoch: 140 train_loss: 0.1072 train_acc: 0.9929 val_loss: 0.8383 val_acc: 0.7420 test_loss: 0.7906 test_acc: 0.7550\n",
      "Epoch: 140 train_loss: 0.1072 train_acc: 0.9929 val_loss: 0.7411 val_acc: 0.7980 test_loss: 0.7906 test_acc: 0.7550\n",
      "Epoch: 140 train_loss: 0.1072 train_acc: 0.9929 val_loss: 0.7411 val_acc: 0.7980 test_loss: 0.6221 test_acc: 0.8050\n",
      "Epoch: 141 train_loss: 0.1109 train_acc: 0.9786 val_loss: 0.8397 val_acc: 0.7500 test_loss: 0.7922 test_acc: 0.7510\n",
      "Epoch: 141 train_loss: 0.1109 train_acc: 0.9786 val_loss: 0.7567 val_acc: 0.7940 test_loss: 0.7922 test_acc: 0.7510\n",
      "Epoch: 141 train_loss: 0.1109 train_acc: 0.9786 val_loss: 0.7567 val_acc: 0.7940 test_loss: 0.6365 test_acc: 0.8050\n",
      "Epoch: 142 train_loss: 0.1148 train_acc: 0.9929 val_loss: 0.8186 val_acc: 0.7780 test_loss: 0.7737 test_acc: 0.7690\n",
      "Epoch: 142 train_loss: 0.1148 train_acc: 0.9929 val_loss: 0.7852 val_acc: 0.7860 test_loss: 0.7737 test_acc: 0.7690\n",
      "Epoch: 142 train_loss: 0.1148 train_acc: 0.9929 val_loss: 0.7852 val_acc: 0.7860 test_loss: 0.6654 test_acc: 0.8110\n",
      "Epoch: 143 train_loss: 0.1195 train_acc: 0.9929 val_loss: 0.8189 val_acc: 0.7820 test_loss: 0.7780 test_acc: 0.7720\n",
      "Epoch: 143 train_loss: 0.1195 train_acc: 0.9929 val_loss: 0.8075 val_acc: 0.7800 test_loss: 0.7780 test_acc: 0.7720\n",
      "Epoch: 143 train_loss: 0.1195 train_acc: 0.9929 val_loss: 0.8075 val_acc: 0.7800 test_loss: 0.6903 test_acc: 0.7980\n",
      "Epoch: 144 train_loss: 0.1296 train_acc: 0.9857 val_loss: 0.8305 val_acc: 0.7600 test_loss: 0.7955 test_acc: 0.7670\n",
      "Epoch: 144 train_loss: 0.1296 train_acc: 0.9857 val_loss: 0.8367 val_acc: 0.7760 test_loss: 0.7955 test_acc: 0.7670\n",
      "Epoch: 144 train_loss: 0.1296 train_acc: 0.9857 val_loss: 0.8367 val_acc: 0.7760 test_loss: 0.7230 test_acc: 0.7850\n",
      "Epoch: 145 train_loss: 0.1396 train_acc: 0.9857 val_loss: 0.8205 val_acc: 0.7820 test_loss: 0.7866 test_acc: 0.7650\n",
      "Epoch: 145 train_loss: 0.1396 train_acc: 0.9857 val_loss: 0.8646 val_acc: 0.7580 test_loss: 0.7866 test_acc: 0.7650\n",
      "Epoch: 145 train_loss: 0.1396 train_acc: 0.9857 val_loss: 0.8646 val_acc: 0.7580 test_loss: 0.7569 test_acc: 0.7740\n",
      "Epoch: 146 train_loss: 0.1436 train_acc: 0.9857 val_loss: 0.8019 val_acc: 0.7800 test_loss: 0.7674 test_acc: 0.7780\n",
      "Epoch: 146 train_loss: 0.1436 train_acc: 0.9857 val_loss: 0.8758 val_acc: 0.7500 test_loss: 0.7674 test_acc: 0.7780\n",
      "Epoch: 146 train_loss: 0.1436 train_acc: 0.9857 val_loss: 0.8758 val_acc: 0.7500 test_loss: 0.7719 test_acc: 0.7680\n",
      "Epoch: 147 train_loss: 0.1357 train_acc: 0.9929 val_loss: 0.7755 val_acc: 0.7960 test_loss: 0.7382 test_acc: 0.8000\n",
      "Epoch: 147 train_loss: 0.1357 train_acc: 0.9929 val_loss: 0.8663 val_acc: 0.7540 test_loss: 0.7382 test_acc: 0.8000\n",
      "Epoch: 147 train_loss: 0.1357 train_acc: 0.9929 val_loss: 0.8663 val_acc: 0.7540 test_loss: 0.7636 test_acc: 0.7700\n",
      "Epoch: 148 train_loss: 0.1249 train_acc: 0.9929 val_loss: 0.7744 val_acc: 0.7960 test_loss: 0.7367 test_acc: 0.8030\n",
      "Epoch: 148 train_loss: 0.1249 train_acc: 0.9929 val_loss: 0.8480 val_acc: 0.7680 test_loss: 0.7367 test_acc: 0.8030\n",
      "Epoch: 148 train_loss: 0.1249 train_acc: 0.9929 val_loss: 0.8480 val_acc: 0.7680 test_loss: 0.7455 test_acc: 0.7730\n",
      "Epoch: 149 train_loss: 0.1127 train_acc: 0.9929 val_loss: 0.7835 val_acc: 0.7940 test_loss: 0.7449 test_acc: 0.8020\n",
      "Epoch: 149 train_loss: 0.1127 train_acc: 0.9929 val_loss: 0.8260 val_acc: 0.7820 test_loss: 0.7449 test_acc: 0.8020\n",
      "Epoch: 149 train_loss: 0.1127 train_acc: 0.9929 val_loss: 0.8260 val_acc: 0.7820 test_loss: 0.7224 test_acc: 0.7780\n",
      "Epoch: 150 train_loss: 0.1009 train_acc: 0.9929 val_loss: 0.8088 val_acc: 0.7780 test_loss: 0.7707 test_acc: 0.7950\n",
      "Epoch: 150 train_loss: 0.1009 train_acc: 0.9929 val_loss: 0.8006 val_acc: 0.7860 test_loss: 0.7707 test_acc: 0.7950\n",
      "Epoch: 150 train_loss: 0.1009 train_acc: 0.9929 val_loss: 0.8006 val_acc: 0.7860 test_loss: 0.6952 test_acc: 0.7820\n",
      "Epoch: 151 train_loss: 0.0918 train_acc: 1.0000 val_loss: 0.8319 val_acc: 0.7720 test_loss: 0.7958 test_acc: 0.7800\n",
      "Epoch: 151 train_loss: 0.0918 train_acc: 1.0000 val_loss: 0.7778 val_acc: 0.7980 test_loss: 0.7958 test_acc: 0.7800\n",
      "Epoch: 151 train_loss: 0.0918 train_acc: 1.0000 val_loss: 0.7778 val_acc: 0.7980 test_loss: 0.6713 test_acc: 0.7900\n",
      "Epoch: 152 train_loss: 0.0889 train_acc: 1.0000 val_loss: 0.8715 val_acc: 0.7560 test_loss: 0.8362 test_acc: 0.7630\n",
      "Epoch: 152 train_loss: 0.0889 train_acc: 1.0000 val_loss: 0.7716 val_acc: 0.8060 test_loss: 0.8362 test_acc: 0.7630\n",
      "Epoch: 152 train_loss: 0.0889 train_acc: 1.0000 val_loss: 0.7716 val_acc: 0.8060 test_loss: 0.6615 test_acc: 0.7910\n",
      "Epoch: 153 train_loss: 0.0884 train_acc: 1.0000 val_loss: 0.9113 val_acc: 0.7560 test_loss: 0.8720 test_acc: 0.7630\n",
      "Epoch: 153 train_loss: 0.0884 train_acc: 1.0000 val_loss: 0.7693 val_acc: 0.7940 test_loss: 0.8720 test_acc: 0.7630\n",
      "Epoch: 153 train_loss: 0.0884 train_acc: 1.0000 val_loss: 0.7693 val_acc: 0.7940 test_loss: 0.6575 test_acc: 0.7890\n",
      "Epoch: 154 train_loss: 0.0890 train_acc: 0.9929 val_loss: 0.9243 val_acc: 0.7520 test_loss: 0.8824 test_acc: 0.7640\n",
      "Epoch: 154 train_loss: 0.0890 train_acc: 0.9929 val_loss: 0.7656 val_acc: 0.7780 test_loss: 0.8824 test_acc: 0.7640\n",
      "Epoch: 154 train_loss: 0.0890 train_acc: 0.9929 val_loss: 0.7656 val_acc: 0.7780 test_loss: 0.6527 test_acc: 0.7890\n",
      "Epoch: 155 train_loss: 0.0891 train_acc: 0.9929 val_loss: 0.9224 val_acc: 0.7540 test_loss: 0.8791 test_acc: 0.7570\n",
      "Epoch: 155 train_loss: 0.0891 train_acc: 0.9929 val_loss: 0.7640 val_acc: 0.7820 test_loss: 0.8791 test_acc: 0.7570\n",
      "Epoch: 155 train_loss: 0.0891 train_acc: 0.9929 val_loss: 0.7640 val_acc: 0.7820 test_loss: 0.6504 test_acc: 0.7880\n",
      "Epoch: 156 train_loss: 0.0861 train_acc: 0.9929 val_loss: 0.9187 val_acc: 0.7540 test_loss: 0.8730 test_acc: 0.7660\n",
      "Epoch: 156 train_loss: 0.0861 train_acc: 0.9929 val_loss: 0.7553 val_acc: 0.7860 test_loss: 0.8730 test_acc: 0.7660\n",
      "Epoch: 156 train_loss: 0.0861 train_acc: 0.9929 val_loss: 0.7553 val_acc: 0.7860 test_loss: 0.6443 test_acc: 0.7950\n",
      "Epoch: 157 train_loss: 0.0795 train_acc: 0.9929 val_loss: 0.8945 val_acc: 0.7680 test_loss: 0.8394 test_acc: 0.7750\n",
      "Epoch: 157 train_loss: 0.0795 train_acc: 0.9929 val_loss: 0.7598 val_acc: 0.7820 test_loss: 0.8394 test_acc: 0.7750\n",
      "Epoch: 157 train_loss: 0.0795 train_acc: 0.9929 val_loss: 0.7598 val_acc: 0.7820 test_loss: 0.6512 test_acc: 0.7900\n",
      "Epoch: 158 train_loss: 0.0743 train_acc: 0.9929 val_loss: 0.8911 val_acc: 0.7720 test_loss: 0.8278 test_acc: 0.7830\n",
      "Epoch: 158 train_loss: 0.0743 train_acc: 0.9929 val_loss: 0.7687 val_acc: 0.7900 test_loss: 0.8278 test_acc: 0.7830\n",
      "Epoch: 158 train_loss: 0.0743 train_acc: 0.9929 val_loss: 0.7687 val_acc: 0.7900 test_loss: 0.6639 test_acc: 0.7910\n",
      "Epoch: 159 train_loss: 0.0774 train_acc: 0.9857 val_loss: 0.8996 val_acc: 0.7660 test_loss: 0.8331 test_acc: 0.7740\n",
      "Epoch: 159 train_loss: 0.0774 train_acc: 0.9857 val_loss: 0.7765 val_acc: 0.7940 test_loss: 0.8331 test_acc: 0.7740\n",
      "Epoch: 159 train_loss: 0.0774 train_acc: 0.9857 val_loss: 0.7765 val_acc: 0.7940 test_loss: 0.6751 test_acc: 0.7890\n",
      "Epoch: 160 train_loss: 0.0816 train_acc: 0.9786 val_loss: 0.9221 val_acc: 0.7560 test_loss: 0.8544 test_acc: 0.7610\n",
      "Epoch: 160 train_loss: 0.0816 train_acc: 0.9786 val_loss: 0.7932 val_acc: 0.7900 test_loss: 0.8544 test_acc: 0.7610\n",
      "Epoch: 160 train_loss: 0.0816 train_acc: 0.9786 val_loss: 0.7932 val_acc: 0.7900 test_loss: 0.6934 test_acc: 0.7790\n",
      "Epoch: 161 train_loss: 0.0917 train_acc: 0.9786 val_loss: 0.9315 val_acc: 0.7560 test_loss: 0.8667 test_acc: 0.7630\n",
      "Epoch: 161 train_loss: 0.0917 train_acc: 0.9786 val_loss: 0.8121 val_acc: 0.7800 test_loss: 0.8667 test_acc: 0.7630\n",
      "Epoch: 161 train_loss: 0.0917 train_acc: 0.9786 val_loss: 0.8121 val_acc: 0.7800 test_loss: 0.7127 test_acc: 0.7700\n",
      "Epoch: 162 train_loss: 0.0987 train_acc: 0.9786 val_loss: 0.9092 val_acc: 0.7580 test_loss: 0.8516 test_acc: 0.7580\n",
      "Epoch: 162 train_loss: 0.0987 train_acc: 0.9786 val_loss: 0.8261 val_acc: 0.7780 test_loss: 0.8516 test_acc: 0.7580\n",
      "Epoch: 162 train_loss: 0.0987 train_acc: 0.9786 val_loss: 0.8261 val_acc: 0.7780 test_loss: 0.7273 test_acc: 0.7700\n",
      "Epoch: 163 train_loss: 0.0988 train_acc: 0.9857 val_loss: 0.8889 val_acc: 0.7660 test_loss: 0.8325 test_acc: 0.7660\n",
      "Epoch: 163 train_loss: 0.0988 train_acc: 0.9857 val_loss: 0.8382 val_acc: 0.7700 test_loss: 0.8325 test_acc: 0.7660\n",
      "Epoch: 163 train_loss: 0.0988 train_acc: 0.9857 val_loss: 0.8382 val_acc: 0.7700 test_loss: 0.7357 test_acc: 0.7710\n",
      "Epoch: 164 train_loss: 0.0968 train_acc: 1.0000 val_loss: 0.8633 val_acc: 0.7720 test_loss: 0.8071 test_acc: 0.7780\n",
      "Epoch: 164 train_loss: 0.0968 train_acc: 1.0000 val_loss: 0.8388 val_acc: 0.7760 test_loss: 0.8071 test_acc: 0.7780\n",
      "Epoch: 164 train_loss: 0.0968 train_acc: 1.0000 val_loss: 0.8388 val_acc: 0.7760 test_loss: 0.7362 test_acc: 0.7780\n",
      "Epoch: 165 train_loss: 0.0951 train_acc: 0.9929 val_loss: 0.8318 val_acc: 0.7800 test_loss: 0.7746 test_acc: 0.7930\n",
      "Epoch: 165 train_loss: 0.0951 train_acc: 0.9929 val_loss: 0.8350 val_acc: 0.7740 test_loss: 0.7746 test_acc: 0.7930\n",
      "Epoch: 165 train_loss: 0.0951 train_acc: 0.9929 val_loss: 0.8350 val_acc: 0.7740 test_loss: 0.7331 test_acc: 0.7780\n",
      "Epoch: 166 train_loss: 0.1012 train_acc: 0.9786 val_loss: 0.8256 val_acc: 0.7860 test_loss: 0.7662 test_acc: 0.7940\n",
      "Epoch: 166 train_loss: 0.1012 train_acc: 0.9786 val_loss: 0.8371 val_acc: 0.7680 test_loss: 0.7662 test_acc: 0.7940\n",
      "Epoch: 166 train_loss: 0.1012 train_acc: 0.9786 val_loss: 0.8371 val_acc: 0.7680 test_loss: 0.7394 test_acc: 0.7760\n",
      "Epoch: 167 train_loss: 0.1074 train_acc: 0.9786 val_loss: 0.8284 val_acc: 0.7900 test_loss: 0.7673 test_acc: 0.7980\n",
      "Epoch: 167 train_loss: 0.1074 train_acc: 0.9786 val_loss: 0.8555 val_acc: 0.7620 test_loss: 0.7673 test_acc: 0.7980\n",
      "Epoch: 167 train_loss: 0.1074 train_acc: 0.9786 val_loss: 0.8555 val_acc: 0.7620 test_loss: 0.7603 test_acc: 0.7660\n",
      "Epoch: 168 train_loss: 0.1044 train_acc: 0.9786 val_loss: 0.8440 val_acc: 0.7800 test_loss: 0.7816 test_acc: 0.7950\n",
      "Epoch: 168 train_loss: 0.1044 train_acc: 0.9786 val_loss: 0.8540 val_acc: 0.7640 test_loss: 0.7816 test_acc: 0.7950\n",
      "Epoch: 168 train_loss: 0.1044 train_acc: 0.9786 val_loss: 0.8540 val_acc: 0.7640 test_loss: 0.7586 test_acc: 0.7730\n",
      "Epoch: 169 train_loss: 0.0942 train_acc: 0.9929 val_loss: 0.8457 val_acc: 0.7700 test_loss: 0.7862 test_acc: 0.7950\n",
      "Epoch: 169 train_loss: 0.0942 train_acc: 0.9929 val_loss: 0.8444 val_acc: 0.7720 test_loss: 0.7862 test_acc: 0.7950\n",
      "Epoch: 169 train_loss: 0.0942 train_acc: 0.9929 val_loss: 0.8444 val_acc: 0.7720 test_loss: 0.7426 test_acc: 0.7800\n",
      "Epoch: 170 train_loss: 0.0898 train_acc: 1.0000 val_loss: 0.8329 val_acc: 0.7780 test_loss: 0.7786 test_acc: 0.7940\n",
      "Epoch: 170 train_loss: 0.0898 train_acc: 1.0000 val_loss: 0.8348 val_acc: 0.7780 test_loss: 0.7786 test_acc: 0.7940\n",
      "Epoch: 170 train_loss: 0.0898 train_acc: 1.0000 val_loss: 0.8348 val_acc: 0.7780 test_loss: 0.7278 test_acc: 0.7800\n",
      "Epoch: 171 train_loss: 0.0893 train_acc: 1.0000 val_loss: 0.8359 val_acc: 0.7740 test_loss: 0.7864 test_acc: 0.7820\n",
      "Epoch: 171 train_loss: 0.0893 train_acc: 1.0000 val_loss: 0.8267 val_acc: 0.7900 test_loss: 0.7864 test_acc: 0.7820\n",
      "Epoch: 171 train_loss: 0.0893 train_acc: 1.0000 val_loss: 0.8267 val_acc: 0.7900 test_loss: 0.7158 test_acc: 0.7820\n",
      "Epoch: 172 train_loss: 0.0949 train_acc: 0.9857 val_loss: 0.8807 val_acc: 0.7540 test_loss: 0.8374 test_acc: 0.7620\n",
      "Epoch: 172 train_loss: 0.0949 train_acc: 0.9857 val_loss: 0.8287 val_acc: 0.7860 test_loss: 0.8374 test_acc: 0.7620\n",
      "Epoch: 172 train_loss: 0.0949 train_acc: 0.9857 val_loss: 0.8287 val_acc: 0.7860 test_loss: 0.7161 test_acc: 0.7800\n",
      "Epoch: 173 train_loss: 0.0942 train_acc: 0.9857 val_loss: 0.9158 val_acc: 0.7340 test_loss: 0.8762 test_acc: 0.7390\n",
      "Epoch: 173 train_loss: 0.0942 train_acc: 0.9857 val_loss: 0.8114 val_acc: 0.7740 test_loss: 0.8762 test_acc: 0.7390\n",
      "Epoch: 173 train_loss: 0.0942 train_acc: 0.9857 val_loss: 0.8114 val_acc: 0.7740 test_loss: 0.7035 test_acc: 0.7880\n",
      "Epoch: 174 train_loss: 0.0914 train_acc: 0.9929 val_loss: 0.9345 val_acc: 0.7420 test_loss: 0.8906 test_acc: 0.7430\n",
      "Epoch: 174 train_loss: 0.0914 train_acc: 0.9929 val_loss: 0.8002 val_acc: 0.7880 test_loss: 0.8906 test_acc: 0.7430\n",
      "Epoch: 174 train_loss: 0.0914 train_acc: 0.9929 val_loss: 0.8002 val_acc: 0.7880 test_loss: 0.7006 test_acc: 0.7840\n",
      "Epoch: 175 train_loss: 0.0902 train_acc: 0.9857 val_loss: 0.9006 val_acc: 0.7520 test_loss: 0.8515 test_acc: 0.7590\n",
      "Epoch: 175 train_loss: 0.0902 train_acc: 0.9857 val_loss: 0.7994 val_acc: 0.7900 test_loss: 0.8515 test_acc: 0.7590\n",
      "Epoch: 175 train_loss: 0.0902 train_acc: 0.9857 val_loss: 0.7994 val_acc: 0.7900 test_loss: 0.7080 test_acc: 0.7760\n",
      "Epoch: 176 train_loss: 0.0865 train_acc: 0.9857 val_loss: 0.8854 val_acc: 0.7460 test_loss: 0.8352 test_acc: 0.7500\n",
      "Epoch: 176 train_loss: 0.0865 train_acc: 0.9857 val_loss: 0.8057 val_acc: 0.7780 test_loss: 0.8352 test_acc: 0.7500\n",
      "Epoch: 176 train_loss: 0.0865 train_acc: 0.9857 val_loss: 0.8057 val_acc: 0.7780 test_loss: 0.7155 test_acc: 0.7770\n",
      "Epoch: 177 train_loss: 0.0877 train_acc: 0.9929 val_loss: 0.8521 val_acc: 0.7600 test_loss: 0.8030 test_acc: 0.7600\n",
      "Epoch: 177 train_loss: 0.0877 train_acc: 0.9929 val_loss: 0.8231 val_acc: 0.7760 test_loss: 0.8030 test_acc: 0.7600\n",
      "Epoch: 177 train_loss: 0.0877 train_acc: 0.9929 val_loss: 0.8231 val_acc: 0.7760 test_loss: 0.7321 test_acc: 0.7710\n",
      "Epoch: 178 train_loss: 0.0952 train_acc: 0.9929 val_loss: 0.8021 val_acc: 0.7800 test_loss: 0.7548 test_acc: 0.7690\n",
      "Epoch: 178 train_loss: 0.0952 train_acc: 0.9929 val_loss: 0.8411 val_acc: 0.7700 test_loss: 0.7548 test_acc: 0.7690\n",
      "Epoch: 178 train_loss: 0.0952 train_acc: 0.9929 val_loss: 0.8411 val_acc: 0.7700 test_loss: 0.7492 test_acc: 0.7730\n",
      "Epoch: 179 train_loss: 0.1014 train_acc: 1.0000 val_loss: 0.7824 val_acc: 0.7800 test_loss: 0.7395 test_acc: 0.7880\n",
      "Epoch: 179 train_loss: 0.1014 train_acc: 1.0000 val_loss: 0.8503 val_acc: 0.7720 test_loss: 0.7395 test_acc: 0.7880\n",
      "Epoch: 179 train_loss: 0.1014 train_acc: 1.0000 val_loss: 0.8503 val_acc: 0.7720 test_loss: 0.7593 test_acc: 0.7710\n",
      "Epoch: 180 train_loss: 0.1015 train_acc: 1.0000 val_loss: 0.7935 val_acc: 0.7820 test_loss: 0.7549 test_acc: 0.7890\n",
      "Epoch: 180 train_loss: 0.1015 train_acc: 1.0000 val_loss: 0.8436 val_acc: 0.7700 test_loss: 0.7549 test_acc: 0.7890\n",
      "Epoch: 180 train_loss: 0.1015 train_acc: 1.0000 val_loss: 0.8436 val_acc: 0.7700 test_loss: 0.7542 test_acc: 0.7690\n",
      "Epoch: 181 train_loss: 0.0983 train_acc: 0.9786 val_loss: 0.8229 val_acc: 0.7820 test_loss: 0.7849 test_acc: 0.7710\n",
      "Epoch: 181 train_loss: 0.0983 train_acc: 0.9786 val_loss: 0.8241 val_acc: 0.7680 test_loss: 0.7849 test_acc: 0.7710\n",
      "Epoch: 181 train_loss: 0.0983 train_acc: 0.9786 val_loss: 0.8241 val_acc: 0.7680 test_loss: 0.7342 test_acc: 0.7800\n",
      "Epoch: 182 train_loss: 0.0982 train_acc: 0.9929 val_loss: 0.8562 val_acc: 0.7700 test_loss: 0.8122 test_acc: 0.7550\n",
      "Epoch: 182 train_loss: 0.0982 train_acc: 0.9929 val_loss: 0.8094 val_acc: 0.7840 test_loss: 0.8122 test_acc: 0.7550\n",
      "Epoch: 182 train_loss: 0.0982 train_acc: 0.9929 val_loss: 0.8094 val_acc: 0.7840 test_loss: 0.7155 test_acc: 0.7830\n",
      "Epoch: 183 train_loss: 0.1020 train_acc: 0.9857 val_loss: 0.8551 val_acc: 0.7740 test_loss: 0.8120 test_acc: 0.7670\n",
      "Epoch: 183 train_loss: 0.1020 train_acc: 0.9857 val_loss: 0.8009 val_acc: 0.7880 test_loss: 0.8120 test_acc: 0.7670\n",
      "Epoch: 183 train_loss: 0.1020 train_acc: 0.9857 val_loss: 0.8009 val_acc: 0.7880 test_loss: 0.7040 test_acc: 0.7880\n",
      "Epoch: 184 train_loss: 0.1069 train_acc: 0.9857 val_loss: 0.8311 val_acc: 0.7880 test_loss: 0.7937 test_acc: 0.7730\n",
      "Epoch: 184 train_loss: 0.1069 train_acc: 0.9857 val_loss: 0.8158 val_acc: 0.7840 test_loss: 0.7937 test_acc: 0.7730\n",
      "Epoch: 184 train_loss: 0.1069 train_acc: 0.9857 val_loss: 0.8158 val_acc: 0.7840 test_loss: 0.7156 test_acc: 0.7850\n",
      "Epoch: 185 train_loss: 0.1063 train_acc: 0.9929 val_loss: 0.8256 val_acc: 0.7740 test_loss: 0.7932 test_acc: 0.7750\n",
      "Epoch: 185 train_loss: 0.1063 train_acc: 0.9929 val_loss: 0.8242 val_acc: 0.7780 test_loss: 0.7932 test_acc: 0.7750\n",
      "Epoch: 185 train_loss: 0.1063 train_acc: 0.9929 val_loss: 0.8242 val_acc: 0.7780 test_loss: 0.7215 test_acc: 0.7880\n",
      "Epoch: 186 train_loss: 0.1032 train_acc: 0.9857 val_loss: 0.8505 val_acc: 0.7760 test_loss: 0.8210 test_acc: 0.7680\n",
      "Epoch: 186 train_loss: 0.1032 train_acc: 0.9857 val_loss: 0.8250 val_acc: 0.7720 test_loss: 0.8210 test_acc: 0.7680\n",
      "Epoch: 186 train_loss: 0.1032 train_acc: 0.9857 val_loss: 0.8250 val_acc: 0.7720 test_loss: 0.7238 test_acc: 0.7850\n",
      "Epoch: 187 train_loss: 0.1038 train_acc: 0.9929 val_loss: 0.8502 val_acc: 0.7800 test_loss: 0.8243 test_acc: 0.7700\n",
      "Epoch: 187 train_loss: 0.1038 train_acc: 0.9929 val_loss: 0.8354 val_acc: 0.7640 test_loss: 0.8243 test_acc: 0.7700\n",
      "Epoch: 187 train_loss: 0.1038 train_acc: 0.9929 val_loss: 0.8354 val_acc: 0.7640 test_loss: 0.7349 test_acc: 0.7770\n",
      "Epoch: 188 train_loss: 0.1051 train_acc: 0.9929 val_loss: 0.8589 val_acc: 0.7800 test_loss: 0.8346 test_acc: 0.7710\n",
      "Epoch: 188 train_loss: 0.1051 train_acc: 0.9929 val_loss: 0.8313 val_acc: 0.7620 test_loss: 0.8346 test_acc: 0.7710\n",
      "Epoch: 188 train_loss: 0.1051 train_acc: 0.9929 val_loss: 0.8313 val_acc: 0.7620 test_loss: 0.7313 test_acc: 0.7740\n",
      "Epoch: 189 train_loss: 0.1102 train_acc: 0.9929 val_loss: 0.8522 val_acc: 0.7940 test_loss: 0.8272 test_acc: 0.7840\n",
      "Epoch: 189 train_loss: 0.1102 train_acc: 0.9929 val_loss: 0.8337 val_acc: 0.7540 test_loss: 0.8272 test_acc: 0.7840\n",
      "Epoch: 189 train_loss: 0.1102 train_acc: 0.9929 val_loss: 0.8337 val_acc: 0.7540 test_loss: 0.7332 test_acc: 0.7710\n",
      "Epoch: 190 train_loss: 0.1109 train_acc: 0.9929 val_loss: 0.8756 val_acc: 0.7880 test_loss: 0.8495 test_acc: 0.7720\n",
      "Epoch: 190 train_loss: 0.1109 train_acc: 0.9929 val_loss: 0.8366 val_acc: 0.7620 test_loss: 0.8495 test_acc: 0.7720\n",
      "Epoch: 190 train_loss: 0.1109 train_acc: 0.9929 val_loss: 0.8366 val_acc: 0.7620 test_loss: 0.7359 test_acc: 0.7740\n",
      "Epoch: 191 train_loss: 0.1073 train_acc: 0.9929 val_loss: 0.8866 val_acc: 0.7740 test_loss: 0.8585 test_acc: 0.7690\n",
      "Epoch: 191 train_loss: 0.1073 train_acc: 0.9929 val_loss: 0.8281 val_acc: 0.7540 test_loss: 0.8585 test_acc: 0.7690\n",
      "Epoch: 191 train_loss: 0.1073 train_acc: 0.9929 val_loss: 0.8281 val_acc: 0.7540 test_loss: 0.7275 test_acc: 0.7760\n",
      "Epoch: 192 train_loss: 0.1005 train_acc: 0.9929 val_loss: 0.8788 val_acc: 0.7760 test_loss: 0.8463 test_acc: 0.7780\n",
      "Epoch: 192 train_loss: 0.1005 train_acc: 0.9929 val_loss: 0.8237 val_acc: 0.7660 test_loss: 0.8463 test_acc: 0.7780\n",
      "Epoch: 192 train_loss: 0.1005 train_acc: 0.9929 val_loss: 0.8237 val_acc: 0.7660 test_loss: 0.7229 test_acc: 0.7750\n",
      "Epoch: 193 train_loss: 0.0956 train_acc: 0.9929 val_loss: 0.8652 val_acc: 0.7700 test_loss: 0.8298 test_acc: 0.7780\n",
      "Epoch: 193 train_loss: 0.0956 train_acc: 0.9929 val_loss: 0.8191 val_acc: 0.7680 test_loss: 0.8298 test_acc: 0.7780\n",
      "Epoch: 193 train_loss: 0.0956 train_acc: 0.9929 val_loss: 0.8191 val_acc: 0.7680 test_loss: 0.7201 test_acc: 0.7770\n",
      "Epoch: 194 train_loss: 0.0929 train_acc: 1.0000 val_loss: 0.8305 val_acc: 0.7740 test_loss: 0.7937 test_acc: 0.7770\n",
      "Epoch: 194 train_loss: 0.0929 train_acc: 1.0000 val_loss: 0.8184 val_acc: 0.7660 test_loss: 0.7937 test_acc: 0.7770\n",
      "Epoch: 194 train_loss: 0.0929 train_acc: 1.0000 val_loss: 0.8184 val_acc: 0.7660 test_loss: 0.7211 test_acc: 0.7780\n",
      "Epoch: 195 train_loss: 0.0907 train_acc: 1.0000 val_loss: 0.8249 val_acc: 0.7760 test_loss: 0.7896 test_acc: 0.7810\n",
      "Epoch: 195 train_loss: 0.0907 train_acc: 1.0000 val_loss: 0.8267 val_acc: 0.7740 test_loss: 0.7896 test_acc: 0.7810\n",
      "Epoch: 195 train_loss: 0.0907 train_acc: 1.0000 val_loss: 0.8267 val_acc: 0.7740 test_loss: 0.7331 test_acc: 0.7750\n",
      "Epoch: 196 train_loss: 0.0901 train_acc: 0.9857 val_loss: 0.7980 val_acc: 0.7880 test_loss: 0.7631 test_acc: 0.7790\n",
      "Epoch: 196 train_loss: 0.0901 train_acc: 0.9857 val_loss: 0.8393 val_acc: 0.7620 test_loss: 0.7631 test_acc: 0.7790\n",
      "Epoch: 196 train_loss: 0.0901 train_acc: 0.9857 val_loss: 0.8393 val_acc: 0.7620 test_loss: 0.7501 test_acc: 0.7710\n",
      "Epoch: 197 train_loss: 0.0897 train_acc: 0.9857 val_loss: 0.7683 val_acc: 0.7960 test_loss: 0.7337 test_acc: 0.7890\n",
      "Epoch: 197 train_loss: 0.0897 train_acc: 0.9857 val_loss: 0.8478 val_acc: 0.7680 test_loss: 0.7337 test_acc: 0.7890\n",
      "Epoch: 197 train_loss: 0.0897 train_acc: 0.9857 val_loss: 0.8478 val_acc: 0.7680 test_loss: 0.7639 test_acc: 0.7730\n",
      "Epoch: 198 train_loss: 0.0891 train_acc: 0.9857 val_loss: 0.7549 val_acc: 0.7960 test_loss: 0.7199 test_acc: 0.7900\n",
      "Epoch: 198 train_loss: 0.0891 train_acc: 0.9857 val_loss: 0.8489 val_acc: 0.7620 test_loss: 0.7199 test_acc: 0.7900\n",
      "Epoch: 198 train_loss: 0.0891 train_acc: 0.9857 val_loss: 0.8489 val_acc: 0.7620 test_loss: 0.7662 test_acc: 0.7690\n",
      "Epoch: 199 train_loss: 0.0837 train_acc: 0.9857 val_loss: 0.7735 val_acc: 0.7920 test_loss: 0.7368 test_acc: 0.7960\n",
      "Epoch: 199 train_loss: 0.0837 train_acc: 0.9857 val_loss: 0.8265 val_acc: 0.7720 test_loss: 0.7368 test_acc: 0.7960\n",
      "Epoch: 199 train_loss: 0.0837 train_acc: 0.9857 val_loss: 0.8265 val_acc: 0.7720 test_loss: 0.7442 test_acc: 0.7700\n",
      "Epoch: 200 train_loss: 0.0793 train_acc: 0.9857 val_loss: 0.8055 val_acc: 0.7960 test_loss: 0.7668 test_acc: 0.7930\n",
      "Epoch: 200 train_loss: 0.0793 train_acc: 0.9857 val_loss: 0.8084 val_acc: 0.7820 test_loss: 0.7668 test_acc: 0.7930\n",
      "Epoch: 200 train_loss: 0.0793 train_acc: 0.9857 val_loss: 0.8084 val_acc: 0.7820 test_loss: 0.7257 test_acc: 0.7720\n",
      "Epoch: 201 train_loss: 0.1037 train_acc: 0.9857 val_loss: 0.8537 val_acc: 0.7620 test_loss: 0.8156 test_acc: 0.7720\n",
      "Epoch: 201 train_loss: 0.1037 train_acc: 0.9857 val_loss: 0.8074 val_acc: 0.7960 test_loss: 0.8156 test_acc: 0.7720\n",
      "Epoch: 201 train_loss: 0.1037 train_acc: 0.9857 val_loss: 0.8074 val_acc: 0.7960 test_loss: 0.7231 test_acc: 0.7840\n",
      "Epoch: 202 train_loss: 0.1279 train_acc: 0.9786 val_loss: 0.9137 val_acc: 0.7280 test_loss: 0.8799 test_acc: 0.7320\n",
      "Epoch: 202 train_loss: 0.1279 train_acc: 0.9786 val_loss: 0.8119 val_acc: 0.7840 test_loss: 0.8799 test_acc: 0.7320\n",
      "Epoch: 202 train_loss: 0.1279 train_acc: 0.9786 val_loss: 0.8119 val_acc: 0.7840 test_loss: 0.7215 test_acc: 0.7790\n",
      "Epoch: 203 train_loss: 0.1478 train_acc: 0.9714 val_loss: 0.9540 val_acc: 0.7000 test_loss: 0.9235 test_acc: 0.7090\n",
      "Epoch: 203 train_loss: 0.1478 train_acc: 0.9714 val_loss: 0.8105 val_acc: 0.7900 test_loss: 0.9235 test_acc: 0.7090\n",
      "Epoch: 203 train_loss: 0.1478 train_acc: 0.9714 val_loss: 0.8105 val_acc: 0.7900 test_loss: 0.7134 test_acc: 0.7780\n",
      "Epoch: 204 train_loss: 0.1620 train_acc: 0.9643 val_loss: 0.9219 val_acc: 0.7220 test_loss: 0.8939 test_acc: 0.7260\n",
      "Epoch: 204 train_loss: 0.1620 train_acc: 0.9643 val_loss: 0.8143 val_acc: 0.7880 test_loss: 0.8939 test_acc: 0.7260\n",
      "Epoch: 204 train_loss: 0.1620 train_acc: 0.9643 val_loss: 0.8143 val_acc: 0.7880 test_loss: 0.7115 test_acc: 0.7840\n",
      "Epoch: 205 train_loss: 0.1711 train_acc: 0.9643 val_loss: 0.8758 val_acc: 0.7680 test_loss: 0.8500 test_acc: 0.7580\n",
      "Epoch: 205 train_loss: 0.1711 train_acc: 0.9643 val_loss: 0.8178 val_acc: 0.7880 test_loss: 0.8500 test_acc: 0.7580\n",
      "Epoch: 205 train_loss: 0.1711 train_acc: 0.9643 val_loss: 0.8178 val_acc: 0.7880 test_loss: 0.7094 test_acc: 0.7830\n",
      "Epoch: 206 train_loss: 0.1722 train_acc: 0.9714 val_loss: 0.8517 val_acc: 0.7760 test_loss: 0.8262 test_acc: 0.7570\n",
      "Epoch: 206 train_loss: 0.1722 train_acc: 0.9714 val_loss: 0.8158 val_acc: 0.7860 test_loss: 0.8262 test_acc: 0.7570\n",
      "Epoch: 206 train_loss: 0.1722 train_acc: 0.9714 val_loss: 0.8158 val_acc: 0.7860 test_loss: 0.7044 test_acc: 0.7860\n",
      "Epoch: 207 train_loss: 0.1589 train_acc: 0.9714 val_loss: 0.8269 val_acc: 0.7700 test_loss: 0.8016 test_acc: 0.7550\n",
      "Epoch: 207 train_loss: 0.1589 train_acc: 0.9714 val_loss: 0.8117 val_acc: 0.7800 test_loss: 0.8016 test_acc: 0.7550\n",
      "Epoch: 207 train_loss: 0.1589 train_acc: 0.9714 val_loss: 0.8117 val_acc: 0.7800 test_loss: 0.7021 test_acc: 0.7860\n",
      "Epoch: 208 train_loss: 0.1409 train_acc: 0.9857 val_loss: 0.7900 val_acc: 0.7860 test_loss: 0.7654 test_acc: 0.7880\n",
      "Epoch: 208 train_loss: 0.1409 train_acc: 0.9857 val_loss: 0.8167 val_acc: 0.7860 test_loss: 0.7654 test_acc: 0.7880\n",
      "Epoch: 208 train_loss: 0.1409 train_acc: 0.9857 val_loss: 0.8167 val_acc: 0.7860 test_loss: 0.7128 test_acc: 0.7830\n",
      "Epoch: 209 train_loss: 0.1265 train_acc: 1.0000 val_loss: 0.7774 val_acc: 0.8020 test_loss: 0.7536 test_acc: 0.8010\n",
      "Epoch: 209 train_loss: 0.1265 train_acc: 1.0000 val_loss: 0.8355 val_acc: 0.7840 test_loss: 0.7536 test_acc: 0.8010\n",
      "Epoch: 209 train_loss: 0.1265 train_acc: 1.0000 val_loss: 0.8355 val_acc: 0.7840 test_loss: 0.7403 test_acc: 0.7800\n",
      "Epoch: 210 train_loss: 0.1191 train_acc: 0.9857 val_loss: 0.7719 val_acc: 0.8100 test_loss: 0.7476 test_acc: 0.8090\n",
      "Epoch: 210 train_loss: 0.1191 train_acc: 0.9857 val_loss: 0.8542 val_acc: 0.7760 test_loss: 0.7476 test_acc: 0.8090\n",
      "Epoch: 210 train_loss: 0.1191 train_acc: 0.9857 val_loss: 0.8542 val_acc: 0.7760 test_loss: 0.7643 test_acc: 0.7770\n",
      "Epoch: 211 train_loss: 0.1147 train_acc: 0.9857 val_loss: 0.7853 val_acc: 0.8060 test_loss: 0.7629 test_acc: 0.8040\n",
      "Epoch: 211 train_loss: 0.1147 train_acc: 0.9857 val_loss: 0.8509 val_acc: 0.7680 test_loss: 0.7629 test_acc: 0.8040\n",
      "Epoch: 211 train_loss: 0.1147 train_acc: 0.9857 val_loss: 0.8509 val_acc: 0.7680 test_loss: 0.7658 test_acc: 0.7670\n",
      "Epoch: 212 train_loss: 0.1171 train_acc: 0.9714 val_loss: 0.8074 val_acc: 0.8000 test_loss: 0.7850 test_acc: 0.7920\n",
      "Epoch: 212 train_loss: 0.1171 train_acc: 0.9714 val_loss: 0.8657 val_acc: 0.7600 test_loss: 0.7850 test_acc: 0.7920\n",
      "Epoch: 212 train_loss: 0.1171 train_acc: 0.9714 val_loss: 0.8657 val_acc: 0.7600 test_loss: 0.7820 test_acc: 0.7650\n",
      "Epoch: 213 train_loss: 0.1203 train_acc: 0.9714 val_loss: 0.8287 val_acc: 0.7880 test_loss: 0.8057 test_acc: 0.7780\n",
      "Epoch: 213 train_loss: 0.1203 train_acc: 0.9714 val_loss: 0.8879 val_acc: 0.7600 test_loss: 0.8057 test_acc: 0.7780\n",
      "Epoch: 213 train_loss: 0.1203 train_acc: 0.9714 val_loss: 0.8879 val_acc: 0.7600 test_loss: 0.8057 test_acc: 0.7580\n",
      "Epoch: 214 train_loss: 0.1084 train_acc: 0.9786 val_loss: 0.8367 val_acc: 0.7820 test_loss: 0.8126 test_acc: 0.7670\n",
      "Epoch: 214 train_loss: 0.1084 train_acc: 0.9786 val_loss: 0.8891 val_acc: 0.7580 test_loss: 0.8126 test_acc: 0.7670\n",
      "Epoch: 214 train_loss: 0.1084 train_acc: 0.9786 val_loss: 0.8891 val_acc: 0.7580 test_loss: 0.8028 test_acc: 0.7540\n",
      "Epoch: 215 train_loss: 0.0948 train_acc: 0.9929 val_loss: 0.8439 val_acc: 0.7800 test_loss: 0.8184 test_acc: 0.7730\n",
      "Epoch: 215 train_loss: 0.0948 train_acc: 0.9929 val_loss: 0.8684 val_acc: 0.7740 test_loss: 0.8184 test_acc: 0.7730\n",
      "Epoch: 215 train_loss: 0.0948 train_acc: 0.9929 val_loss: 0.8684 val_acc: 0.7740 test_loss: 0.7780 test_acc: 0.7660\n",
      "Epoch: 216 train_loss: 0.0881 train_acc: 0.9857 val_loss: 0.8547 val_acc: 0.7640 test_loss: 0.8271 test_acc: 0.7730\n",
      "Epoch: 216 train_loss: 0.0881 train_acc: 0.9857 val_loss: 0.8546 val_acc: 0.7800 test_loss: 0.8271 test_acc: 0.7730\n",
      "Epoch: 216 train_loss: 0.0881 train_acc: 0.9857 val_loss: 0.8546 val_acc: 0.7800 test_loss: 0.7594 test_acc: 0.7710\n",
      "Epoch: 217 train_loss: 0.0877 train_acc: 0.9857 val_loss: 0.8509 val_acc: 0.7720 test_loss: 0.8214 test_acc: 0.7690\n",
      "Epoch: 217 train_loss: 0.0877 train_acc: 0.9857 val_loss: 0.8495 val_acc: 0.7820 test_loss: 0.8214 test_acc: 0.7690\n",
      "Epoch: 217 train_loss: 0.0877 train_acc: 0.9857 val_loss: 0.8495 val_acc: 0.7820 test_loss: 0.7477 test_acc: 0.7710\n",
      "Epoch: 218 train_loss: 0.0895 train_acc: 0.9857 val_loss: 0.8464 val_acc: 0.7820 test_loss: 0.8181 test_acc: 0.7710\n",
      "Epoch: 218 train_loss: 0.0895 train_acc: 0.9857 val_loss: 0.8489 val_acc: 0.7820 test_loss: 0.8181 test_acc: 0.7710\n",
      "Epoch: 218 train_loss: 0.0895 train_acc: 0.9857 val_loss: 0.8489 val_acc: 0.7820 test_loss: 0.7391 test_acc: 0.7780\n",
      "Epoch: 219 train_loss: 0.0949 train_acc: 0.9857 val_loss: 0.8208 val_acc: 0.8020 test_loss: 0.7945 test_acc: 0.7980\n",
      "Epoch: 219 train_loss: 0.0949 train_acc: 0.9857 val_loss: 0.8472 val_acc: 0.7820 test_loss: 0.7945 test_acc: 0.7980\n",
      "Epoch: 219 train_loss: 0.0949 train_acc: 0.9857 val_loss: 0.8472 val_acc: 0.7820 test_loss: 0.7310 test_acc: 0.7800\n",
      "Epoch: 220 train_loss: 0.0997 train_acc: 0.9857 val_loss: 0.8112 val_acc: 0.8040 test_loss: 0.7870 test_acc: 0.7990\n",
      "Epoch: 220 train_loss: 0.0997 train_acc: 0.9857 val_loss: 0.8353 val_acc: 0.7780 test_loss: 0.7870 test_acc: 0.7990\n",
      "Epoch: 220 train_loss: 0.0997 train_acc: 0.9857 val_loss: 0.8353 val_acc: 0.7780 test_loss: 0.7136 test_acc: 0.7860\n",
      "Epoch: 221 train_loss: 0.0981 train_acc: 0.9857 val_loss: 0.8408 val_acc: 0.7960 test_loss: 0.8159 test_acc: 0.7930\n",
      "Epoch: 221 train_loss: 0.0981 train_acc: 0.9857 val_loss: 0.8400 val_acc: 0.7740 test_loss: 0.8159 test_acc: 0.7930\n",
      "Epoch: 221 train_loss: 0.0981 train_acc: 0.9857 val_loss: 0.8400 val_acc: 0.7740 test_loss: 0.7153 test_acc: 0.7880\n",
      "Epoch: 222 train_loss: 0.0922 train_acc: 0.9929 val_loss: 0.8901 val_acc: 0.7820 test_loss: 0.8635 test_acc: 0.7800\n",
      "Epoch: 222 train_loss: 0.0922 train_acc: 0.9929 val_loss: 0.8491 val_acc: 0.7780 test_loss: 0.8635 test_acc: 0.7800\n",
      "Epoch: 222 train_loss: 0.0922 train_acc: 0.9929 val_loss: 0.8491 val_acc: 0.7780 test_loss: 0.7210 test_acc: 0.7920\n",
      "Epoch: 223 train_loss: 0.0768 train_acc: 0.9929 val_loss: 0.9066 val_acc: 0.7560 test_loss: 0.8743 test_acc: 0.7640\n",
      "Epoch: 223 train_loss: 0.0768 train_acc: 0.9929 val_loss: 0.8390 val_acc: 0.8020 test_loss: 0.8743 test_acc: 0.7640\n",
      "Epoch: 223 train_loss: 0.0768 train_acc: 0.9929 val_loss: 0.8390 val_acc: 0.8020 test_loss: 0.7093 test_acc: 0.8010\n",
      "Epoch: 224 train_loss: 0.0632 train_acc: 0.9929 val_loss: 0.9013 val_acc: 0.7660 test_loss: 0.8633 test_acc: 0.7690\n",
      "Epoch: 224 train_loss: 0.0632 train_acc: 0.9929 val_loss: 0.8240 val_acc: 0.8000 test_loss: 0.8633 test_acc: 0.7690\n",
      "Epoch: 224 train_loss: 0.0632 train_acc: 0.9929 val_loss: 0.8240 val_acc: 0.8000 test_loss: 0.6957 test_acc: 0.7990\n",
      "Epoch: 225 train_loss: 0.0568 train_acc: 0.9929 val_loss: 0.8885 val_acc: 0.7880 test_loss: 0.8480 test_acc: 0.7800\n",
      "Epoch: 225 train_loss: 0.0568 train_acc: 0.9929 val_loss: 0.8215 val_acc: 0.8100 test_loss: 0.8480 test_acc: 0.7800\n",
      "Epoch: 225 train_loss: 0.0568 train_acc: 0.9929 val_loss: 0.8215 val_acc: 0.8100 test_loss: 0.6926 test_acc: 0.7990\n",
      "Epoch: 226 train_loss: 0.0568 train_acc: 0.9929 val_loss: 0.8857 val_acc: 0.7800 test_loss: 0.8405 test_acc: 0.7850\n",
      "Epoch: 226 train_loss: 0.0568 train_acc: 0.9929 val_loss: 0.7837 val_acc: 0.8160 test_loss: 0.8405 test_acc: 0.7850\n",
      "Epoch: 226 train_loss: 0.0568 train_acc: 0.9929 val_loss: 0.7837 val_acc: 0.8160 test_loss: 0.6519 test_acc: 0.7960\n",
      "Epoch: 227 train_loss: 0.0615 train_acc: 0.9929 val_loss: 0.8952 val_acc: 0.7780 test_loss: 0.8456 test_acc: 0.7820\n",
      "Epoch: 227 train_loss: 0.0615 train_acc: 0.9929 val_loss: 0.7962 val_acc: 0.8140 test_loss: 0.8456 test_acc: 0.7820\n",
      "Epoch: 227 train_loss: 0.0615 train_acc: 0.9929 val_loss: 0.7962 val_acc: 0.8140 test_loss: 0.6658 test_acc: 0.8000\n",
      "Epoch: 228 train_loss: 0.0704 train_acc: 0.9929 val_loss: 0.8568 val_acc: 0.7900 test_loss: 0.8095 test_acc: 0.7870\n",
      "Epoch: 228 train_loss: 0.0704 train_acc: 0.9929 val_loss: 0.8079 val_acc: 0.8100 test_loss: 0.8095 test_acc: 0.7870\n",
      "Epoch: 228 train_loss: 0.0704 train_acc: 0.9929 val_loss: 0.8079 val_acc: 0.8100 test_loss: 0.6796 test_acc: 0.7960\n",
      "Epoch: 229 train_loss: 0.0748 train_acc: 0.9929 val_loss: 0.8519 val_acc: 0.8000 test_loss: 0.8104 test_acc: 0.7880\n",
      "Epoch: 229 train_loss: 0.0748 train_acc: 0.9929 val_loss: 0.8196 val_acc: 0.8060 test_loss: 0.8104 test_acc: 0.7880\n",
      "Epoch: 229 train_loss: 0.0748 train_acc: 0.9929 val_loss: 0.8196 val_acc: 0.8060 test_loss: 0.6917 test_acc: 0.7940\n",
      "Epoch: 230 train_loss: 0.0764 train_acc: 0.9857 val_loss: 0.8530 val_acc: 0.8000 test_loss: 0.8140 test_acc: 0.7860\n",
      "Epoch: 230 train_loss: 0.0764 train_acc: 0.9857 val_loss: 0.8372 val_acc: 0.7840 test_loss: 0.8140 test_acc: 0.7860\n",
      "Epoch: 230 train_loss: 0.0764 train_acc: 0.9857 val_loss: 0.8372 val_acc: 0.7840 test_loss: 0.7055 test_acc: 0.7930\n",
      "Epoch: 231 train_loss: 0.0807 train_acc: 0.9857 val_loss: 0.8533 val_acc: 0.7960 test_loss: 0.8182 test_acc: 0.7800\n",
      "Epoch: 231 train_loss: 0.0807 train_acc: 0.9857 val_loss: 0.8298 val_acc: 0.7840 test_loss: 0.8182 test_acc: 0.7800\n",
      "Epoch: 231 train_loss: 0.0807 train_acc: 0.9857 val_loss: 0.8298 val_acc: 0.7840 test_loss: 0.6970 test_acc: 0.7890\n",
      "Epoch: 232 train_loss: 0.0880 train_acc: 0.9786 val_loss: 0.8634 val_acc: 0.7940 test_loss: 0.8310 test_acc: 0.7710\n",
      "Epoch: 232 train_loss: 0.0880 train_acc: 0.9786 val_loss: 0.8333 val_acc: 0.7920 test_loss: 0.8310 test_acc: 0.7710\n",
      "Epoch: 232 train_loss: 0.0880 train_acc: 0.9786 val_loss: 0.8333 val_acc: 0.7920 test_loss: 0.6993 test_acc: 0.7910\n",
      "Epoch: 233 train_loss: 0.0937 train_acc: 0.9786 val_loss: 0.9003 val_acc: 0.7660 test_loss: 0.8714 test_acc: 0.7470\n",
      "Epoch: 233 train_loss: 0.0937 train_acc: 0.9786 val_loss: 0.8338 val_acc: 0.7880 test_loss: 0.8714 test_acc: 0.7470\n",
      "Epoch: 233 train_loss: 0.0937 train_acc: 0.9786 val_loss: 0.8338 val_acc: 0.7880 test_loss: 0.6953 test_acc: 0.7940\n",
      "Epoch: 234 train_loss: 0.0968 train_acc: 0.9857 val_loss: 0.9568 val_acc: 0.7440 test_loss: 0.9326 test_acc: 0.7200\n",
      "Epoch: 234 train_loss: 0.0968 train_acc: 0.9857 val_loss: 0.8374 val_acc: 0.7880 test_loss: 0.9326 test_acc: 0.7200\n",
      "Epoch: 234 train_loss: 0.0968 train_acc: 0.9857 val_loss: 0.8374 val_acc: 0.7880 test_loss: 0.6946 test_acc: 0.7940\n",
      "Epoch: 235 train_loss: 0.0970 train_acc: 0.9857 val_loss: 0.9705 val_acc: 0.7300 test_loss: 0.9460 test_acc: 0.7190\n",
      "Epoch: 235 train_loss: 0.0970 train_acc: 0.9857 val_loss: 0.8339 val_acc: 0.7860 test_loss: 0.9460 test_acc: 0.7190\n",
      "Epoch: 235 train_loss: 0.0970 train_acc: 0.9857 val_loss: 0.8339 val_acc: 0.7860 test_loss: 0.6877 test_acc: 0.7980\n",
      "Epoch: 236 train_loss: 0.0947 train_acc: 0.9857 val_loss: 0.9479 val_acc: 0.7500 test_loss: 0.9207 test_acc: 0.7370\n",
      "Epoch: 236 train_loss: 0.0947 train_acc: 0.9857 val_loss: 0.8264 val_acc: 0.7840 test_loss: 0.9207 test_acc: 0.7370\n",
      "Epoch: 236 train_loss: 0.0947 train_acc: 0.9857 val_loss: 0.8264 val_acc: 0.7840 test_loss: 0.6790 test_acc: 0.7980\n",
      "Epoch: 237 train_loss: 0.0854 train_acc: 0.9857 val_loss: 0.8916 val_acc: 0.7620 test_loss: 0.8586 test_acc: 0.7550\n",
      "Epoch: 237 train_loss: 0.0854 train_acc: 0.9857 val_loss: 0.8244 val_acc: 0.7820 test_loss: 0.8586 test_acc: 0.7550\n",
      "Epoch: 237 train_loss: 0.0854 train_acc: 0.9857 val_loss: 0.8244 val_acc: 0.7820 test_loss: 0.6808 test_acc: 0.7940\n",
      "Epoch: 238 train_loss: 0.0797 train_acc: 0.9929 val_loss: 0.8161 val_acc: 0.7740 test_loss: 0.7799 test_acc: 0.7790\n",
      "Epoch: 238 train_loss: 0.0797 train_acc: 0.9929 val_loss: 0.8303 val_acc: 0.7860 test_loss: 0.7799 test_acc: 0.7790\n",
      "Epoch: 238 train_loss: 0.0797 train_acc: 0.9929 val_loss: 0.8303 val_acc: 0.7860 test_loss: 0.6906 test_acc: 0.7940\n",
      "Epoch: 239 train_loss: 0.0794 train_acc: 0.9929 val_loss: 0.7740 val_acc: 0.7880 test_loss: 0.7375 test_acc: 0.8040\n",
      "Epoch: 239 train_loss: 0.0794 train_acc: 0.9929 val_loss: 0.8446 val_acc: 0.7820 test_loss: 0.7375 test_acc: 0.8040\n",
      "Epoch: 239 train_loss: 0.0794 train_acc: 0.9929 val_loss: 0.8446 val_acc: 0.7820 test_loss: 0.7058 test_acc: 0.7870\n",
      "Epoch: 240 train_loss: 0.0772 train_acc: 1.0000 val_loss: 0.7684 val_acc: 0.7980 test_loss: 0.7326 test_acc: 0.8110\n",
      "Epoch: 240 train_loss: 0.0772 train_acc: 1.0000 val_loss: 0.8435 val_acc: 0.7840 test_loss: 0.7326 test_acc: 0.8110\n",
      "Epoch: 240 train_loss: 0.0772 train_acc: 1.0000 val_loss: 0.8435 val_acc: 0.7840 test_loss: 0.7073 test_acc: 0.7880\n",
      "Epoch: 241 train_loss: 0.0725 train_acc: 1.0000 val_loss: 0.7787 val_acc: 0.8140 test_loss: 0.7453 test_acc: 0.8150\n",
      "Epoch: 241 train_loss: 0.0725 train_acc: 1.0000 val_loss: 0.8258 val_acc: 0.7860 test_loss: 0.7453 test_acc: 0.8150\n",
      "Epoch: 241 train_loss: 0.0725 train_acc: 1.0000 val_loss: 0.8258 val_acc: 0.7860 test_loss: 0.6947 test_acc: 0.7960\n",
      "Epoch: 242 train_loss: 0.0700 train_acc: 1.0000 val_loss: 0.8245 val_acc: 0.7840 test_loss: 0.7929 test_acc: 0.7920\n",
      "Epoch: 242 train_loss: 0.0700 train_acc: 1.0000 val_loss: 0.8018 val_acc: 0.7980 test_loss: 0.7929 test_acc: 0.7920\n",
      "Epoch: 242 train_loss: 0.0700 train_acc: 1.0000 val_loss: 0.8018 val_acc: 0.7980 test_loss: 0.6755 test_acc: 0.8030\n",
      "Epoch: 243 train_loss: 0.0702 train_acc: 1.0000 val_loss: 0.8704 val_acc: 0.7680 test_loss: 0.8421 test_acc: 0.7650\n",
      "Epoch: 243 train_loss: 0.0702 train_acc: 1.0000 val_loss: 0.7986 val_acc: 0.8000 test_loss: 0.8421 test_acc: 0.7650\n",
      "Epoch: 243 train_loss: 0.0702 train_acc: 1.0000 val_loss: 0.7986 val_acc: 0.8000 test_loss: 0.6766 test_acc: 0.8020\n",
      "Epoch: 244 train_loss: 0.0706 train_acc: 1.0000 val_loss: 0.8936 val_acc: 0.7580 test_loss: 0.8718 test_acc: 0.7470\n",
      "Epoch: 244 train_loss: 0.0706 train_acc: 1.0000 val_loss: 0.7898 val_acc: 0.8000 test_loss: 0.8718 test_acc: 0.7470\n",
      "Epoch: 244 train_loss: 0.0706 train_acc: 1.0000 val_loss: 0.7898 val_acc: 0.8000 test_loss: 0.6704 test_acc: 0.8010\n",
      "Epoch: 245 train_loss: 0.0667 train_acc: 1.0000 val_loss: 0.9155 val_acc: 0.7580 test_loss: 0.9001 test_acc: 0.7480\n",
      "Epoch: 245 train_loss: 0.0667 train_acc: 1.0000 val_loss: 0.8251 val_acc: 0.7820 test_loss: 0.9001 test_acc: 0.7480\n",
      "Epoch: 245 train_loss: 0.0667 train_acc: 1.0000 val_loss: 0.8251 val_acc: 0.7820 test_loss: 0.7054 test_acc: 0.7890\n",
      "Epoch: 246 train_loss: 0.0702 train_acc: 1.0000 val_loss: 0.9173 val_acc: 0.7560 test_loss: 0.9022 test_acc: 0.7420\n",
      "Epoch: 246 train_loss: 0.0702 train_acc: 1.0000 val_loss: 0.8731 val_acc: 0.7780 test_loss: 0.9022 test_acc: 0.7420\n",
      "Epoch: 246 train_loss: 0.0702 train_acc: 1.0000 val_loss: 0.8731 val_acc: 0.7780 test_loss: 0.7513 test_acc: 0.7850\n",
      "Epoch: 247 train_loss: 0.0757 train_acc: 1.0000 val_loss: 0.9023 val_acc: 0.7480 test_loss: 0.8790 test_acc: 0.7400\n",
      "Epoch: 247 train_loss: 0.0757 train_acc: 1.0000 val_loss: 0.9050 val_acc: 0.7620 test_loss: 0.8790 test_acc: 0.7400\n",
      "Epoch: 247 train_loss: 0.0757 train_acc: 1.0000 val_loss: 0.9050 val_acc: 0.7620 test_loss: 0.7815 test_acc: 0.7730\n",
      "Epoch: 248 train_loss: 0.0762 train_acc: 0.9929 val_loss: 0.8612 val_acc: 0.7580 test_loss: 0.8318 test_acc: 0.7680\n",
      "Epoch: 248 train_loss: 0.0762 train_acc: 0.9929 val_loss: 0.8976 val_acc: 0.7640 test_loss: 0.8318 test_acc: 0.7680\n",
      "Epoch: 248 train_loss: 0.0762 train_acc: 0.9929 val_loss: 0.8976 val_acc: 0.7640 test_loss: 0.7750 test_acc: 0.7800\n",
      "Epoch: 249 train_loss: 0.0743 train_acc: 0.9929 val_loss: 0.8176 val_acc: 0.7780 test_loss: 0.7871 test_acc: 0.7950\n",
      "Epoch: 249 train_loss: 0.0743 train_acc: 0.9929 val_loss: 0.8751 val_acc: 0.7740 test_loss: 0.7871 test_acc: 0.7950\n",
      "Epoch: 249 train_loss: 0.0743 train_acc: 0.9929 val_loss: 0.8751 val_acc: 0.7740 test_loss: 0.7538 test_acc: 0.7820\n",
      "Epoch: 250 train_loss: 0.0744 train_acc: 0.9929 val_loss: 0.7925 val_acc: 0.7880 test_loss: 0.7606 test_acc: 0.8070\n",
      "Epoch: 250 train_loss: 0.0744 train_acc: 0.9929 val_loss: 0.8504 val_acc: 0.7760 test_loss: 0.7606 test_acc: 0.8070\n",
      "Epoch: 250 train_loss: 0.0744 train_acc: 0.9929 val_loss: 0.8504 val_acc: 0.7760 test_loss: 0.7289 test_acc: 0.7870\n",
      "Epoch: 251 train_loss: 0.0770 train_acc: 0.9929 val_loss: 0.7889 val_acc: 0.7920 test_loss: 0.7588 test_acc: 0.7980\n",
      "Epoch: 251 train_loss: 0.0770 train_acc: 0.9929 val_loss: 0.8430 val_acc: 0.7780 test_loss: 0.7588 test_acc: 0.7980\n",
      "Epoch: 251 train_loss: 0.0770 train_acc: 0.9929 val_loss: 0.8430 val_acc: 0.7780 test_loss: 0.7173 test_acc: 0.7920\n",
      "Epoch: 252 train_loss: 0.0782 train_acc: 0.9929 val_loss: 0.7818 val_acc: 0.7920 test_loss: 0.7537 test_acc: 0.8030\n",
      "Epoch: 252 train_loss: 0.0782 train_acc: 0.9929 val_loss: 0.8411 val_acc: 0.7760 test_loss: 0.7537 test_acc: 0.8030\n",
      "Epoch: 252 train_loss: 0.0782 train_acc: 0.9929 val_loss: 0.8411 val_acc: 0.7760 test_loss: 0.7114 test_acc: 0.7960\n",
      "Epoch: 253 train_loss: 0.0774 train_acc: 0.9929 val_loss: 0.7761 val_acc: 0.7880 test_loss: 0.7478 test_acc: 0.8000\n",
      "Epoch: 253 train_loss: 0.0774 train_acc: 0.9929 val_loss: 0.8244 val_acc: 0.7960 test_loss: 0.7478 test_acc: 0.8000\n",
      "Epoch: 253 train_loss: 0.0774 train_acc: 0.9929 val_loss: 0.8244 val_acc: 0.7960 test_loss: 0.6918 test_acc: 0.7970\n",
      "Epoch: 254 train_loss: 0.0782 train_acc: 0.9857 val_loss: 0.7775 val_acc: 0.7920 test_loss: 0.7521 test_acc: 0.7900\n",
      "Epoch: 254 train_loss: 0.0782 train_acc: 0.9857 val_loss: 0.8109 val_acc: 0.7880 test_loss: 0.7521 test_acc: 0.7900\n",
      "Epoch: 254 train_loss: 0.0782 train_acc: 0.9857 val_loss: 0.8109 val_acc: 0.7880 test_loss: 0.6765 test_acc: 0.7990\n",
      "Epoch: 255 train_loss: 0.0828 train_acc: 0.9786 val_loss: 0.7766 val_acc: 0.8000 test_loss: 0.7500 test_acc: 0.7950\n",
      "Epoch: 255 train_loss: 0.0828 train_acc: 0.9786 val_loss: 0.8045 val_acc: 0.7860 test_loss: 0.7500 test_acc: 0.7950\n",
      "Epoch: 255 train_loss: 0.0828 train_acc: 0.9786 val_loss: 0.8045 val_acc: 0.7860 test_loss: 0.6690 test_acc: 0.7970\n",
      "Epoch: 256 train_loss: 0.0820 train_acc: 0.9786 val_loss: 0.7890 val_acc: 0.7960 test_loss: 0.7623 test_acc: 0.7870\n",
      "Epoch: 256 train_loss: 0.0820 train_acc: 0.9786 val_loss: 0.7987 val_acc: 0.7900 test_loss: 0.7623 test_acc: 0.7870\n",
      "Epoch: 256 train_loss: 0.0820 train_acc: 0.9786 val_loss: 0.7987 val_acc: 0.7900 test_loss: 0.6665 test_acc: 0.7960\n",
      "Epoch: 257 train_loss: 0.0833 train_acc: 0.9786 val_loss: 0.7952 val_acc: 0.7880 test_loss: 0.7679 test_acc: 0.7830\n",
      "Epoch: 257 train_loss: 0.0833 train_acc: 0.9786 val_loss: 0.8005 val_acc: 0.7840 test_loss: 0.7679 test_acc: 0.7830\n",
      "Epoch: 257 train_loss: 0.0833 train_acc: 0.9786 val_loss: 0.8005 val_acc: 0.7840 test_loss: 0.6708 test_acc: 0.7940\n",
      "Epoch: 258 train_loss: 0.0845 train_acc: 0.9857 val_loss: 0.8038 val_acc: 0.7840 test_loss: 0.7751 test_acc: 0.7850\n",
      "Epoch: 258 train_loss: 0.0845 train_acc: 0.9857 val_loss: 0.8071 val_acc: 0.7880 test_loss: 0.7751 test_acc: 0.7850\n",
      "Epoch: 258 train_loss: 0.0845 train_acc: 0.9857 val_loss: 0.8071 val_acc: 0.7880 test_loss: 0.6794 test_acc: 0.7820\n",
      "Epoch: 259 train_loss: 0.0814 train_acc: 0.9857 val_loss: 0.8100 val_acc: 0.7880 test_loss: 0.7803 test_acc: 0.7860\n",
      "Epoch: 259 train_loss: 0.0814 train_acc: 0.9857 val_loss: 0.8087 val_acc: 0.7940 test_loss: 0.7803 test_acc: 0.7860\n",
      "Epoch: 259 train_loss: 0.0814 train_acc: 0.9857 val_loss: 0.8087 val_acc: 0.7940 test_loss: 0.6813 test_acc: 0.7890\n",
      "Epoch: 260 train_loss: 0.0752 train_acc: 0.9929 val_loss: 0.8106 val_acc: 0.7900 test_loss: 0.7805 test_acc: 0.7850\n",
      "Epoch: 260 train_loss: 0.0752 train_acc: 0.9929 val_loss: 0.8046 val_acc: 0.8000 test_loss: 0.7805 test_acc: 0.7850\n",
      "Epoch: 260 train_loss: 0.0752 train_acc: 0.9929 val_loss: 0.8046 val_acc: 0.8000 test_loss: 0.6774 test_acc: 0.7960\n",
      "Epoch: 261 train_loss: 0.0708 train_acc: 0.9929 val_loss: 0.8096 val_acc: 0.7840 test_loss: 0.7792 test_acc: 0.7860\n",
      "Epoch: 261 train_loss: 0.0708 train_acc: 0.9929 val_loss: 0.8128 val_acc: 0.7880 test_loss: 0.7792 test_acc: 0.7860\n",
      "Epoch: 261 train_loss: 0.0708 train_acc: 0.9929 val_loss: 0.8128 val_acc: 0.7880 test_loss: 0.6832 test_acc: 0.7940\n",
      "Epoch: 262 train_loss: 0.0729 train_acc: 0.9857 val_loss: 0.8156 val_acc: 0.7840 test_loss: 0.7870 test_acc: 0.7890\n",
      "Epoch: 262 train_loss: 0.0729 train_acc: 0.9857 val_loss: 0.8347 val_acc: 0.7680 test_loss: 0.7870 test_acc: 0.7890\n",
      "Epoch: 262 train_loss: 0.0729 train_acc: 0.9857 val_loss: 0.8347 val_acc: 0.7680 test_loss: 0.7025 test_acc: 0.7870\n",
      "Epoch: 263 train_loss: 0.0768 train_acc: 0.9929 val_loss: 0.8147 val_acc: 0.7800 test_loss: 0.7869 test_acc: 0.7860\n",
      "Epoch: 263 train_loss: 0.0768 train_acc: 0.9929 val_loss: 0.8611 val_acc: 0.7580 test_loss: 0.7869 test_acc: 0.7860\n",
      "Epoch: 263 train_loss: 0.0768 train_acc: 0.9929 val_loss: 0.8611 val_acc: 0.7580 test_loss: 0.7252 test_acc: 0.7890\n",
      "Epoch: 264 train_loss: 0.0813 train_acc: 0.9857 val_loss: 0.7934 val_acc: 0.7900 test_loss: 0.7649 test_acc: 0.7960\n",
      "Epoch: 264 train_loss: 0.0813 train_acc: 0.9857 val_loss: 0.8848 val_acc: 0.7600 test_loss: 0.7649 test_acc: 0.7960\n",
      "Epoch: 264 train_loss: 0.0813 train_acc: 0.9857 val_loss: 0.8848 val_acc: 0.7600 test_loss: 0.7465 test_acc: 0.7870\n",
      "Epoch: 265 train_loss: 0.0828 train_acc: 0.9786 val_loss: 0.7949 val_acc: 0.7760 test_loss: 0.7649 test_acc: 0.7840\n",
      "Epoch: 265 train_loss: 0.0828 train_acc: 0.9786 val_loss: 0.8807 val_acc: 0.7600 test_loss: 0.7649 test_acc: 0.7840\n",
      "Epoch: 265 train_loss: 0.0828 train_acc: 0.9786 val_loss: 0.8807 val_acc: 0.7600 test_loss: 0.7460 test_acc: 0.7860\n",
      "Epoch: 266 train_loss: 0.0804 train_acc: 0.9929 val_loss: 0.7885 val_acc: 0.7800 test_loss: 0.7554 test_acc: 0.7840\n",
      "Epoch: 266 train_loss: 0.0804 train_acc: 0.9929 val_loss: 0.8600 val_acc: 0.7640 test_loss: 0.7554 test_acc: 0.7840\n",
      "Epoch: 266 train_loss: 0.0804 train_acc: 0.9929 val_loss: 0.8600 val_acc: 0.7640 test_loss: 0.7280 test_acc: 0.7890\n",
      "Epoch: 267 train_loss: 0.0821 train_acc: 0.9857 val_loss: 0.7970 val_acc: 0.7860 test_loss: 0.7606 test_acc: 0.7840\n",
      "Epoch: 267 train_loss: 0.0821 train_acc: 0.9857 val_loss: 0.8493 val_acc: 0.7820 test_loss: 0.7606 test_acc: 0.7840\n",
      "Epoch: 267 train_loss: 0.0821 train_acc: 0.9857 val_loss: 0.8493 val_acc: 0.7820 test_loss: 0.7199 test_acc: 0.7880\n",
      "Epoch: 268 train_loss: 0.0863 train_acc: 0.9929 val_loss: 0.8235 val_acc: 0.7640 test_loss: 0.7814 test_acc: 0.7750\n",
      "Epoch: 268 train_loss: 0.0863 train_acc: 0.9929 val_loss: 0.8372 val_acc: 0.7880 test_loss: 0.7814 test_acc: 0.7750\n",
      "Epoch: 268 train_loss: 0.0863 train_acc: 0.9929 val_loss: 0.8372 val_acc: 0.7880 test_loss: 0.7113 test_acc: 0.7990\n",
      "Epoch: 269 train_loss: 0.0911 train_acc: 0.9929 val_loss: 0.7898 val_acc: 0.7780 test_loss: 0.7504 test_acc: 0.7850\n",
      "Epoch: 269 train_loss: 0.0911 train_acc: 0.9929 val_loss: 0.8215 val_acc: 0.7940 test_loss: 0.7504 test_acc: 0.7850\n",
      "Epoch: 269 train_loss: 0.0911 train_acc: 0.9929 val_loss: 0.8215 val_acc: 0.7940 test_loss: 0.6990 test_acc: 0.8050\n",
      "Epoch: 270 train_loss: 0.0912 train_acc: 0.9929 val_loss: 0.7882 val_acc: 0.7800 test_loss: 0.7510 test_acc: 0.7880\n",
      "Epoch: 270 train_loss: 0.0912 train_acc: 0.9929 val_loss: 0.8117 val_acc: 0.7960 test_loss: 0.7510 test_acc: 0.7880\n",
      "Epoch: 270 train_loss: 0.0912 train_acc: 0.9929 val_loss: 0.8117 val_acc: 0.7960 test_loss: 0.6919 test_acc: 0.8090\n",
      "Epoch: 271 train_loss: 0.0893 train_acc: 0.9857 val_loss: 0.7962 val_acc: 0.7820 test_loss: 0.7593 test_acc: 0.7900\n",
      "Epoch: 271 train_loss: 0.0893 train_acc: 0.9857 val_loss: 0.7984 val_acc: 0.7960 test_loss: 0.7593 test_acc: 0.7900\n",
      "Epoch: 271 train_loss: 0.0893 train_acc: 0.9857 val_loss: 0.7984 val_acc: 0.7960 test_loss: 0.6803 test_acc: 0.8060\n",
      "Epoch: 272 train_loss: 0.0804 train_acc: 0.9857 val_loss: 0.8259 val_acc: 0.7740 test_loss: 0.7945 test_acc: 0.7810\n",
      "Epoch: 272 train_loss: 0.0804 train_acc: 0.9857 val_loss: 0.8017 val_acc: 0.7960 test_loss: 0.7945 test_acc: 0.7810\n",
      "Epoch: 272 train_loss: 0.0804 train_acc: 0.9857 val_loss: 0.8017 val_acc: 0.7960 test_loss: 0.6857 test_acc: 0.7990\n",
      "Epoch: 273 train_loss: 0.0740 train_acc: 0.9857 val_loss: 0.8398 val_acc: 0.7700 test_loss: 0.8097 test_acc: 0.7690\n",
      "Epoch: 273 train_loss: 0.0740 train_acc: 0.9857 val_loss: 0.8198 val_acc: 0.7940 test_loss: 0.8097 test_acc: 0.7690\n",
      "Epoch: 273 train_loss: 0.0740 train_acc: 0.9857 val_loss: 0.8198 val_acc: 0.7940 test_loss: 0.7059 test_acc: 0.7910\n",
      "Epoch: 274 train_loss: 0.0726 train_acc: 0.9857 val_loss: 0.8323 val_acc: 0.7660 test_loss: 0.7995 test_acc: 0.7710\n",
      "Epoch: 274 train_loss: 0.0726 train_acc: 0.9857 val_loss: 0.8347 val_acc: 0.7860 test_loss: 0.7995 test_acc: 0.7710\n",
      "Epoch: 274 train_loss: 0.0726 train_acc: 0.9857 val_loss: 0.8347 val_acc: 0.7860 test_loss: 0.7219 test_acc: 0.7820\n",
      "Epoch: 275 train_loss: 0.0721 train_acc: 0.9857 val_loss: 0.8058 val_acc: 0.7660 test_loss: 0.7642 test_acc: 0.7820\n",
      "Epoch: 275 train_loss: 0.0721 train_acc: 0.9857 val_loss: 0.8341 val_acc: 0.7840 test_loss: 0.7642 test_acc: 0.7820\n",
      "Epoch: 275 train_loss: 0.0721 train_acc: 0.9857 val_loss: 0.8341 val_acc: 0.7840 test_loss: 0.7229 test_acc: 0.7880\n",
      "Epoch: 276 train_loss: 0.0693 train_acc: 0.9786 val_loss: 0.7903 val_acc: 0.7820 test_loss: 0.7390 test_acc: 0.7850\n",
      "Epoch: 276 train_loss: 0.0693 train_acc: 0.9786 val_loss: 0.8201 val_acc: 0.7800 test_loss: 0.7390 test_acc: 0.7850\n",
      "Epoch: 276 train_loss: 0.0693 train_acc: 0.9786 val_loss: 0.8201 val_acc: 0.7800 test_loss: 0.7121 test_acc: 0.7940\n",
      "Epoch: 277 train_loss: 0.0655 train_acc: 0.9786 val_loss: 0.8203 val_acc: 0.7660 test_loss: 0.7616 test_acc: 0.7810\n",
      "Epoch: 277 train_loss: 0.0655 train_acc: 0.9786 val_loss: 0.8068 val_acc: 0.7820 test_loss: 0.7616 test_acc: 0.7810\n",
      "Epoch: 277 train_loss: 0.0655 train_acc: 0.9786 val_loss: 0.8068 val_acc: 0.7820 test_loss: 0.7000 test_acc: 0.7960\n",
      "Epoch: 278 train_loss: 0.0648 train_acc: 0.9857 val_loss: 0.8530 val_acc: 0.7560 test_loss: 0.7898 test_acc: 0.7680\n",
      "Epoch: 278 train_loss: 0.0648 train_acc: 0.9857 val_loss: 0.8045 val_acc: 0.7800 test_loss: 0.7898 test_acc: 0.7680\n",
      "Epoch: 278 train_loss: 0.0648 train_acc: 0.9857 val_loss: 0.8045 val_acc: 0.7800 test_loss: 0.6979 test_acc: 0.7990\n",
      "Epoch: 279 train_loss: 0.0651 train_acc: 0.9857 val_loss: 0.8492 val_acc: 0.7640 test_loss: 0.7854 test_acc: 0.7770\n",
      "Epoch: 279 train_loss: 0.0651 train_acc: 0.9857 val_loss: 0.8091 val_acc: 0.7860 test_loss: 0.7854 test_acc: 0.7770\n",
      "Epoch: 279 train_loss: 0.0651 train_acc: 0.9857 val_loss: 0.8091 val_acc: 0.7860 test_loss: 0.7002 test_acc: 0.7940\n",
      "Epoch: 280 train_loss: 0.0674 train_acc: 0.9929 val_loss: 0.8079 val_acc: 0.7760 test_loss: 0.7539 test_acc: 0.7820\n",
      "Epoch: 280 train_loss: 0.0674 train_acc: 0.9929 val_loss: 0.8195 val_acc: 0.7840 test_loss: 0.7539 test_acc: 0.7820\n",
      "Epoch: 280 train_loss: 0.0674 train_acc: 0.9929 val_loss: 0.8195 val_acc: 0.7840 test_loss: 0.7090 test_acc: 0.7900\n",
      "Epoch: 281 train_loss: 0.0720 train_acc: 0.9929 val_loss: 0.8224 val_acc: 0.7680 test_loss: 0.7758 test_acc: 0.7830\n",
      "Epoch: 281 train_loss: 0.0720 train_acc: 0.9929 val_loss: 0.8322 val_acc: 0.7860 test_loss: 0.7758 test_acc: 0.7830\n",
      "Epoch: 281 train_loss: 0.0720 train_acc: 0.9929 val_loss: 0.8322 val_acc: 0.7860 test_loss: 0.7186 test_acc: 0.7900\n",
      "Epoch: 282 train_loss: 0.0768 train_acc: 0.9929 val_loss: 0.8503 val_acc: 0.7500 test_loss: 0.8068 test_acc: 0.7700\n",
      "Epoch: 282 train_loss: 0.0768 train_acc: 0.9929 val_loss: 0.8398 val_acc: 0.7860 test_loss: 0.8068 test_acc: 0.7700\n",
      "Epoch: 282 train_loss: 0.0768 train_acc: 0.9929 val_loss: 0.8398 val_acc: 0.7860 test_loss: 0.7240 test_acc: 0.7930\n",
      "Epoch: 283 train_loss: 0.0773 train_acc: 0.9929 val_loss: 0.8369 val_acc: 0.7560 test_loss: 0.7889 test_acc: 0.7800\n",
      "Epoch: 283 train_loss: 0.0773 train_acc: 0.9929 val_loss: 0.8519 val_acc: 0.7840 test_loss: 0.7889 test_acc: 0.7800\n",
      "Epoch: 283 train_loss: 0.0773 train_acc: 0.9929 val_loss: 0.8519 val_acc: 0.7840 test_loss: 0.7349 test_acc: 0.7920\n",
      "Epoch: 284 train_loss: 0.0725 train_acc: 0.9929 val_loss: 0.8170 val_acc: 0.7620 test_loss: 0.7612 test_acc: 0.7980\n",
      "Epoch: 284 train_loss: 0.0725 train_acc: 0.9929 val_loss: 0.8537 val_acc: 0.7840 test_loss: 0.7612 test_acc: 0.7980\n",
      "Epoch: 284 train_loss: 0.0725 train_acc: 0.9929 val_loss: 0.8537 val_acc: 0.7840 test_loss: 0.7368 test_acc: 0.7910\n",
      "Epoch: 285 train_loss: 0.0681 train_acc: 0.9929 val_loss: 0.8207 val_acc: 0.7660 test_loss: 0.7580 test_acc: 0.7820\n",
      "Epoch: 285 train_loss: 0.0681 train_acc: 0.9929 val_loss: 0.8490 val_acc: 0.7860 test_loss: 0.7580 test_acc: 0.7820\n",
      "Epoch: 285 train_loss: 0.0681 train_acc: 0.9929 val_loss: 0.8490 val_acc: 0.7860 test_loss: 0.7314 test_acc: 0.7900\n",
      "Epoch: 286 train_loss: 0.0633 train_acc: 0.9929 val_loss: 0.8469 val_acc: 0.7500 test_loss: 0.7790 test_acc: 0.7680\n",
      "Epoch: 286 train_loss: 0.0633 train_acc: 0.9929 val_loss: 0.8399 val_acc: 0.7920 test_loss: 0.7790 test_acc: 0.7680\n",
      "Epoch: 286 train_loss: 0.0633 train_acc: 0.9929 val_loss: 0.8399 val_acc: 0.7920 test_loss: 0.7239 test_acc: 0.7920\n",
      "Epoch: 287 train_loss: 0.0604 train_acc: 0.9929 val_loss: 0.8529 val_acc: 0.7420 test_loss: 0.7842 test_acc: 0.7620\n",
      "Epoch: 287 train_loss: 0.0604 train_acc: 0.9929 val_loss: 0.8326 val_acc: 0.7840 test_loss: 0.7842 test_acc: 0.7620\n",
      "Epoch: 287 train_loss: 0.0604 train_acc: 0.9929 val_loss: 0.8326 val_acc: 0.7840 test_loss: 0.7180 test_acc: 0.7940\n",
      "Epoch: 288 train_loss: 0.0604 train_acc: 0.9857 val_loss: 0.8836 val_acc: 0.7500 test_loss: 0.8171 test_acc: 0.7460\n",
      "Epoch: 288 train_loss: 0.0604 train_acc: 0.9857 val_loss: 0.8445 val_acc: 0.7760 test_loss: 0.8171 test_acc: 0.7460\n",
      "Epoch: 288 train_loss: 0.0604 train_acc: 0.9857 val_loss: 0.8445 val_acc: 0.7760 test_loss: 0.7321 test_acc: 0.7900\n",
      "Epoch: 289 train_loss: 0.0642 train_acc: 0.9857 val_loss: 0.8941 val_acc: 0.7600 test_loss: 0.8296 test_acc: 0.7460\n",
      "Epoch: 289 train_loss: 0.0642 train_acc: 0.9857 val_loss: 0.8660 val_acc: 0.7760 test_loss: 0.8296 test_acc: 0.7460\n",
      "Epoch: 289 train_loss: 0.0642 train_acc: 0.9857 val_loss: 0.8660 val_acc: 0.7760 test_loss: 0.7493 test_acc: 0.7790\n",
      "Epoch: 290 train_loss: 0.0652 train_acc: 0.9857 val_loss: 0.9227 val_acc: 0.7720 test_loss: 0.8595 test_acc: 0.7440\n",
      "Epoch: 290 train_loss: 0.0652 train_acc: 0.9857 val_loss: 0.8801 val_acc: 0.7740 test_loss: 0.8595 test_acc: 0.7440\n",
      "Epoch: 290 train_loss: 0.0652 train_acc: 0.9857 val_loss: 0.8801 val_acc: 0.7740 test_loss: 0.7554 test_acc: 0.7840\n",
      "Epoch: 291 train_loss: 0.0663 train_acc: 0.9929 val_loss: 0.9247 val_acc: 0.7600 test_loss: 0.8596 test_acc: 0.7400\n",
      "Epoch: 291 train_loss: 0.0663 train_acc: 0.9929 val_loss: 0.9085 val_acc: 0.7720 test_loss: 0.8596 test_acc: 0.7400\n",
      "Epoch: 291 train_loss: 0.0663 train_acc: 0.9929 val_loss: 0.9085 val_acc: 0.7720 test_loss: 0.7731 test_acc: 0.7740\n",
      "Epoch: 292 train_loss: 0.0800 train_acc: 0.9857 val_loss: 0.8931 val_acc: 0.7620 test_loss: 0.8238 test_acc: 0.7500\n",
      "Epoch: 292 train_loss: 0.0800 train_acc: 0.9857 val_loss: 0.9056 val_acc: 0.7700 test_loss: 0.8238 test_acc: 0.7500\n",
      "Epoch: 292 train_loss: 0.0800 train_acc: 0.9857 val_loss: 0.9056 val_acc: 0.7700 test_loss: 0.7609 test_acc: 0.7770\n",
      "Epoch: 293 train_loss: 0.0892 train_acc: 0.9786 val_loss: 0.8529 val_acc: 0.7680 test_loss: 0.7803 test_acc: 0.7710\n",
      "Epoch: 293 train_loss: 0.0892 train_acc: 0.9786 val_loss: 0.9099 val_acc: 0.7580 test_loss: 0.7803 test_acc: 0.7710\n",
      "Epoch: 293 train_loss: 0.0892 train_acc: 0.9786 val_loss: 0.9099 val_acc: 0.7580 test_loss: 0.7613 test_acc: 0.7780\n",
      "Epoch: 294 train_loss: 0.0875 train_acc: 0.9786 val_loss: 0.7951 val_acc: 0.7820 test_loss: 0.7244 test_acc: 0.7870\n",
      "Epoch: 294 train_loss: 0.0875 train_acc: 0.9786 val_loss: 0.9123 val_acc: 0.7600 test_loss: 0.7244 test_acc: 0.7870\n",
      "Epoch: 294 train_loss: 0.0875 train_acc: 0.9786 val_loss: 0.9123 val_acc: 0.7600 test_loss: 0.7613 test_acc: 0.7790\n",
      "Epoch: 295 train_loss: 0.0766 train_acc: 0.9857 val_loss: 0.7992 val_acc: 0.7900 test_loss: 0.7307 test_acc: 0.7870\n",
      "Epoch: 295 train_loss: 0.0766 train_acc: 0.9857 val_loss: 0.9030 val_acc: 0.7740 test_loss: 0.7307 test_acc: 0.7870\n",
      "Epoch: 295 train_loss: 0.0766 train_acc: 0.9857 val_loss: 0.9030 val_acc: 0.7740 test_loss: 0.7500 test_acc: 0.7790\n",
      "Epoch: 296 train_loss: 0.0708 train_acc: 0.9857 val_loss: 0.8261 val_acc: 0.7780 test_loss: 0.7642 test_acc: 0.7690\n",
      "Epoch: 296 train_loss: 0.0708 train_acc: 0.9857 val_loss: 0.9002 val_acc: 0.7600 test_loss: 0.7642 test_acc: 0.7690\n",
      "Epoch: 296 train_loss: 0.0708 train_acc: 0.9857 val_loss: 0.9002 val_acc: 0.7600 test_loss: 0.7419 test_acc: 0.7880\n",
      "Epoch: 297 train_loss: 0.0833 train_acc: 0.9786 val_loss: 0.8495 val_acc: 0.7700 test_loss: 0.7900 test_acc: 0.7610\n",
      "Epoch: 297 train_loss: 0.0833 train_acc: 0.9786 val_loss: 0.9190 val_acc: 0.7460 test_loss: 0.7900 test_acc: 0.7610\n",
      "Epoch: 297 train_loss: 0.0833 train_acc: 0.9786 val_loss: 0.9190 val_acc: 0.7460 test_loss: 0.7578 test_acc: 0.7820\n",
      "Epoch: 298 train_loss: 0.1102 train_acc: 0.9643 val_loss: 0.8697 val_acc: 0.7720 test_loss: 0.8092 test_acc: 0.7600\n",
      "Epoch: 298 train_loss: 0.1102 train_acc: 0.9643 val_loss: 0.9455 val_acc: 0.7500 test_loss: 0.8092 test_acc: 0.7600\n",
      "Epoch: 298 train_loss: 0.1102 train_acc: 0.9643 val_loss: 0.9455 val_acc: 0.7500 test_loss: 0.7841 test_acc: 0.7830\n",
      "Epoch: 299 train_loss: 0.1042 train_acc: 0.9643 val_loss: 0.8639 val_acc: 0.7620 test_loss: 0.7990 test_acc: 0.7670\n",
      "Epoch: 299 train_loss: 0.1042 train_acc: 0.9643 val_loss: 0.9421 val_acc: 0.7520 test_loss: 0.7990 test_acc: 0.7670\n",
      "Epoch: 299 train_loss: 0.1042 train_acc: 0.9643 val_loss: 0.9421 val_acc: 0.7520 test_loss: 0.7736 test_acc: 0.7820\n",
      "Epoch: 300 train_loss: 0.0793 train_acc: 0.9857 val_loss: 0.8453 val_acc: 0.7660 test_loss: 0.7803 test_acc: 0.7840\n",
      "Epoch: 300 train_loss: 0.0793 train_acc: 0.9857 val_loss: 0.9284 val_acc: 0.7540 test_loss: 0.7803 test_acc: 0.7840\n",
      "Epoch: 300 train_loss: 0.0793 train_acc: 0.9857 val_loss: 0.9284 val_acc: 0.7540 test_loss: 0.7547 test_acc: 0.7850\n",
      "Epoch: 301 train_loss: 0.0891 train_acc: 0.9857 val_loss: 0.8318 val_acc: 0.7700 test_loss: 0.7684 test_acc: 0.7870\n",
      "Epoch: 301 train_loss: 0.0891 train_acc: 0.9857 val_loss: 0.9474 val_acc: 0.7480 test_loss: 0.7684 test_acc: 0.7870\n",
      "Epoch: 301 train_loss: 0.0891 train_acc: 0.9857 val_loss: 0.9474 val_acc: 0.7480 test_loss: 0.7789 test_acc: 0.7630\n",
      "Epoch: 302 train_loss: 0.1243 train_acc: 0.9571 val_loss: 0.8214 val_acc: 0.7700 test_loss: 0.7610 test_acc: 0.7920\n",
      "Epoch: 302 train_loss: 0.1243 train_acc: 0.9571 val_loss: 0.9725 val_acc: 0.7540 test_loss: 0.7610 test_acc: 0.7920\n",
      "Epoch: 302 train_loss: 0.1243 train_acc: 0.9571 val_loss: 0.9725 val_acc: 0.7540 test_loss: 0.8146 test_acc: 0.7630\n",
      "Epoch: 303 train_loss: 0.1532 train_acc: 0.9571 val_loss: 0.8094 val_acc: 0.7640 test_loss: 0.7548 test_acc: 0.7950\n",
      "Epoch: 303 train_loss: 0.1532 train_acc: 0.9571 val_loss: 0.9962 val_acc: 0.7480 test_loss: 0.7548 test_acc: 0.7950\n",
      "Epoch: 303 train_loss: 0.1532 train_acc: 0.9571 val_loss: 0.9962 val_acc: 0.7480 test_loss: 0.8457 test_acc: 0.7500\n",
      "Epoch: 304 train_loss: 0.1537 train_acc: 0.9714 val_loss: 0.8090 val_acc: 0.7600 test_loss: 0.7597 test_acc: 0.7910\n",
      "Epoch: 304 train_loss: 0.1537 train_acc: 0.9714 val_loss: 0.9749 val_acc: 0.7440 test_loss: 0.7597 test_acc: 0.7910\n",
      "Epoch: 304 train_loss: 0.1537 train_acc: 0.9714 val_loss: 0.9749 val_acc: 0.7440 test_loss: 0.8284 test_acc: 0.7490\n",
      "Epoch: 305 train_loss: 0.1566 train_acc: 0.9643 val_loss: 0.8135 val_acc: 0.7620 test_loss: 0.7691 test_acc: 0.7860\n",
      "Epoch: 305 train_loss: 0.1566 train_acc: 0.9643 val_loss: 0.9376 val_acc: 0.7620 test_loss: 0.7691 test_acc: 0.7860\n",
      "Epoch: 305 train_loss: 0.1566 train_acc: 0.9643 val_loss: 0.9376 val_acc: 0.7620 test_loss: 0.7910 test_acc: 0.7550\n",
      "Epoch: 306 train_loss: 0.1531 train_acc: 0.9643 val_loss: 0.8091 val_acc: 0.7640 test_loss: 0.7644 test_acc: 0.7970\n",
      "Epoch: 306 train_loss: 0.1531 train_acc: 0.9643 val_loss: 0.9399 val_acc: 0.7500 test_loss: 0.7644 test_acc: 0.7970\n",
      "Epoch: 306 train_loss: 0.1531 train_acc: 0.9643 val_loss: 0.9399 val_acc: 0.7500 test_loss: 0.7904 test_acc: 0.7600\n",
      "Epoch: 307 train_loss: 0.1410 train_acc: 0.9786 val_loss: 0.8042 val_acc: 0.7680 test_loss: 0.7565 test_acc: 0.7960\n",
      "Epoch: 307 train_loss: 0.1410 train_acc: 0.9786 val_loss: 0.9487 val_acc: 0.7520 test_loss: 0.7565 test_acc: 0.7960\n",
      "Epoch: 307 train_loss: 0.1410 train_acc: 0.9786 val_loss: 0.9487 val_acc: 0.7520 test_loss: 0.7970 test_acc: 0.7600\n",
      "Epoch: 308 train_loss: 0.1270 train_acc: 0.9786 val_loss: 0.8004 val_acc: 0.7720 test_loss: 0.7493 test_acc: 0.7960\n",
      "Epoch: 308 train_loss: 0.1270 train_acc: 0.9786 val_loss: 0.9572 val_acc: 0.7480 test_loss: 0.7493 test_acc: 0.7960\n",
      "Epoch: 308 train_loss: 0.1270 train_acc: 0.9786 val_loss: 0.9572 val_acc: 0.7480 test_loss: 0.8074 test_acc: 0.7600\n",
      "Epoch: 309 train_loss: 0.1222 train_acc: 0.9786 val_loss: 0.8041 val_acc: 0.7840 test_loss: 0.7486 test_acc: 0.7940\n",
      "Epoch: 309 train_loss: 0.1222 train_acc: 0.9786 val_loss: 0.9710 val_acc: 0.7340 test_loss: 0.7486 test_acc: 0.7940\n",
      "Epoch: 309 train_loss: 0.1222 train_acc: 0.9786 val_loss: 0.9710 val_acc: 0.7340 test_loss: 0.8246 test_acc: 0.7550\n",
      "Epoch: 310 train_loss: 0.1268 train_acc: 0.9643 val_loss: 0.8183 val_acc: 0.7820 test_loss: 0.7606 test_acc: 0.7840\n",
      "Epoch: 310 train_loss: 0.1268 train_acc: 0.9643 val_loss: 0.9908 val_acc: 0.7220 test_loss: 0.7606 test_acc: 0.7840\n",
      "Epoch: 310 train_loss: 0.1268 train_acc: 0.9643 val_loss: 0.9908 val_acc: 0.7220 test_loss: 0.8489 test_acc: 0.7470\n",
      "Epoch: 311 train_loss: 0.1374 train_acc: 0.9571 val_loss: 0.8408 val_acc: 0.7740 test_loss: 0.7804 test_acc: 0.7730\n",
      "Epoch: 311 train_loss: 0.1374 train_acc: 0.9571 val_loss: 1.0169 val_acc: 0.7120 test_loss: 0.7804 test_acc: 0.7730\n",
      "Epoch: 311 train_loss: 0.1374 train_acc: 0.9571 val_loss: 1.0169 val_acc: 0.7120 test_loss: 0.8813 test_acc: 0.7400\n",
      "Epoch: 312 train_loss: 0.1365 train_acc: 0.9571 val_loss: 0.8433 val_acc: 0.7720 test_loss: 0.7858 test_acc: 0.7770\n",
      "Epoch: 312 train_loss: 0.1365 train_acc: 0.9571 val_loss: 1.0047 val_acc: 0.7140 test_loss: 0.7858 test_acc: 0.7770\n",
      "Epoch: 312 train_loss: 0.1365 train_acc: 0.9571 val_loss: 1.0047 val_acc: 0.7140 test_loss: 0.8691 test_acc: 0.7450\n",
      "Epoch: 313 train_loss: 0.1243 train_acc: 0.9643 val_loss: 0.8162 val_acc: 0.7740 test_loss: 0.7670 test_acc: 0.7800\n",
      "Epoch: 313 train_loss: 0.1243 train_acc: 0.9643 val_loss: 0.9744 val_acc: 0.7260 test_loss: 0.7670 test_acc: 0.7800\n",
      "Epoch: 313 train_loss: 0.1243 train_acc: 0.9643 val_loss: 0.9744 val_acc: 0.7260 test_loss: 0.8413 test_acc: 0.7510\n",
      "Epoch: 314 train_loss: 0.1054 train_acc: 0.9714 val_loss: 0.8070 val_acc: 0.7660 test_loss: 0.7622 test_acc: 0.7750\n",
      "Epoch: 314 train_loss: 0.1054 train_acc: 0.9714 val_loss: 0.9386 val_acc: 0.7360 test_loss: 0.7622 test_acc: 0.7750\n",
      "Epoch: 314 train_loss: 0.1054 train_acc: 0.9714 val_loss: 0.9386 val_acc: 0.7360 test_loss: 0.8062 test_acc: 0.7610\n",
      "Epoch: 315 train_loss: 0.0900 train_acc: 0.9857 val_loss: 0.7800 val_acc: 0.7780 test_loss: 0.7355 test_acc: 0.7910\n",
      "Epoch: 315 train_loss: 0.0900 train_acc: 0.9857 val_loss: 0.9199 val_acc: 0.7580 test_loss: 0.7355 test_acc: 0.7910\n",
      "Epoch: 315 train_loss: 0.0900 train_acc: 0.9857 val_loss: 0.9199 val_acc: 0.7580 test_loss: 0.7869 test_acc: 0.7620\n",
      "Epoch: 316 train_loss: 0.0858 train_acc: 0.9929 val_loss: 0.7652 val_acc: 0.7820 test_loss: 0.7213 test_acc: 0.8030\n",
      "Epoch: 316 train_loss: 0.0858 train_acc: 0.9929 val_loss: 0.9192 val_acc: 0.7640 test_loss: 0.7213 test_acc: 0.8030\n",
      "Epoch: 316 train_loss: 0.0858 train_acc: 0.9929 val_loss: 0.9192 val_acc: 0.7640 test_loss: 0.7837 test_acc: 0.7650\n",
      "Epoch: 317 train_loss: 0.0905 train_acc: 1.0000 val_loss: 0.7702 val_acc: 0.7740 test_loss: 0.7252 test_acc: 0.7980\n",
      "Epoch: 317 train_loss: 0.0905 train_acc: 1.0000 val_loss: 0.9271 val_acc: 0.7640 test_loss: 0.7252 test_acc: 0.7980\n",
      "Epoch: 317 train_loss: 0.0905 train_acc: 1.0000 val_loss: 0.9271 val_acc: 0.7640 test_loss: 0.7909 test_acc: 0.7650\n",
      "Epoch: 318 train_loss: 0.0935 train_acc: 0.9786 val_loss: 0.7897 val_acc: 0.7720 test_loss: 0.7445 test_acc: 0.7930\n",
      "Epoch: 318 train_loss: 0.0935 train_acc: 0.9786 val_loss: 0.9131 val_acc: 0.7600 test_loss: 0.7445 test_acc: 0.7930\n",
      "Epoch: 318 train_loss: 0.0935 train_acc: 0.9786 val_loss: 0.9131 val_acc: 0.7600 test_loss: 0.7752 test_acc: 0.7780\n",
      "Epoch: 319 train_loss: 0.1005 train_acc: 0.9786 val_loss: 0.8143 val_acc: 0.7700 test_loss: 0.7663 test_acc: 0.7840\n",
      "Epoch: 319 train_loss: 0.1005 train_acc: 0.9786 val_loss: 0.9043 val_acc: 0.7660 test_loss: 0.7663 test_acc: 0.7840\n",
      "Epoch: 319 train_loss: 0.1005 train_acc: 0.9786 val_loss: 0.9043 val_acc: 0.7660 test_loss: 0.7651 test_acc: 0.7790\n",
      "Epoch: 320 train_loss: 0.1100 train_acc: 0.9714 val_loss: 0.8637 val_acc: 0.7600 test_loss: 0.8126 test_acc: 0.7650\n",
      "Epoch: 320 train_loss: 0.1100 train_acc: 0.9714 val_loss: 0.8926 val_acc: 0.7640 test_loss: 0.8126 test_acc: 0.7650\n",
      "Epoch: 320 train_loss: 0.1100 train_acc: 0.9714 val_loss: 0.8926 val_acc: 0.7640 test_loss: 0.7461 test_acc: 0.7860\n",
      "Epoch: 321 train_loss: 0.1080 train_acc: 0.9714 val_loss: 0.8866 val_acc: 0.7540 test_loss: 0.8367 test_acc: 0.7640\n",
      "Epoch: 321 train_loss: 0.1080 train_acc: 0.9714 val_loss: 0.8824 val_acc: 0.7660 test_loss: 0.8367 test_acc: 0.7640\n",
      "Epoch: 321 train_loss: 0.1080 train_acc: 0.9714 val_loss: 0.8824 val_acc: 0.7660 test_loss: 0.7348 test_acc: 0.7880\n",
      "Epoch: 322 train_loss: 0.0995 train_acc: 0.9714 val_loss: 0.8786 val_acc: 0.7660 test_loss: 0.8300 test_acc: 0.7690\n",
      "Epoch: 322 train_loss: 0.0995 train_acc: 0.9714 val_loss: 0.8741 val_acc: 0.7660 test_loss: 0.8300 test_acc: 0.7690\n",
      "Epoch: 322 train_loss: 0.0995 train_acc: 0.9714 val_loss: 0.8741 val_acc: 0.7660 test_loss: 0.7272 test_acc: 0.7900\n",
      "Epoch: 323 train_loss: 0.0936 train_acc: 0.9857 val_loss: 0.8531 val_acc: 0.7660 test_loss: 0.8078 test_acc: 0.7760\n",
      "Epoch: 323 train_loss: 0.0936 train_acc: 0.9857 val_loss: 0.8738 val_acc: 0.7660 test_loss: 0.8078 test_acc: 0.7760\n",
      "Epoch: 323 train_loss: 0.0936 train_acc: 0.9857 val_loss: 0.8738 val_acc: 0.7660 test_loss: 0.7266 test_acc: 0.7920\n",
      "Epoch: 324 train_loss: 0.0847 train_acc: 0.9929 val_loss: 0.8335 val_acc: 0.7600 test_loss: 0.7942 test_acc: 0.7670\n",
      "Epoch: 324 train_loss: 0.0847 train_acc: 0.9929 val_loss: 0.8782 val_acc: 0.7720 test_loss: 0.7942 test_acc: 0.7670\n",
      "Epoch: 324 train_loss: 0.0847 train_acc: 0.9929 val_loss: 0.8782 val_acc: 0.7720 test_loss: 0.7351 test_acc: 0.7870\n",
      "Epoch: 325 train_loss: 0.0745 train_acc: 0.9929 val_loss: 0.8176 val_acc: 0.7580 test_loss: 0.7790 test_acc: 0.7710\n",
      "Epoch: 325 train_loss: 0.0745 train_acc: 0.9929 val_loss: 0.8885 val_acc: 0.7740 test_loss: 0.7790 test_acc: 0.7710\n",
      "Epoch: 325 train_loss: 0.0745 train_acc: 0.9929 val_loss: 0.8885 val_acc: 0.7740 test_loss: 0.7540 test_acc: 0.7740\n",
      "Epoch: 326 train_loss: 0.0716 train_acc: 0.9929 val_loss: 0.8036 val_acc: 0.7620 test_loss: 0.7639 test_acc: 0.7810\n",
      "Epoch: 326 train_loss: 0.0716 train_acc: 0.9929 val_loss: 0.9205 val_acc: 0.7680 test_loss: 0.7639 test_acc: 0.7810\n",
      "Epoch: 326 train_loss: 0.0716 train_acc: 0.9929 val_loss: 0.9205 val_acc: 0.7680 test_loss: 0.7958 test_acc: 0.7740\n",
      "Epoch: 327 train_loss: 0.0716 train_acc: 0.9929 val_loss: 0.7852 val_acc: 0.7680 test_loss: 0.7431 test_acc: 0.7900\n",
      "Epoch: 327 train_loss: 0.0716 train_acc: 0.9929 val_loss: 0.9301 val_acc: 0.7700 test_loss: 0.7431 test_acc: 0.7900\n",
      "Epoch: 327 train_loss: 0.0716 train_acc: 0.9929 val_loss: 0.9301 val_acc: 0.7700 test_loss: 0.8098 test_acc: 0.7710\n",
      "Epoch: 328 train_loss: 0.0742 train_acc: 0.9857 val_loss: 0.7822 val_acc: 0.7940 test_loss: 0.7374 test_acc: 0.8060\n",
      "Epoch: 328 train_loss: 0.0742 train_acc: 0.9857 val_loss: 0.9435 val_acc: 0.7640 test_loss: 0.7374 test_acc: 0.8060\n",
      "Epoch: 328 train_loss: 0.0742 train_acc: 0.9857 val_loss: 0.9435 val_acc: 0.7640 test_loss: 0.8250 test_acc: 0.7680\n",
      "Epoch: 329 train_loss: 0.0755 train_acc: 0.9786 val_loss: 0.7930 val_acc: 0.7880 test_loss: 0.7478 test_acc: 0.7970\n",
      "Epoch: 329 train_loss: 0.0755 train_acc: 0.9786 val_loss: 0.9392 val_acc: 0.7640 test_loss: 0.7478 test_acc: 0.7970\n",
      "Epoch: 329 train_loss: 0.0755 train_acc: 0.9786 val_loss: 0.9392 val_acc: 0.7640 test_loss: 0.8216 test_acc: 0.7670\n",
      "Epoch: 330 train_loss: 0.0758 train_acc: 0.9786 val_loss: 0.7976 val_acc: 0.7900 test_loss: 0.7533 test_acc: 0.8000\n",
      "Epoch: 330 train_loss: 0.0758 train_acc: 0.9786 val_loss: 0.9209 val_acc: 0.7680 test_loss: 0.7533 test_acc: 0.8000\n",
      "Epoch: 330 train_loss: 0.0758 train_acc: 0.9786 val_loss: 0.9209 val_acc: 0.7680 test_loss: 0.8007 test_acc: 0.7700\n",
      "Epoch: 331 train_loss: 0.0804 train_acc: 0.9786 val_loss: 0.8094 val_acc: 0.7840 test_loss: 0.7696 test_acc: 0.7820\n",
      "Epoch: 331 train_loss: 0.0804 train_acc: 0.9786 val_loss: 0.8992 val_acc: 0.7640 test_loss: 0.7696 test_acc: 0.7820\n",
      "Epoch: 331 train_loss: 0.0804 train_acc: 0.9786 val_loss: 0.8992 val_acc: 0.7640 test_loss: 0.7741 test_acc: 0.7800\n",
      "Epoch: 332 train_loss: 0.0781 train_acc: 0.9857 val_loss: 0.8557 val_acc: 0.7620 test_loss: 0.8215 test_acc: 0.7660\n",
      "Epoch: 332 train_loss: 0.0781 train_acc: 0.9857 val_loss: 0.8852 val_acc: 0.7640 test_loss: 0.8215 test_acc: 0.7660\n",
      "Epoch: 332 train_loss: 0.0781 train_acc: 0.9857 val_loss: 0.8852 val_acc: 0.7640 test_loss: 0.7548 test_acc: 0.7800\n",
      "Epoch: 333 train_loss: 0.0740 train_acc: 0.9857 val_loss: 0.8624 val_acc: 0.7620 test_loss: 0.8304 test_acc: 0.7660\n",
      "Epoch: 333 train_loss: 0.0740 train_acc: 0.9857 val_loss: 0.8587 val_acc: 0.7800 test_loss: 0.8304 test_acc: 0.7660\n",
      "Epoch: 333 train_loss: 0.0740 train_acc: 0.9857 val_loss: 0.8587 val_acc: 0.7800 test_loss: 0.7182 test_acc: 0.7860\n",
      "Epoch: 334 train_loss: 0.0697 train_acc: 0.9857 val_loss: 0.8405 val_acc: 0.7640 test_loss: 0.8066 test_acc: 0.7710\n",
      "Epoch: 334 train_loss: 0.0697 train_acc: 0.9857 val_loss: 0.8444 val_acc: 0.7860 test_loss: 0.8066 test_acc: 0.7710\n",
      "Epoch: 334 train_loss: 0.0697 train_acc: 0.9857 val_loss: 0.8444 val_acc: 0.7860 test_loss: 0.6967 test_acc: 0.7990\n",
      "Epoch: 335 train_loss: 0.0700 train_acc: 0.9929 val_loss: 0.8145 val_acc: 0.7660 test_loss: 0.7763 test_acc: 0.7730\n",
      "Epoch: 335 train_loss: 0.0700 train_acc: 0.9929 val_loss: 0.8544 val_acc: 0.7780 test_loss: 0.7763 test_acc: 0.7730\n",
      "Epoch: 335 train_loss: 0.0700 train_acc: 0.9929 val_loss: 0.8544 val_acc: 0.7780 test_loss: 0.7042 test_acc: 0.7980\n",
      "Epoch: 336 train_loss: 0.0784 train_acc: 0.9929 val_loss: 0.7835 val_acc: 0.7740 test_loss: 0.7437 test_acc: 0.8020\n",
      "Epoch: 336 train_loss: 0.0784 train_acc: 0.9929 val_loss: 0.9028 val_acc: 0.7620 test_loss: 0.7437 test_acc: 0.8020\n",
      "Epoch: 336 train_loss: 0.0784 train_acc: 0.9929 val_loss: 0.9028 val_acc: 0.7620 test_loss: 0.7617 test_acc: 0.7740\n",
      "Epoch: 337 train_loss: 0.0901 train_acc: 0.9929 val_loss: 0.7638 val_acc: 0.7880 test_loss: 0.7204 test_acc: 0.8140\n",
      "Epoch: 337 train_loss: 0.0901 train_acc: 0.9929 val_loss: 0.9611 val_acc: 0.7700 test_loss: 0.7204 test_acc: 0.8140\n",
      "Epoch: 337 train_loss: 0.0901 train_acc: 0.9929 val_loss: 0.9611 val_acc: 0.7700 test_loss: 0.8301 test_acc: 0.7650\n",
      "Epoch: 338 train_loss: 0.1047 train_acc: 0.9786 val_loss: 0.7539 val_acc: 0.7960 test_loss: 0.7071 test_acc: 0.8140\n",
      "Epoch: 338 train_loss: 0.1047 train_acc: 0.9786 val_loss: 1.0180 val_acc: 0.7580 test_loss: 0.7071 test_acc: 0.8140\n",
      "Epoch: 338 train_loss: 0.1047 train_acc: 0.9786 val_loss: 1.0180 val_acc: 0.7580 test_loss: 0.8910 test_acc: 0.7450\n",
      "Epoch: 339 train_loss: 0.1064 train_acc: 0.9857 val_loss: 0.7621 val_acc: 0.7940 test_loss: 0.7149 test_acc: 0.8060\n",
      "Epoch: 339 train_loss: 0.1064 train_acc: 0.9857 val_loss: 1.0322 val_acc: 0.7580 test_loss: 0.7149 test_acc: 0.8060\n",
      "Epoch: 339 train_loss: 0.1064 train_acc: 0.9857 val_loss: 1.0322 val_acc: 0.7580 test_loss: 0.9043 test_acc: 0.7460\n",
      "Epoch: 340 train_loss: 0.0983 train_acc: 0.9786 val_loss: 0.7769 val_acc: 0.7920 test_loss: 0.7285 test_acc: 0.7990\n",
      "Epoch: 340 train_loss: 0.0983 train_acc: 0.9786 val_loss: 1.0017 val_acc: 0.7640 test_loss: 0.7285 test_acc: 0.7990\n",
      "Epoch: 340 train_loss: 0.0983 train_acc: 0.9786 val_loss: 1.0017 val_acc: 0.7640 test_loss: 0.8725 test_acc: 0.7560\n",
      "Epoch: 341 train_loss: 0.0915 train_acc: 0.9786 val_loss: 0.7896 val_acc: 0.7940 test_loss: 0.7411 test_acc: 0.7960\n",
      "Epoch: 341 train_loss: 0.0915 train_acc: 0.9786 val_loss: 0.9668 val_acc: 0.7700 test_loss: 0.7411 test_acc: 0.7960\n",
      "Epoch: 341 train_loss: 0.0915 train_acc: 0.9786 val_loss: 0.9668 val_acc: 0.7700 test_loss: 0.8371 test_acc: 0.7640\n",
      "Epoch: 342 train_loss: 0.0793 train_acc: 0.9786 val_loss: 0.7726 val_acc: 0.7960 test_loss: 0.7286 test_acc: 0.7940\n",
      "Epoch: 342 train_loss: 0.0793 train_acc: 0.9786 val_loss: 0.9141 val_acc: 0.7720 test_loss: 0.7286 test_acc: 0.7940\n",
      "Epoch: 342 train_loss: 0.0793 train_acc: 0.9786 val_loss: 0.9141 val_acc: 0.7720 test_loss: 0.7827 test_acc: 0.7680\n",
      "Epoch: 343 train_loss: 0.0770 train_acc: 0.9714 val_loss: 0.7679 val_acc: 0.7960 test_loss: 0.7271 test_acc: 0.7850\n",
      "Epoch: 343 train_loss: 0.0770 train_acc: 0.9714 val_loss: 0.8337 val_acc: 0.7820 test_loss: 0.7271 test_acc: 0.7850\n",
      "Epoch: 343 train_loss: 0.0770 train_acc: 0.9714 val_loss: 0.8337 val_acc: 0.7820 test_loss: 0.7087 test_acc: 0.7780\n",
      "Epoch: 344 train_loss: 0.1062 train_acc: 0.9643 val_loss: 0.8180 val_acc: 0.7800 test_loss: 0.7830 test_acc: 0.7630\n",
      "Epoch: 344 train_loss: 0.1062 train_acc: 0.9643 val_loss: 0.8249 val_acc: 0.7780 test_loss: 0.7830 test_acc: 0.7630\n",
      "Epoch: 344 train_loss: 0.1062 train_acc: 0.9643 val_loss: 0.8249 val_acc: 0.7780 test_loss: 0.7067 test_acc: 0.7880\n",
      "Epoch: 345 train_loss: 0.1473 train_acc: 0.9643 val_loss: 0.8539 val_acc: 0.7680 test_loss: 0.8216 test_acc: 0.7550\n",
      "Epoch: 345 train_loss: 0.1473 train_acc: 0.9643 val_loss: 0.8478 val_acc: 0.7660 test_loss: 0.8216 test_acc: 0.7550\n",
      "Epoch: 345 train_loss: 0.1473 train_acc: 0.9643 val_loss: 0.8478 val_acc: 0.7660 test_loss: 0.7444 test_acc: 0.7870\n",
      "Epoch: 346 train_loss: 0.1863 train_acc: 0.9500 val_loss: 0.8189 val_acc: 0.7900 test_loss: 0.7849 test_acc: 0.7730\n",
      "Epoch: 346 train_loss: 0.1863 train_acc: 0.9500 val_loss: 0.9004 val_acc: 0.7540 test_loss: 0.7849 test_acc: 0.7730\n",
      "Epoch: 346 train_loss: 0.1863 train_acc: 0.9500 val_loss: 0.9004 val_acc: 0.7540 test_loss: 0.8183 test_acc: 0.7610\n",
      "Epoch: 347 train_loss: 0.1477 train_acc: 0.9714 val_loss: 0.7992 val_acc: 0.7920 test_loss: 0.7592 test_acc: 0.7840\n",
      "Epoch: 347 train_loss: 0.1477 train_acc: 0.9714 val_loss: 0.8791 val_acc: 0.7640 test_loss: 0.7592 test_acc: 0.7840\n",
      "Epoch: 347 train_loss: 0.1477 train_acc: 0.9714 val_loss: 0.8791 val_acc: 0.7640 test_loss: 0.7905 test_acc: 0.7650\n",
      "Epoch: 348 train_loss: 0.0990 train_acc: 0.9929 val_loss: 0.7828 val_acc: 0.7960 test_loss: 0.7394 test_acc: 0.7880\n",
      "Epoch: 348 train_loss: 0.0990 train_acc: 0.9929 val_loss: 0.8595 val_acc: 0.7660 test_loss: 0.7394 test_acc: 0.7880\n",
      "Epoch: 348 train_loss: 0.0990 train_acc: 0.9929 val_loss: 0.8595 val_acc: 0.7660 test_loss: 0.7502 test_acc: 0.7750\n",
      "Epoch: 349 train_loss: 0.0781 train_acc: 0.9857 val_loss: 0.7738 val_acc: 0.7860 test_loss: 0.7288 test_acc: 0.7820\n",
      "Epoch: 349 train_loss: 0.0781 train_acc: 0.9857 val_loss: 0.8676 val_acc: 0.7800 test_loss: 0.7288 test_acc: 0.7820\n",
      "Epoch: 349 train_loss: 0.0781 train_acc: 0.9857 val_loss: 0.8676 val_acc: 0.7800 test_loss: 0.7424 test_acc: 0.7690\n",
      "Epoch: 350 train_loss: 0.0781 train_acc: 0.9929 val_loss: 0.7670 val_acc: 0.7860 test_loss: 0.7246 test_acc: 0.7880\n",
      "Epoch: 350 train_loss: 0.0781 train_acc: 0.9929 val_loss: 0.8945 val_acc: 0.7820 test_loss: 0.7246 test_acc: 0.7880\n",
      "Epoch: 350 train_loss: 0.0781 train_acc: 0.9929 val_loss: 0.8945 val_acc: 0.7820 test_loss: 0.7615 test_acc: 0.7810\n",
      "Epoch: 351 train_loss: 0.0949 train_acc: 0.9857 val_loss: 0.7719 val_acc: 0.7900 test_loss: 0.7327 test_acc: 0.7920\n",
      "Epoch: 351 train_loss: 0.0949 train_acc: 0.9857 val_loss: 0.9326 val_acc: 0.7800 test_loss: 0.7327 test_acc: 0.7920\n",
      "Epoch: 351 train_loss: 0.0949 train_acc: 0.9857 val_loss: 0.9326 val_acc: 0.7800 test_loss: 0.8003 test_acc: 0.7730\n",
      "Epoch: 352 train_loss: 0.1149 train_acc: 0.9714 val_loss: 0.7774 val_acc: 0.7920 test_loss: 0.7421 test_acc: 0.7830\n",
      "Epoch: 352 train_loss: 0.1149 train_acc: 0.9714 val_loss: 0.9757 val_acc: 0.7660 test_loss: 0.7421 test_acc: 0.7830\n",
      "Epoch: 352 train_loss: 0.1149 train_acc: 0.9714 val_loss: 0.9757 val_acc: 0.7660 test_loss: 0.8443 test_acc: 0.7660\n",
      "Epoch: 353 train_loss: 0.1286 train_acc: 0.9643 val_loss: 0.7877 val_acc: 0.7800 test_loss: 0.7553 test_acc: 0.7760\n",
      "Epoch: 353 train_loss: 0.1286 train_acc: 0.9643 val_loss: 1.0057 val_acc: 0.7440 test_loss: 0.7553 test_acc: 0.7760\n",
      "Epoch: 353 train_loss: 0.1286 train_acc: 0.9643 val_loss: 1.0057 val_acc: 0.7440 test_loss: 0.8729 test_acc: 0.7510\n",
      "Epoch: 354 train_loss: 0.1274 train_acc: 0.9643 val_loss: 0.8020 val_acc: 0.7840 test_loss: 0.7729 test_acc: 0.7660\n",
      "Epoch: 354 train_loss: 0.1274 train_acc: 0.9643 val_loss: 0.9965 val_acc: 0.7380 test_loss: 0.7729 test_acc: 0.7660\n",
      "Epoch: 354 train_loss: 0.1274 train_acc: 0.9643 val_loss: 0.9965 val_acc: 0.7380 test_loss: 0.8657 test_acc: 0.7480\n",
      "Epoch: 355 train_loss: 0.1177 train_acc: 0.9714 val_loss: 0.8048 val_acc: 0.7720 test_loss: 0.7758 test_acc: 0.7770\n",
      "Epoch: 355 train_loss: 0.1177 train_acc: 0.9714 val_loss: 0.9618 val_acc: 0.7380 test_loss: 0.7758 test_acc: 0.7770\n",
      "Epoch: 355 train_loss: 0.1177 train_acc: 0.9714 val_loss: 0.9618 val_acc: 0.7380 test_loss: 0.8324 test_acc: 0.7520\n",
      "Epoch: 356 train_loss: 0.1054 train_acc: 0.9714 val_loss: 0.8108 val_acc: 0.7700 test_loss: 0.7809 test_acc: 0.7710\n",
      "Epoch: 356 train_loss: 0.1054 train_acc: 0.9714 val_loss: 0.9260 val_acc: 0.7520 test_loss: 0.7809 test_acc: 0.7710\n",
      "Epoch: 356 train_loss: 0.1054 train_acc: 0.9714 val_loss: 0.9260 val_acc: 0.7520 test_loss: 0.7974 test_acc: 0.7670\n",
      "Epoch: 357 train_loss: 0.0922 train_acc: 0.9857 val_loss: 0.8555 val_acc: 0.7480 test_loss: 0.8241 test_acc: 0.7380\n",
      "Epoch: 357 train_loss: 0.0922 train_acc: 0.9857 val_loss: 0.8918 val_acc: 0.7640 test_loss: 0.8241 test_acc: 0.7380\n",
      "Epoch: 357 train_loss: 0.0922 train_acc: 0.9857 val_loss: 0.8918 val_acc: 0.7640 test_loss: 0.7627 test_acc: 0.7790\n",
      "Epoch: 358 train_loss: 0.0801 train_acc: 0.9929 val_loss: 0.8778 val_acc: 0.7360 test_loss: 0.8433 test_acc: 0.7260\n",
      "Epoch: 358 train_loss: 0.0801 train_acc: 0.9929 val_loss: 0.8641 val_acc: 0.7760 test_loss: 0.8433 test_acc: 0.7260\n",
      "Epoch: 358 train_loss: 0.0801 train_acc: 0.9929 val_loss: 0.8641 val_acc: 0.7760 test_loss: 0.7345 test_acc: 0.7890\n",
      "Epoch: 359 train_loss: 0.0750 train_acc: 1.0000 val_loss: 0.8521 val_acc: 0.7520 test_loss: 0.8122 test_acc: 0.7620\n",
      "Epoch: 359 train_loss: 0.0750 train_acc: 1.0000 val_loss: 0.8503 val_acc: 0.7840 test_loss: 0.8122 test_acc: 0.7620\n",
      "Epoch: 359 train_loss: 0.0750 train_acc: 1.0000 val_loss: 0.8503 val_acc: 0.7840 test_loss: 0.7235 test_acc: 0.7920\n",
      "Epoch: 360 train_loss: 0.0761 train_acc: 0.9929 val_loss: 0.8362 val_acc: 0.7720 test_loss: 0.7928 test_acc: 0.7750\n",
      "Epoch: 360 train_loss: 0.0761 train_acc: 0.9929 val_loss: 0.8472 val_acc: 0.7780 test_loss: 0.7928 test_acc: 0.7750\n",
      "Epoch: 360 train_loss: 0.0761 train_acc: 0.9929 val_loss: 0.8472 val_acc: 0.7780 test_loss: 0.7254 test_acc: 0.8000\n",
      "Epoch: 361 train_loss: 0.0822 train_acc: 0.9857 val_loss: 0.8114 val_acc: 0.7860 test_loss: 0.7685 test_acc: 0.7820\n",
      "Epoch: 361 train_loss: 0.0822 train_acc: 0.9857 val_loss: 0.8625 val_acc: 0.7820 test_loss: 0.7685 test_acc: 0.7820\n",
      "Epoch: 361 train_loss: 0.0822 train_acc: 0.9857 val_loss: 0.8625 val_acc: 0.7820 test_loss: 0.7533 test_acc: 0.7860\n",
      "Epoch: 362 train_loss: 0.0860 train_acc: 0.9786 val_loss: 0.8001 val_acc: 0.7920 test_loss: 0.7579 test_acc: 0.7920\n",
      "Epoch: 362 train_loss: 0.0860 train_acc: 0.9786 val_loss: 0.8749 val_acc: 0.7660 test_loss: 0.7579 test_acc: 0.7920\n",
      "Epoch: 362 train_loss: 0.0860 train_acc: 0.9786 val_loss: 0.8749 val_acc: 0.7660 test_loss: 0.7708 test_acc: 0.7750\n",
      "Epoch: 363 train_loss: 0.0782 train_acc: 0.9786 val_loss: 0.8141 val_acc: 0.7760 test_loss: 0.7773 test_acc: 0.7800\n",
      "Epoch: 363 train_loss: 0.0782 train_acc: 0.9786 val_loss: 0.8714 val_acc: 0.7660 test_loss: 0.7773 test_acc: 0.7800\n",
      "Epoch: 363 train_loss: 0.0782 train_acc: 0.9786 val_loss: 0.8714 val_acc: 0.7660 test_loss: 0.7651 test_acc: 0.7720\n",
      "Epoch: 364 train_loss: 0.0769 train_acc: 0.9786 val_loss: 0.8304 val_acc: 0.7700 test_loss: 0.8003 test_acc: 0.7650\n",
      "Epoch: 364 train_loss: 0.0769 train_acc: 0.9786 val_loss: 0.8764 val_acc: 0.7780 test_loss: 0.8003 test_acc: 0.7650\n",
      "Epoch: 364 train_loss: 0.0769 train_acc: 0.9786 val_loss: 0.8764 val_acc: 0.7780 test_loss: 0.7647 test_acc: 0.7800\n",
      "Epoch: 365 train_loss: 0.0840 train_acc: 0.9857 val_loss: 0.8390 val_acc: 0.7720 test_loss: 0.8111 test_acc: 0.7660\n",
      "Epoch: 365 train_loss: 0.0840 train_acc: 0.9857 val_loss: 0.8849 val_acc: 0.7760 test_loss: 0.8111 test_acc: 0.7660\n",
      "Epoch: 365 train_loss: 0.0840 train_acc: 0.9857 val_loss: 0.8849 val_acc: 0.7760 test_loss: 0.7663 test_acc: 0.7860\n",
      "Epoch: 366 train_loss: 0.0869 train_acc: 0.9786 val_loss: 0.8301 val_acc: 0.7780 test_loss: 0.7998 test_acc: 0.7730\n",
      "Epoch: 366 train_loss: 0.0869 train_acc: 0.9786 val_loss: 0.8786 val_acc: 0.7780 test_loss: 0.7998 test_acc: 0.7730\n",
      "Epoch: 366 train_loss: 0.0869 train_acc: 0.9786 val_loss: 0.8786 val_acc: 0.7780 test_loss: 0.7547 test_acc: 0.7840\n",
      "Epoch: 367 train_loss: 0.0828 train_acc: 0.9786 val_loss: 0.8064 val_acc: 0.7860 test_loss: 0.7719 test_acc: 0.7790\n",
      "Epoch: 367 train_loss: 0.0828 train_acc: 0.9786 val_loss: 0.8634 val_acc: 0.7800 test_loss: 0.7719 test_acc: 0.7790\n",
      "Epoch: 367 train_loss: 0.0828 train_acc: 0.9786 val_loss: 0.8634 val_acc: 0.7800 test_loss: 0.7329 test_acc: 0.7850\n",
      "Epoch: 368 train_loss: 0.0783 train_acc: 0.9786 val_loss: 0.7915 val_acc: 0.7920 test_loss: 0.7530 test_acc: 0.7810\n",
      "Epoch: 368 train_loss: 0.0783 train_acc: 0.9786 val_loss: 0.8556 val_acc: 0.7860 test_loss: 0.7530 test_acc: 0.7810\n",
      "Epoch: 368 train_loss: 0.0783 train_acc: 0.9786 val_loss: 0.8556 val_acc: 0.7860 test_loss: 0.7154 test_acc: 0.7970\n",
      "Epoch: 369 train_loss: 0.0767 train_acc: 0.9929 val_loss: 0.7669 val_acc: 0.7980 test_loss: 0.7256 test_acc: 0.7880\n",
      "Epoch: 369 train_loss: 0.0767 train_acc: 0.9929 val_loss: 0.8526 val_acc: 0.7820 test_loss: 0.7256 test_acc: 0.7880\n",
      "Epoch: 369 train_loss: 0.0767 train_acc: 0.9929 val_loss: 0.8526 val_acc: 0.7820 test_loss: 0.7054 test_acc: 0.7930\n",
      "Epoch: 370 train_loss: 0.0756 train_acc: 0.9929 val_loss: 0.7669 val_acc: 0.7960 test_loss: 0.7254 test_acc: 0.7890\n",
      "Epoch: 370 train_loss: 0.0756 train_acc: 0.9929 val_loss: 0.8540 val_acc: 0.7820 test_loss: 0.7254 test_acc: 0.7890\n",
      "Epoch: 370 train_loss: 0.0756 train_acc: 0.9929 val_loss: 0.8540 val_acc: 0.7820 test_loss: 0.7017 test_acc: 0.7960\n",
      "Epoch: 371 train_loss: 0.0750 train_acc: 0.9929 val_loss: 0.7789 val_acc: 0.7900 test_loss: 0.7355 test_acc: 0.7890\n",
      "Epoch: 371 train_loss: 0.0750 train_acc: 0.9929 val_loss: 0.8558 val_acc: 0.7860 test_loss: 0.7355 test_acc: 0.7890\n",
      "Epoch: 371 train_loss: 0.0750 train_acc: 0.9929 val_loss: 0.8558 val_acc: 0.7860 test_loss: 0.7014 test_acc: 0.7990\n",
      "Epoch: 372 train_loss: 0.0674 train_acc: 0.9857 val_loss: 0.8114 val_acc: 0.7840 test_loss: 0.7654 test_acc: 0.7890\n",
      "Epoch: 372 train_loss: 0.0674 train_acc: 0.9857 val_loss: 0.8629 val_acc: 0.7860 test_loss: 0.7654 test_acc: 0.7890\n",
      "Epoch: 372 train_loss: 0.0674 train_acc: 0.9857 val_loss: 0.8629 val_acc: 0.7860 test_loss: 0.7121 test_acc: 0.7970\n",
      "Epoch: 373 train_loss: 0.0642 train_acc: 0.9929 val_loss: 0.8482 val_acc: 0.7840 test_loss: 0.8007 test_acc: 0.7730\n",
      "Epoch: 373 train_loss: 0.0642 train_acc: 0.9929 val_loss: 0.8757 val_acc: 0.7860 test_loss: 0.8007 test_acc: 0.7730\n",
      "Epoch: 373 train_loss: 0.0642 train_acc: 0.9929 val_loss: 0.8757 val_acc: 0.7860 test_loss: 0.7308 test_acc: 0.7920\n",
      "Epoch: 374 train_loss: 0.0642 train_acc: 0.9929 val_loss: 0.8582 val_acc: 0.7680 test_loss: 0.8097 test_acc: 0.7640\n",
      "Epoch: 374 train_loss: 0.0642 train_acc: 0.9929 val_loss: 0.8813 val_acc: 0.7820 test_loss: 0.8097 test_acc: 0.7640\n",
      "Epoch: 374 train_loss: 0.0642 train_acc: 0.9929 val_loss: 0.8813 val_acc: 0.7820 test_loss: 0.7407 test_acc: 0.7880\n",
      "Epoch: 375 train_loss: 0.0663 train_acc: 0.9929 val_loss: 0.8452 val_acc: 0.7760 test_loss: 0.7972 test_acc: 0.7670\n",
      "Epoch: 375 train_loss: 0.0663 train_acc: 0.9929 val_loss: 0.8873 val_acc: 0.7740 test_loss: 0.7972 test_acc: 0.7670\n",
      "Epoch: 375 train_loss: 0.0663 train_acc: 0.9929 val_loss: 0.8873 val_acc: 0.7740 test_loss: 0.7533 test_acc: 0.7830\n",
      "Epoch: 376 train_loss: 0.0672 train_acc: 0.9857 val_loss: 0.7695 val_acc: 0.8000 test_loss: 0.7282 test_acc: 0.7940\n",
      "Epoch: 376 train_loss: 0.0672 train_acc: 0.9857 val_loss: 0.8912 val_acc: 0.7760 test_loss: 0.7282 test_acc: 0.7940\n",
      "Epoch: 376 train_loss: 0.0672 train_acc: 0.9857 val_loss: 0.8912 val_acc: 0.7760 test_loss: 0.7610 test_acc: 0.7810\n",
      "Epoch: 377 train_loss: 0.0659 train_acc: 0.9857 val_loss: 0.7522 val_acc: 0.7960 test_loss: 0.7223 test_acc: 0.7850\n",
      "Epoch: 377 train_loss: 0.0659 train_acc: 0.9857 val_loss: 0.8928 val_acc: 0.7720 test_loss: 0.7223 test_acc: 0.7850\n",
      "Epoch: 377 train_loss: 0.0659 train_acc: 0.9857 val_loss: 0.8928 val_acc: 0.7720 test_loss: 0.7648 test_acc: 0.7830\n",
      "Epoch: 378 train_loss: 0.0596 train_acc: 0.9857 val_loss: 0.7704 val_acc: 0.7980 test_loss: 0.7489 test_acc: 0.7900\n",
      "Epoch: 378 train_loss: 0.0596 train_acc: 0.9857 val_loss: 0.8752 val_acc: 0.7800 test_loss: 0.7489 test_acc: 0.7900\n",
      "Epoch: 378 train_loss: 0.0596 train_acc: 0.9857 val_loss: 0.8752 val_acc: 0.7800 test_loss: 0.7496 test_acc: 0.7850\n",
      "Epoch: 379 train_loss: 0.0550 train_acc: 0.9857 val_loss: 0.7998 val_acc: 0.7820 test_loss: 0.7815 test_acc: 0.7770\n",
      "Epoch: 379 train_loss: 0.0550 train_acc: 0.9857 val_loss: 0.8660 val_acc: 0.7860 test_loss: 0.7815 test_acc: 0.7770\n",
      "Epoch: 379 train_loss: 0.0550 train_acc: 0.9857 val_loss: 0.8660 val_acc: 0.7860 test_loss: 0.7393 test_acc: 0.7870\n",
      "Epoch: 380 train_loss: 0.0574 train_acc: 0.9929 val_loss: 0.8129 val_acc: 0.7820 test_loss: 0.7978 test_acc: 0.7800\n",
      "Epoch: 380 train_loss: 0.0574 train_acc: 0.9929 val_loss: 0.8758 val_acc: 0.7780 test_loss: 0.7978 test_acc: 0.7800\n",
      "Epoch: 380 train_loss: 0.0574 train_acc: 0.9929 val_loss: 0.8758 val_acc: 0.7780 test_loss: 0.7454 test_acc: 0.7880\n",
      "Epoch: 381 train_loss: 0.0597 train_acc: 0.9929 val_loss: 0.7965 val_acc: 0.7880 test_loss: 0.7800 test_acc: 0.7820\n",
      "Epoch: 381 train_loss: 0.0597 train_acc: 0.9929 val_loss: 0.8861 val_acc: 0.7800 test_loss: 0.7800 test_acc: 0.7820\n",
      "Epoch: 381 train_loss: 0.0597 train_acc: 0.9929 val_loss: 0.8861 val_acc: 0.7800 test_loss: 0.7520 test_acc: 0.7870\n",
      "Epoch: 382 train_loss: 0.0616 train_acc: 0.9929 val_loss: 0.7694 val_acc: 0.7820 test_loss: 0.7480 test_acc: 0.7850\n",
      "Epoch: 382 train_loss: 0.0616 train_acc: 0.9929 val_loss: 0.8934 val_acc: 0.7820 test_loss: 0.7480 test_acc: 0.7850\n",
      "Epoch: 382 train_loss: 0.0616 train_acc: 0.9929 val_loss: 0.8934 val_acc: 0.7820 test_loss: 0.7559 test_acc: 0.7860\n",
      "Epoch: 383 train_loss: 0.0656 train_acc: 0.9929 val_loss: 0.7740 val_acc: 0.7880 test_loss: 0.7421 test_acc: 0.7920\n",
      "Epoch: 383 train_loss: 0.0656 train_acc: 0.9929 val_loss: 0.9199 val_acc: 0.7800 test_loss: 0.7421 test_acc: 0.7920\n",
      "Epoch: 383 train_loss: 0.0656 train_acc: 0.9929 val_loss: 0.9199 val_acc: 0.7800 test_loss: 0.7807 test_acc: 0.7850\n",
      "Epoch: 384 train_loss: 0.0727 train_acc: 0.9857 val_loss: 0.7702 val_acc: 0.7860 test_loss: 0.7356 test_acc: 0.7970\n",
      "Epoch: 384 train_loss: 0.0727 train_acc: 0.9857 val_loss: 0.9608 val_acc: 0.7640 test_loss: 0.7356 test_acc: 0.7970\n",
      "Epoch: 384 train_loss: 0.0727 train_acc: 0.9857 val_loss: 0.9608 val_acc: 0.7640 test_loss: 0.8214 test_acc: 0.7770\n",
      "Epoch: 385 train_loss: 0.0771 train_acc: 0.9857 val_loss: 0.8064 val_acc: 0.7820 test_loss: 0.7703 test_acc: 0.7740\n",
      "Epoch: 385 train_loss: 0.0771 train_acc: 0.9857 val_loss: 0.9798 val_acc: 0.7560 test_loss: 0.7703 test_acc: 0.7740\n",
      "Epoch: 385 train_loss: 0.0771 train_acc: 0.9857 val_loss: 0.9798 val_acc: 0.7560 test_loss: 0.8421 test_acc: 0.7720\n",
      "Epoch: 386 train_loss: 0.0778 train_acc: 0.9929 val_loss: 0.8539 val_acc: 0.7660 test_loss: 0.8150 test_acc: 0.7570\n",
      "Epoch: 386 train_loss: 0.0778 train_acc: 0.9929 val_loss: 0.9745 val_acc: 0.7540 test_loss: 0.8150 test_acc: 0.7570\n",
      "Epoch: 386 train_loss: 0.0778 train_acc: 0.9929 val_loss: 0.9745 val_acc: 0.7540 test_loss: 0.8412 test_acc: 0.7700\n",
      "Epoch: 387 train_loss: 0.0744 train_acc: 0.9929 val_loss: 0.8595 val_acc: 0.7520 test_loss: 0.8246 test_acc: 0.7370\n",
      "Epoch: 387 train_loss: 0.0744 train_acc: 0.9929 val_loss: 0.9367 val_acc: 0.7740 test_loss: 0.8246 test_acc: 0.7370\n",
      "Epoch: 387 train_loss: 0.0744 train_acc: 0.9929 val_loss: 0.9367 val_acc: 0.7740 test_loss: 0.8108 test_acc: 0.7790\n",
      "Epoch: 388 train_loss: 0.0730 train_acc: 0.9929 val_loss: 0.8090 val_acc: 0.7840 test_loss: 0.7765 test_acc: 0.7720\n",
      "Epoch: 388 train_loss: 0.0730 train_acc: 0.9929 val_loss: 0.9121 val_acc: 0.7740 test_loss: 0.7765 test_acc: 0.7720\n",
      "Epoch: 388 train_loss: 0.0730 train_acc: 0.9929 val_loss: 0.9121 val_acc: 0.7740 test_loss: 0.7936 test_acc: 0.7820\n",
      "Epoch: 389 train_loss: 0.0718 train_acc: 0.9929 val_loss: 0.7799 val_acc: 0.7920 test_loss: 0.7479 test_acc: 0.7880\n",
      "Epoch: 389 train_loss: 0.0718 train_acc: 0.9929 val_loss: 0.8905 val_acc: 0.7800 test_loss: 0.7479 test_acc: 0.7880\n",
      "Epoch: 389 train_loss: 0.0718 train_acc: 0.9929 val_loss: 0.8905 val_acc: 0.7800 test_loss: 0.7807 test_acc: 0.7790\n",
      "Epoch: 390 train_loss: 0.0711 train_acc: 0.9929 val_loss: 0.7837 val_acc: 0.8020 test_loss: 0.7517 test_acc: 0.7880\n",
      "Epoch: 390 train_loss: 0.0711 train_acc: 0.9929 val_loss: 0.8815 val_acc: 0.7800 test_loss: 0.7517 test_acc: 0.7880\n",
      "Epoch: 390 train_loss: 0.0711 train_acc: 0.9929 val_loss: 0.8815 val_acc: 0.7800 test_loss: 0.7770 test_acc: 0.7780\n",
      "Epoch: 391 train_loss: 0.0697 train_acc: 0.9929 val_loss: 0.7770 val_acc: 0.8040 test_loss: 0.7446 test_acc: 0.7780\n",
      "Epoch: 391 train_loss: 0.0697 train_acc: 0.9929 val_loss: 0.8797 val_acc: 0.7860 test_loss: 0.7446 test_acc: 0.7780\n",
      "Epoch: 391 train_loss: 0.0697 train_acc: 0.9929 val_loss: 0.8797 val_acc: 0.7860 test_loss: 0.7750 test_acc: 0.7820\n",
      "Epoch: 392 train_loss: 0.0687 train_acc: 0.9929 val_loss: 0.7681 val_acc: 0.8100 test_loss: 0.7364 test_acc: 0.7870\n",
      "Epoch: 392 train_loss: 0.0687 train_acc: 0.9929 val_loss: 0.8872 val_acc: 0.7880 test_loss: 0.7364 test_acc: 0.7870\n",
      "Epoch: 392 train_loss: 0.0687 train_acc: 0.9929 val_loss: 0.8872 val_acc: 0.7880 test_loss: 0.7806 test_acc: 0.7860\n",
      "Epoch: 393 train_loss: 0.0668 train_acc: 0.9929 val_loss: 0.7587 val_acc: 0.8080 test_loss: 0.7289 test_acc: 0.7910\n",
      "Epoch: 393 train_loss: 0.0668 train_acc: 0.9929 val_loss: 0.8866 val_acc: 0.7920 test_loss: 0.7289 test_acc: 0.7910\n",
      "Epoch: 393 train_loss: 0.0668 train_acc: 0.9929 val_loss: 0.8866 val_acc: 0.7920 test_loss: 0.7780 test_acc: 0.7900\n",
      "Epoch: 394 train_loss: 0.0639 train_acc: 0.9929 val_loss: 0.7707 val_acc: 0.8000 test_loss: 0.7417 test_acc: 0.7860\n",
      "Epoch: 394 train_loss: 0.0639 train_acc: 0.9929 val_loss: 0.8891 val_acc: 0.7920 test_loss: 0.7417 test_acc: 0.7860\n",
      "Epoch: 394 train_loss: 0.0639 train_acc: 0.9929 val_loss: 0.8891 val_acc: 0.7920 test_loss: 0.7788 test_acc: 0.7940\n",
      "Epoch: 395 train_loss: 0.0628 train_acc: 1.0000 val_loss: 0.7595 val_acc: 0.7920 test_loss: 0.7276 test_acc: 0.7960\n",
      "Epoch: 395 train_loss: 0.0628 train_acc: 1.0000 val_loss: 0.8718 val_acc: 0.7880 test_loss: 0.7276 test_acc: 0.7960\n",
      "Epoch: 395 train_loss: 0.0628 train_acc: 1.0000 val_loss: 0.8718 val_acc: 0.7880 test_loss: 0.7572 test_acc: 0.7940\n",
      "Epoch: 396 train_loss: 0.0650 train_acc: 0.9857 val_loss: 0.7724 val_acc: 0.7840 test_loss: 0.7351 test_acc: 0.7810\n",
      "Epoch: 396 train_loss: 0.0650 train_acc: 0.9857 val_loss: 0.8692 val_acc: 0.7840 test_loss: 0.7351 test_acc: 0.7810\n",
      "Epoch: 396 train_loss: 0.0650 train_acc: 0.9857 val_loss: 0.8692 val_acc: 0.7840 test_loss: 0.7514 test_acc: 0.7950\n",
      "Epoch: 397 train_loss: 0.0773 train_acc: 0.9714 val_loss: 0.7851 val_acc: 0.7820 test_loss: 0.7422 test_acc: 0.7800\n",
      "Epoch: 397 train_loss: 0.0773 train_acc: 0.9714 val_loss: 0.8587 val_acc: 0.7900 test_loss: 0.7422 test_acc: 0.7800\n",
      "Epoch: 397 train_loss: 0.0773 train_acc: 0.9714 val_loss: 0.8587 val_acc: 0.7900 test_loss: 0.7441 test_acc: 0.7990\n",
      "Epoch: 398 train_loss: 0.1017 train_acc: 0.9786 val_loss: 0.7770 val_acc: 0.8040 test_loss: 0.7292 test_acc: 0.7850\n",
      "Epoch: 398 train_loss: 0.1017 train_acc: 0.9786 val_loss: 0.8841 val_acc: 0.7860 test_loss: 0.7292 test_acc: 0.7850\n",
      "Epoch: 398 train_loss: 0.1017 train_acc: 0.9786 val_loss: 0.8841 val_acc: 0.7860 test_loss: 0.7885 test_acc: 0.7850\n",
      "Epoch: 399 train_loss: 0.0601 train_acc: 0.9786 val_loss: 0.7900 val_acc: 0.8020 test_loss: 0.7400 test_acc: 0.7810\n",
      "Epoch: 399 train_loss: 0.0601 train_acc: 0.9786 val_loss: 0.8903 val_acc: 0.7900 test_loss: 0.7400 test_acc: 0.7810\n",
      "Epoch: 399 train_loss: 0.0601 train_acc: 0.9786 val_loss: 0.8903 val_acc: 0.7900 test_loss: 0.7716 test_acc: 0.7850\n",
      "Epoch: 400 train_loss: 0.0541 train_acc: 0.9929 val_loss: 0.8172 val_acc: 0.7900 test_loss: 0.7679 test_acc: 0.7830\n",
      "Epoch: 400 train_loss: 0.0541 train_acc: 0.9929 val_loss: 0.8875 val_acc: 0.7760 test_loss: 0.7679 test_acc: 0.7830\n",
      "Epoch: 400 train_loss: 0.0541 train_acc: 0.9929 val_loss: 0.8875 val_acc: 0.7760 test_loss: 0.7655 test_acc: 0.7860\n",
      "#### Rep  1 Finished!  test acc: 0.8150 ####\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# initialize the learning rate scheduler\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "# training\n",
    "for rep in range(num_reps):\n",
    "    print('****** Rep {}: training start ******'.format(rep + 1))\n",
    "    max_acc = 0.0\n",
    "\n",
    "    # initialize the model\n",
    "    model = Net(dataset.num_node_features, nhid, dataset.num_classes, r, Lev, num_nodes, shrinkage=None,\n",
    "                threshold=1e-3, dropout_prob=0.6).to(device)\n",
    "\n",
    "    # initialize the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # training mode\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data, d_list)\n",
    "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # evaluation mode\n",
    "        model.eval()\n",
    "        out = model(data, d_list)\n",
    "        for i, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "            pred = out[mask].max(dim=1)[1]\n",
    "            correct = float(pred.eq(data.y[mask]).sum().item())\n",
    "            e_acc = correct / mask.sum().item()\n",
    "            epoch_acc[i][rep, epoch] = e_acc\n",
    "\n",
    "            e_loss = F.nll_loss(out[mask], data.y[mask])\n",
    "            epoch_loss[i][rep, epoch] = e_loss\n",
    "\n",
    "            # scheduler.step(epoch_loss['val_mask'][rep, epoch])\n",
    "\n",
    "            # print out results\n",
    "            print('Epoch: {:3d}'.format(epoch + 1),\n",
    "              'train_loss: {:.4f}'.format(epoch_loss['train_mask'][rep, epoch]),\n",
    "              'train_acc: {:.4f}'.format(epoch_acc['train_mask'][rep, epoch]),\n",
    "              'val_loss: {:.4f}'.format(epoch_loss['val_mask'][rep, epoch]),\n",
    "              'val_acc: {:.4f}'.format(epoch_acc['val_mask'][rep, epoch]),\n",
    "              'test_loss: {:.4f}'.format(epoch_loss['test_mask'][rep, epoch]),\n",
    "              'test_acc: {:.4f}'.format(epoch_acc['test_mask'][rep, epoch]))\n",
    "\n",
    "            if epoch_acc['test_mask'][rep, epoch] > max_acc:\n",
    "                max_acc = epoch_acc['test_mask'][rep, epoch]\n",
    "\n",
    "    print('#### Rep {0:2d} Finished!  test acc: {1:.4f} ####\\n'.format(rep + 1, max_acc))\n",
    "\n",
    "#                 if epoch_acc['val_mask'][rep, epoch] > max_acc:\n",
    "#                     print('=== Model saved at epoch: {:3d}'.format(epoch + 1))\n",
    "#                     max_acc = epoch_acc['val_mask'][rep, epoch]\n",
    "#                     record_test_acc = epoch_acc['test_mask'][rep, epoch]\n",
    "\n",
    "#         saved_model_val_acc[rep] = max_acc\n",
    "#         saved_model_test_acc[rep] = record_test_acc\n",
    "#         print('#### Rep {0:2d} Finished! val acc: {1:.4f}, test acc: {2:.4f} ####\\n'.format(rep + 1, max_acc, record_test_acc))\n",
    "\n",
    "#     print('***************************************************************************************************************************')\n",
    "#     print('Average test accuracy over {0:2d} reps: {1:.4f} with stdev {2:.4f}'.format(num_reps, np.mean(saved_model_test_acc), np.std(saved_model_test_acc)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
